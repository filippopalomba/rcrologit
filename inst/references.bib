@article{a.smith2005DoesMatchingOvercome,
  title = {Does Matching Overcome {{LaLonde}}'s Critique of Nonexperimental Estimators?},
  author = {A. Smith, Jeffrey and E. Todd, Petra},
  year = {2005},
  month = mar,
  journal = {Journal of Econometrics},
  series = {Experimental and Non-Experimental Evaluation of Economic Policy and Models},
  volume = {125},
  number = {1},
  pages = {305--353},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2004.04.011},
  urldate = {2023-05-13},
  abstract = {This paper applies cross-sectional and longitudinal propensity score matching estimators to data from the National Supported Work (NSW) Demonstration that have been previously analyzed by LaLonde (1986) and Dehejia and Wahba (1999, 2002). We find that estimates of the impact of NSW based on propensity score matching are highly sensitive to both the set of variables included in the scores and the particular analysis sample used in the estimation. Among the estimators we study, the difference-in-differences matching estimator performs the best. We attribute its performance to the fact that it eliminates potential sources of temporally invariant bias present in the NSW data, such as geographic mismatch between participants and nonparticipants and the use of a dependent variable measured in different ways for the two groups. Our analysis demonstrates that while propensity score matching is a potentially useful econometric tool, it does not represent a general solution to the evaluation problem.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/3VH8P2L6/A. Smith_E. Todd_2005_Does matching overcome LaLonde's critique of nonexperimental estimators.pdf;/Users/fpalomba/Zotero/storage/33QZAZ6L/S030440760400082X.html}
}

@article{abadie2010SyntheticControlMethods,
  title = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}: {{Estimating}} the {{Effect}} of {{California}}'s {{Tobacco Control Program}}},
  shorttitle = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}},
  author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
  year = {2010},
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {490},
  eprint = {29747059},
  eprinttype = {jstor},
  pages = {493--505},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  urldate = {2023-01-05},
  abstract = {Building on an idea in Abadie and Gardeazabal (2003), this article investigates the application of synthetic control methods to comparative case studies. We discuss the advantages of these methods and apply them to study the effects of Proposition 99, a large-scale tobacco control program that California implemented in 1988. We demonstrate that, following Proposition 99, tobacco consumption fell markedly in California relative to a comparable synthetic control region. We estimate that by the year 2000 annual per-capita cigarette sales in California were about 26 packs lower than what they would have been in the absence of Proposition 99. Using new inferential methods proposed in this article, we demonstrate the significance of our estimates. Given that many policy interventions and events of interest in social sciences take place at an aggregate level (countries, regions, cities, etc.) and affect a small number of aggregate units, the potential applicability of synthetic control methods to comparative case studies is very large, especially in situations where traditional regression methods are not appropriate.},
  file = {/Users/fpalomba/Zotero/storage/S2RFYVKY/Abadie et al_2010_Synthetic Control Methods for Comparative Case Studies.pdf}
}

@article{ackerberg2009NewUseImportance,
  title = {A New Use of Importance Sampling to Reduce Computational Burden in Simulation Estimation},
  author = {Ackerberg, Daniel A.},
  year = {2009},
  month = dec,
  journal = {QME},
  volume = {7},
  number = {4},
  pages = {343--376},
  issn = {1573-711X},
  doi = {10.1007/s11129-009-9074-z},
  urldate = {2023-03-16},
  abstract = {Simulation estimators (Lerman and Manski 1981; McFadden, Econometrica 57(5):995\textendash 1026, 1989; Pakes and Pollard, Econometrica 57:1027\textendash 1057, 1989) have been of great use to applied economists and marketers. They are simple and relatively easy to use, even for very complicated empirical models. That said, they can be computationally demanding, since these complicated models often need to be solved numerically, and these models need to be solved many times within an estimation procedure. This paper suggests methods that combine importance sampling techniques with changes-of-variables to address this caveat. These methods can dramatically reduce the number of times a particular model needs to be solved in an estimation procedure, significantly decreasing computational burden. The methods have other advantages as well, e.g. they can smooth otherwise non-smooth objective functions and can allow one to compute derivatives analytically. There are also caveats\textemdash if one is not careful, they can magnify simulation error. We illustrate with examples and a small Monte-Carlo study.},
  langid = {english},
  keywords = {C13,C16,C63,Importance sampling,Monte-Carlo study,Simulation estimators},
  file = {/Users/fpalomba/Zotero/storage/RIUP5RWK/Ackerberg - 2009 - A new use of importance sampling to reduce computa.pdf}
}

@misc{agarwal2022CausalInferenceCorrupted,
  title = {Causal {{Inference}} with {{Corrupted Data}}: {{Measurement Error}}, {{Missing Values}}, {{Discretization}}, and {{Differential Privacy}}},
  shorttitle = {Causal {{Inference}} with {{Corrupted Data}}},
  author = {Agarwal, Anish and Singh, Rahul},
  year = {2022},
  month = nov,
  number = {arXiv:2107.02780},
  eprint = {2107.02780},
  primaryclass = {cs, econ, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.02780},
  urldate = {2023-01-12},
  abstract = {The US Census Bureau will deliberately corrupt data sets derived from the 2020 US Census in an effort to maintain privacy, suggesting a painful trade-off between the privacy of respondents and the precision of economic analysis. To investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. We propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments, with a rate of \$n\^\{-1/2\}\$ for semiparametric estimands that degrades gracefully for nonparametric estimands. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and validate in the Census. In our analysis, we provide nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. Calibrated simulations verify the coverage of our data cleaning-adjusted confidence intervals and demonstrate the relevance of our results for 2020 Census data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,G.3,J.4,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/fpalomba/Zotero/storage/WFZU6T9K/Agarwal_Singh_2022_Causal Inference with Corrupted Data.pdf;/Users/fpalomba/Zotero/storage/LECI3SGH/2107.html}
}

@book{anderson1971StatisticalAnalysisTime,
  title = {The {{Statistical Analysis}} of {{Time Series}}},
  author = {Anderson, Theodore W.},
  year = {1971},
  publisher = {{John Wiley \& Sons}},
  abstract = {The Wiley Classics Library consists of selected books that havebecome recognized classics in their respective fields. With thesenew unabridged and inexpensive editions, Wiley hopes to extend thelife of these important works by making them available to futuregenerations of mathematicians and scientists. Currently availablein the Series: T. W. Anderson Statistical Analysis of Time SeriesT. S. Arthanari \& Yadolah Dodge Mathematical Programming inStatistics Emil Artin Geometric Algebra Norman T. J. Bailey TheElements of Stochastic Processes with Applications to the NaturalSciences George E. P. Box \& George C. Tiao Bayesian Inferencein Statistical Analysis R. W. Carter Simple Groups of Lie TypeWilliam G. Cochran \& Gertrude M. Cox Experimental Designs,Second Edition Richard Courant Differential and Integral Calculus,Volume I Richard Courant Differential and Integral Calculus, VolumeII Richard Courant \& D. Hilbert Methods of MathematicalPhysics, Volume I Richard Courant \& D. Hilbert Methods ofMathematical Physics, Volume II D. R. Cox Planning of ExperimentsHarold M. S. Coxeter Introduction to Modern Geometry, SecondEdition Charles W. Curtis \& Irving Reiner Representation Theoryof Finite Groups and Associative Algebras Charles W. Curtis \&Irving Reiner Methods of Representation Theory with Applications toFinite Groups and Orders, Volume I Charles W. Curtis \& IrvingReiner Methods of Representation Theory with Applications to FiniteGroups and Orders, Volume II Bruno de Finetti Theory ofProbability, Volume 1 Bruno de Finetti Theory of Probability,Volume 2 W. Edwards Deming Sample Design in Business Research Amosde Shalit \& Herman Feshbach Theoretical Nuclear Physics, Volume1 --Nuclear Structure J. L. Doob Stochastic Processes NelsonDunford \& Jacob T. Schwartz Linear Operators, Part One, GeneralTheory Nelson Dunford \& Jacob T. Schwartz Linear Operators,Part Two, Spectral Theory--Self Adjoint Operators in Hilbert SpaceNelson Dunford \& Jacob T. Schwartz Linear Operators, PartThree, Spectral Operators Herman Fsehbach Theoretical NuclearPhysics: Nuclear Reactions Bernard Friedman Lectures onApplications-Oriented Mathematics Gerald d. Hahn \& Samuel S.Shapiro Statistical Models in Engineering Morris H. Hansen, WilliamN. Hurwitz \& William G. Madow Sample Survey Methods and Theory,Volume I--Methods and Applications Morris H. Hansen, William N.Hurwitz \& William G. Madow Sample Survey Methods and Theory,Volume II--Theory Peter Henrici Applied and Computational ComplexAnalysis, Volume 1--Power Series--lntegration--ConformalMapping--Location of Zeros Peter Henrici Applied and ComputationalComplex Analysis, Volume 2--Special Functions--IntegralTransforms--Asymptotics--Continued Fractions Peter Henrici Appliedand Computational Complex Analysis, Volume 3--Discrete FourierAnalysis--Cauchy Integrals--Construction of ConformalMaps--Univalent Functions Peter Hilton \& Yel-Chiang Wu A Coursein Modern Algebra Harry Hochetadt Integral Equations Erwin O.Kreyezig Introductory Functional Analysis with Applications WilliamH. Louisell Quantum Statistical Properties of Radiation All HasanNayfeh Introduction to Perturbation Techniques Emanuel ParzenModern Probability Theory and Its Applications P.M. Prenter Splinesand Variational Methods Walter Rudin Fourier Analysis on Groups C.L. Siegel Topics in Complex Function Theory, Volume I--EllipticFunctions and Uniformization Theory C. L. Siegel Topics in ComplexFunction Theory, Volume II--Automorphic and Abelian integrals C. LSiegel Topics in Complex Function Theory, Volume III--AbelianFunctions \& Modular Functions of Several Variables J. J. StokerDifferential Geometry J. J. Stoker Water Waves: The MathematicalTheory with Applications J. J. Stoker Nonlinear Vibrations inMechanical and Electrical Systems},
  googlebooks = {rCOzXIC8ZLkC},
  isbn = {978-1-118-15039-9},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / Stochastic Processes,Mathematics / Probability \& Statistics / Time Series},
  file = {/Users/fpalomba/Zotero/storage/C3X73VBG/Anderson_1971_The Statistical Analysis of Time Series.pdf}
}

@article{angrist1998EstimatingLaborMarket,
  title = {Estimating the {{Labor Market Impact}} of {{Voluntary Military Service Using Social Security Data}} on {{Military Applicants}}},
  author = {Angrist, Joshua D.},
  year = {1998},
  journal = {Econometrica},
  volume = {66},
  number = {2},
  eprint = {2998558},
  eprinttype = {jstor},
  pages = {249--288},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/2998558},
  urldate = {2023-03-05},
  abstract = {The volunteer armed forces play a major role in the American youth labor market, but little is known about the effects of voluntary military service on earnings. The effects of military service are difficult to measure because veterans are both self-selected and screened by the military. This study uses two strategies to reduce selection bias in estimates of the effects of military service on the earnings of veterans. Both approaches involve the analysis of a special match of Social Security earning records to administrative data on applicants to the armed forces. The first strategy compares applicants who enlisted with applicants who did not enlist, while controlling for most of the characteristics used by the military to select soldiers from the applicant pool. This is implemented using matching methods and regression. The second strategy uses instrumental variables that were generated by an error in the scoring of the exams that screen military applicants. Estimates from both strategies are interpreted using models with heterogeneous potential outcomes. The empirical results suggest that soldiers who served in the early 1980s were paid considerably more than comparable civilians while in the military, and that military service is associated with higher employment rates for veterans after service. In spite of this employment gain, however, military service led to only a modest long-run increase in the civilian earnings of nonwhite veterans while actually reducing the civilian earnings of white veterans.},
  file = {/Users/fpalomba/Zotero/storage/JYUCSPRP/Angrist_1998_Estimating the Labor Market Impact of Voluntary Military Service Using Social.pdf}
}

@book{angrist2009MostlyHarmlessEconometrics,
  title = {Mostly {{Harmless Econometrics}}},
  author = {Angrist, Joshua and Pischke, J{\"o}rn-Steffen},
  year = {Sun, 01/04/2009 - 12:00},
  publisher = {{‎Princeton University Press}},
  urldate = {2023-03-05},
  isbn = {978-0-691-12035-5},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/F9K3L44D/2009_Mostly Harmless Econometrics.pdf;/Users/fpalomba/Zotero/storage/IW7ZPXZ9/mostly-harmless-econometrics.html}
}

@incollection{arellano2007UnderstandingBiasNonlineara,
  title = {Understanding {{Bias}} in {{Nonlinear Panel Models}}: {{Some Recent Developments}}},
  shorttitle = {Understanding {{Bias}} in {{Nonlinear Panel Models}}},
  booktitle = {Advances in {{Economics}} and {{Econometrics}}: {{Theory}} and {{Applications}}, {{Ninth World Congress}}},
  author = {Arellano, Manuel and Hahn, Jinyong},
  editor = {Blundell, Richard and Persson, Torsten and Newey, Whitney},
  year = {2007},
  series = {Econometric {{Society Monographs}}},
  volume = {3},
  pages = {381--409},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511607547.013},
  urldate = {2023-03-22},
  abstract = {INTRODUCTIONThe purpose of this paper is to review recently developed bias-adjusted methods of estimation of nonlinear panel data models with fixed effects. Standard estimators such as maximum likelihood estimators are usually inconsistent if the number of individuals n goes to infinity while the number of time periods T is held fixed. For some models, like static linear and logit regressions, there exist fixed-T consistent estimators as n\textrightarrow{$\infty$} (see, e.g., Andersen, 1970). Fixed T consistency is a desirable property because for many panels T is much smaller than n. However, these type of estimators are not available in general, and when they are, their properties do not normally extend to estimates of average marginal effects, which are often parameters of interest. Moreover, without auxiliary assumptions, the common parameters of certain nonlinear fixed effects models are simply unidentified in a fixed T setting, so that fixed-T consistent point estimation is not possible (see, e.g., Chamberlain, 1992). In other cases, although identifiable, fixed-T consistent estimation at the standard root-n rate is impossible (see, e.g., Honor\'e and Kyriazidou, 2000; Hahn, 2001).The number of periods available for many household, firm-level or country panels is such that it is not less natural to talk of time-series finite sample bias than of fixed-T inconsistency or underidentification. In this light, an alternative reaction to the fact that micro panels are short is to ask for approximately unbiased estimators as opposed to estimators with no bias at all.},
  isbn = {978-0-521-87154-9},
  file = {/Users/fpalomba/Zotero/storage/STA4MDDJ/Arellano_Hahn_2007_Understanding Bias in Nonlinear Panel Models.pdf;/Users/fpalomba/Zotero/storage/PCKZWCW5/3512BD2D5CDD8A37A53C61CE8CE4DB41.html}
}

@misc{arkhangelsky2022RolePropensityScore,
  title = {The {{Role}} of the {{Propensity Score}} in {{Fixed Effect Models}}},
  author = {Arkhangelsky, Dmitry and Imbens, Guido},
  year = {2022},
  month = apr,
  number = {arXiv:1807.02099},
  eprint = {1807.02099},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.02099},
  urldate = {2023-01-04},
  abstract = {We develop a new approach for estimating average treatment effects in observational studies with unobserved group-level heterogeneity. We consider a general model with group-level unconfoundedness and provide conditions under which aggregate statistics -- group-level averages of functions of treatments and covariates -- are sufficient to eliminate differences between groups. Building on these results, we generalize commonly used linear fixed-effect regression estimators in three ways. First, we allow researchers to explicitly select sufficient statistics themselves, whereas the standard specifications make this choice implicit. Second, we suggest using flexible adjustments for sufficient statistics. Finally, we propose robustifying the regression estimators using inverse propensity score weighting. In practice, we recommend researchers use all three modifications.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/UA3KHAS3/Arkhangelsky_Imbens_2022_The Role of the Propensity Score in Fixed Effect Models.pdf;/Users/fpalomba/Zotero/storage/VF5IA92Y/1807.html}
}

@article{bardet2008DependentLindebergCentral,
  title = {Dependent {{Lindeberg}} Central Limit Theorem and Some Applications},
  author = {Bardet, Jean-Marc and Doukhan, Paul and Lang, Gabriel and Ragache, Nicolas},
  year = {2008},
  journal = {ESAIM: Probability and Statistics},
  volume = {12},
  pages = {154--172},
  publisher = {{EDP Sciences}},
  issn = {1292-8100, 1262-3318},
  doi = {10.1051/ps:2007053},
  urldate = {2023-03-17},
  abstract = {In this paper, a very useful lemma (in two versions) is proved: it simplifies notably the essential step to establish a Lindeberg central limit theorem for dependent processes. Then, applying this lemma to weakly dependent processes introduced in Doukhan and Louhichi (1999), a new central limit theorem is obtained for sample mean or kernel density estimator. Moreover, by using the subsampling, extensions under weaker assumptions of these central limit theorems are provided. All the usual causal or non causal time series: Gaussian, associated, linear, ARCH(\emph{{$\infty<$}i/{$>$}), bilinear, Volterra processes, ..., enter this frame.}},
  copyright = {\textcopyright{} EDP Sciences, SMAI, 2008},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/T2MS6GVM/Bardet et al. - 2008 - Dependent Lindeberg central limit theorem and some.pdf}
}

@article{beggs1981AssessingPotentialDemand,
  title = {Assessing the Potential Demand for Electric Cars},
  author = {Beggs, S and Cardell, S and Hausman, J},
  year = {1981},
  month = sep,
  journal = {Journal of Econometrics},
  volume = {17},
  number = {1},
  pages = {1--19},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(81)90056-7},
  urldate = {2023-03-09},
  abstract = {An ordered logit specification for use on ranked individual data is used to analyze survey data on potential consumer demand for electric cars. In many situations in economics and marketing we would like to be able to forecast consumer demands for goods which have not yet appeared in actual markets. By defining goods as a bundle of underlying attributes, we can use discrete choice models to estimate consumer evaluations. Then new good demand is forecast by use of the estimated coefficients to compare consumer evaluation of the new good to existing choices. When ranked individual data are available, we can estimate separate coefficients for each individual rather than assuming identical coefficients as is usual with logit models. Our results indicate considerable dispersion in individual coefficients. This finding can have important implications for new product analysis.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CFG8C86J/Beggs et al_1981_Assessing the potential demand for electric cars.pdf;/Users/fpalomba/Zotero/storage/9PZWBN6Y/0304407681900567.html}
}

@article{beltrato1987DeterminingBandwidthKernela,
  title = {Determining the {{Bandwidth}} of a {{Kernel Spectrum Estimate}}},
  author = {BeltraTo, Kaiz{\^o} I. and Bloomfield, Peter},
  year = {1987},
  journal = {Journal of Time Series Analysis},
  volume = {8},
  number = {1},
  pages = {21--38},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1987.tb00418.x},
  urldate = {2023-02-23},
  abstract = {Abstract. A cross-validated form of Whittle's frequency domain approximation to the likelihood function of a stationary Gaussian process is described, and proposed as a criterion for choosing the bandwidth in a kernel spectrum estimate. The criterion is shown to be equivalent, in large samples, to the mean integrated squared error. The statistical properties of the spectrum estimate whose bandwidth maximizes the criterion have been explored in a limited simulation.},
  langid = {english},
  keywords = {cross-validated likelihood,Density estimation,smoothing parameter},
  file = {/Users/fpalomba/Zotero/storage/U5LLHT43/BeltraTo_Bloomfield_1987_Determining the Bandwidth of a Kernel Spectrum Estimate.pdf;/Users/fpalomba/Zotero/storage/7A3CRS5V/j.1467-9892.1987.tb00418.html}
}

@article{berk1974ConsistentAutoregressiveSpectral,
  title = {Consistent {{Autoregressive Spectral Estimates}}},
  author = {Berk, Kenneth N.},
  year = {1974},
  journal = {The Annals of Statistics},
  volume = {2},
  number = {3},
  eprint = {2958135},
  eprinttype = {jstor},
  pages = {489--502},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  urldate = {2023-05-22},
  abstract = {We consider an autoregressive linear process \{xt\}, a one-sided moving average, with summable coefficients, of independent identically distributed variables \{et\} with zero mean and fourth moment, such that \{et\} is expressible in terms of past values of \{xt\}. The spectral density of \{xt\} is assumed bounded and bounded away from zero. Using data x1,{$\cdots$}, xn from the process, we fit an autoregression of order k, where k3/n \textrightarrow{} 0 as n \textrightarrow{} {$\infty$}. Assuming the order k is asymptotically sufficient to overcome bias, the autoregression yields a consistent estimator of the spectral density of \{xt\}. Furthermore, assuming k goes to infinity so that the bias from using a finite autoregression vanishes at a sufficient rate, the autoregressive spectral estimates are asymptotically normal, uncorrelated at different fixed frequencies. The asymptotic variance is the same as for spectral estimates based on a truncated periodogram.},
  file = {/Users/fpalomba/Zotero/storage/SSGS7QJC/Berk_1974_Consistent Autoregressive Spectral Estimates.pdf}
}

@article{bickel1999NewMixingNotion,
  title = {A New Mixing Notion and Functional Central Limit Theorems for a Sieve Bootstrap in Time Series},
  author = {Bickel, Peter J. and B{\"u}hlmann, Peter},
  year = {1999},
  month = jun,
  journal = {Bernoulli},
  volume = {5},
  number = {3},
  pages = {413--446},
  publisher = {{Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  urldate = {2023-03-21},
  abstract = {We study a bootstrap method for stationary real-valued time series, which is based on the sieve of autoregressive processes. Given a sample X 1 ,...,X n from a linear process \{ X t\} t {$\in\mathbb{Z}$} , we approximate the underlying process by an autoregressive model with order p =p(n) , where p (n)\textrightarrow{$\infty$},p(n)=o(n) as the sample size n \textrightarrow{$\infty$} . Based on such a model, a bootstrap process \{ X t *\} t {$\in\mathbb{Z}$} is constructed from which one can draw samples of any size. We show that, with high probability, such a sieve bootstrap process \{ X t *\} t {$\in\mathbb{Z}$} satisfies a new type of mixing condition. This implies that many results for stationary mixing sequences carry over to the sieve bootstrap process. As an example we derive a functional central limit theorem under a bracketing condition.},
  keywords = {AR({$\infty$}),ARMA,autoregressive approximation,bracketing,convex sets,linear process,MA({$\infty$}),smooth bootstrap,stationary process,strong-mixing},
  file = {/Users/fpalomba/Zotero/storage/VBERTEN7/Bickel_Bühlmann_1999_A new mixing notion and functional central limit theorems for a sieve bootstrap.pdf}
}

@book{billingsley1968ConvergenceProbabilityMeasures,
  title = {Convergence of {{Probability Measures}}},
  author = {Billingsley, Patrick},
  year = {1968},
  month = jan,
  publisher = {{Wiley}},
  abstract = {A new look at weak-convergence methods in metric spaces-from a master of probability theory In this new edition, Patrick Billingsley updates his classic work Convergence of Probability Measures to reflect developments of the past thirty years. Widely known for his straightforward approach and reader-friendly style, Dr. Billingsley presents a clear, precise, up-to-date account of probability limit theory in metric spaces. He incorporates many examples and applications that illustrate the power and utility of this theory in a range of disciplines-from analysis and number theory to statistics, engineering, economics, and population biology. With an emphasis on the simplicity of the mathematics and smooth transitions between topics, the Second Edition boasts major revisions of the sections on dependent random variables as well as new sections on relative measure, on lacunary trigonometric series, and on the Poisson-Dirichlet distribution as a description of the long cycles in permutations and the large divisors of integers. Assuming only standard measure-theoretic probability and metric-space topology, Convergence of Probability Measures provides statisticians and mathematicians with basic tools of probability theory as well as a springboard to the "industrial-strength" literature available today.},
  googlebooks = {O9oQAQAAIAAJ},
  isbn = {978-0-471-07242-3},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/Users/fpalomba/Zotero/storage/MTMABFZE/Billingsley_1968_Convergence of Probability Measures.pdf}
}

@misc{blandhol2022WhenTSLSActually,
  type = {Working {{Paper}}},
  title = {When Is {{TSLS Actually LATE}}?},
  author = {Blandhol, Christine and Bonney, John and Mogstad, Magne and Torgovitsky, Alexander},
  year = {2022},
  month = jan,
  series = {Working {{Paper Series}}},
  number = {29709},
  eprint = {29709},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w29709},
  urldate = {2023-03-07},
  abstract = {Linear instrumental variable estimators, such as two-stage least squares (TSLS), are commonly interpreted as estimating positively weighted averages of causal effects, referred to as local average treatment effects (LATEs). We examine whether the LATE interpretation actually applies to the types of TSLS specifications that are used in practice. We show that if the specification includes covariates\textemdash which most empirical work does\textemdash then the LATE interpretation does not apply in general. Instead, the TSLS estimator will, in general, reflect treatment effects for both compliers and always/nevertakers, and some of the treatment effects for the always/never-takers will necessarily be negatively weighted. We show that the only specifications that have a LATE interpretation are "saturated" specifications that control for covariates nonparametrically, implying that such specifications are both sufficient and necessary for TSLS to have a LATE interpretation, at least without additional parametric assumptions. This result is concerning because, as we document, empirical researchers almost never control for covariates nonparametrically, and rarely discuss or justify parametric specifications of covariates. We develop a decomposition that quantifies the extent to which the usual LATE interpretation fails. We apply the decomposition to four empirical analyses and find strong evidence that the LATE interpretation of TSLS is far from accurate for the types of specifications actually used in practice.},
  archiveprefix = {National Bureau of Economic Research},
  file = {/Users/fpalomba/Zotero/storage/4INPD5BU/Blandhol et al_2022_When is TSLS Actually LATE.pdf}
}

@book{brillinger1981TimeSeriesData,
  title = {Time {{Series}}: {{Data Analysis}} and {{Theory}}},
  shorttitle = {Time {{Series}}},
  author = {Brillinger, David R.},
  year = {1981},
  publisher = {{Holden-Day}},
  abstract = {The nature of time series and their frequency analysis. Foundations. Analytic properties of fourier transforms and complex matrices. Stochastic properties of finite fourier transforms. The estimation of power spectra. Analysis of a linear time invariant relation between a stochastic series and several deterministic series. Estimating the second-order spectra of vector-valued series. Analysis of a linear time invariant between two vector-valued stochastic series. Principal components in the frequency domain. The canonical analysis of time series. Proofs of theorems.},
  googlebooks = {iOdQAAAAMAAJ},
  isbn = {978-0-8162-1150-0},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/C6DJ43RG/Brillinger_1981_Time Series.pdf}
}

@book{brockwell1991TimeSeriesTheory,
  title = {Time {{Series}}: {{Theory}} and {{Methods}}},
  shorttitle = {Time {{Series}}},
  author = {Brockwell, Peter J. and Davis, Richard A.},
  year = {1991},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This edition contains a large number of additions and corrections scattered throughout the text, including the incorporation of a new chapter on state-space models. The companion diskette for the IBM PC has expanded into the software package ITSM: An Interactive Time Series Modelling Package for the PC, which includes a manual and can be ordered from Springer-Verlag. * We are indebted to many readers who have used the book and programs and made suggestions for improvements. Unfortunately there is not enough space to acknowledge all who have contributed in this way; however, special mention must be made of our prize-winning fault-finders, Sid Resnick and F. Pukelsheim. Special mention should also be made of Anthony Brockwell, whose advice and support on computing matters was invaluable in the preparation of the new diskettes. We have been fortunate to work on the new edition in the excellent environments provided by the University of Melbourne and Colorado State University. We thank Duane Boes particularly for his support and encouragement throughout, and the Australian Research Council and National Science Foundation for their support of research related to the new material. We are also indebted to Springer-Verlag for their constant support and assistance in preparing the second edition. Fort Collins, Colorado P. J. BROCKWELL November, 1990 R. A. DAVIS * /TSM: An Interactive Time Series Modelling Package for the PC by P. J. Brockwell and R. A. Davis. ISBN: 0-387-97482-2; 1991.},
  googlebooks = {TVIpBgAAQBAJ},
  isbn = {978-1-4419-0320-4},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/Users/fpalomba/Zotero/storage/24LNBKRP/Brockwell_Davis_2009_Time Series.pdf}
}

@article{buhlmann1996LocallyAdaptiveLagWindow,
  title = {Locally {{Adaptive Lag-Window Spectral Estimation}}},
  author = {B{\"u}hlmann, Peter},
  year = {1996},
  journal = {Journal of Time Series Analysis},
  volume = {17},
  number = {3},
  pages = {247--270},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1996.tb00275.x},
  urldate = {2023-02-24},
  abstract = {Abstract. We propose a procedure for the locally optimal window width in nonparametric spectral estimation, minimizing the asymptotic mean square error at a fixed frequency {$\Lambda$} of a lag-window estimator. Our approach is based on an iterative plug-in scheme. Besides the estimation of a spectral density at a fixed frequency, e.g. at frequency {$\Lambda$}= 0, our procedure allows to perform nonparametric spectral estimation with variable window width which adapts to the smoothness of the true underlying density.},
  langid = {english},
  keywords = {Bandwidth,iterative plug-in,nonparametric spectral estimation,strong-mixing sequence,time series,window width},
  file = {/Users/fpalomba/Zotero/storage/WLJC6K3Q/Bühlmann_1996_Locally Adaptive Lag-Window Spectral Estimation.pdf;/Users/fpalomba/Zotero/storage/WK8QQGLR/j.1467-9892.1996.tb00275.html}
}

@misc{callaway2021DifferenceinDifferencesContinuousTreatmenta,
  title = {Difference-in-{{Differences}} with a {{Continuous Treatment}}},
  author = {Callaway, Brantly and {Goodman-Bacon}, Andrew and Sant'Anna, Pedro H. C.},
  year = {2021},
  month = jul,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2107.02637},
  urldate = {2023-02-21},
  abstract = {This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work.},
  howpublished = {https://arxiv.org/abs/2107.02637v2},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CYD5PTSA/Callaway et al_2021_Difference-in-Differences with a Continuous Treatment.pdf}
}

@article{callaway2021DifferenceinDifferencesMultipleTimea,
  title = {Difference-in-{{Differences}} with Multiple Time Periods},
  author = {Callaway, Brantly and Sant'Anna, Pedro H.C.},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {200--230},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.12.001},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/WT64EGPK/Callaway_Sant’Anna_2021_Difference-in-Differences with multiple time periods.pdf}
}

@article{calonico2019RegressionDiscontinuityDesigns,
  title = {Regression {{Discontinuity Designs Using Covariates}}},
  author = {Calonico, Sebastian and Cattaneo, Matias D. and Farrell, Max H. and Titiunik, Roc{\'i}o},
  year = {2019},
  month = jul,
  journal = {The Review of Economics and Statistics},
  volume = {101},
  number = {3},
  pages = {442--451},
  issn = {0034-6535},
  doi = {10.1162/rest_a_00760},
  urldate = {2023-04-15},
  abstract = {We study regression discontinuity designs when covariates are included in the estimation. We examine local polynomial estimators that include discrete or continuous covariates in an additive separable way, but without imposing any parametric restrictions on the underlying population regression functions. We recommend a covariate-adjustment approach that retains consistency under intuitive conditions and characterize the potential for estimation and inference improvements. We also present new covariate-adjusted mean-squared error expansions and robust bias-corrected inference procedures, with heteroskedasticity-consistent and cluster-robust standard errors. We provide an empirical illustration and an extensive simulation study. All methods are implemented in R and Stata software packages.},
  file = {/Users/fpalomba/Zotero/storage/8CD6LJ99/Calonico-Cattaneo-Farrell-Titiunik_2019_RESTAT--Supplement.pdf;/Users/fpalomba/Zotero/storage/X96HDG56/Calonico et al. - 2019 - Regression Discontinuity Designs Using Covariates.pdf}
}

@book{cameron2005MicroeconometricsMethodsApplications,
  title = {Microeconometrics: Methods and Applications},
  shorttitle = {Microeconometrics},
  author = {Cameron, A. Colin and Trivedi, Pravin K.},
  year = {2005},
  publisher = {{Cambridge university press}},
  file = {/Users/fpalomba/Zotero/storage/4GQDWTVQ/Cameron_Trivedi_2005_Microeconometrics.pdf;/Users/fpalomba/Zotero/storage/BMSBCDR6/Cameron and Trivedi (2005) - Microeconometrics Methods and applications.pdf;/Users/fpalomba/Zotero/storage/2JGRGM42/books.html}
}

@article{cattaneo2013OptimalConvergenceRates,
  title = {Optimal Convergence Rates, {{Bahadur}} Representation, and Asymptotic Normality of Partitioning Estimators},
  author = {Cattaneo, Matias D. and Farrell, Max H.},
  year = {2013},
  month = jun,
  journal = {Journal of Econometrics},
  volume = {174},
  number = {2},
  pages = {127--143},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2013.02.002},
  urldate = {2023-04-18},
  abstract = {This paper studies the asymptotic properties of partitioning estimators of the conditional expectation function and its derivatives. Mean-square and uniform convergence rates are established and shown to be optimal under simple and intuitive conditions. The uniform rate explicitly accounts for the effect of moment assumptions, which is useful in semiparametric inference. A general asymptotic integrated mean-square error approximation is obtained and used to derive an optimal plug-in tuning parameter selector. A uniform Bahadur representation is developed for linear functionals of the estimator. Using this representation, asymptotic normality is established, along with consistency of a standard-error estimator. The finite-sample performance of the partitioning estimator is examined and compared to other nonparametric techniques in an extensive simulation study.},
  langid = {english},
  keywords = {Asymptotic normality,Bahadur representation,Convergence rates,Nonparametric estimation,Partitioning,Subclassification},
  file = {/Users/fpalomba/Zotero/storage/5MUFY6XR/Cattaneo and Farrell - 2013 - Optimal convergence rates, Bahadur representation,.pdf;/Users/fpalomba/Zotero/storage/CDHJ2GD6/Cattaneo-Farrell_2013_JoE--Supplemental.pdf;/Users/fpalomba/Zotero/storage/TY3IPCQP/S0304407613000365.html}
}

@article{chetverikov2018EconometricsShapeRestrictions,
  title = {The {{Econometrics}} of {{Shape Restrictions}}},
  author = {Chetverikov, Denis and Santos, Andres and Shaikh, Azeem M.},
  year = {2018},
  journal = {Annual Review of Economics},
  volume = {10},
  number = {1},
  pages = {31--63},
  doi = {10.1146/annurev-economics-080217-053417},
  urldate = {2022-12-16},
  abstract = {We review recent developments in the econometrics of shape restrictions and their role in applied work. Our objectives are threefold. First, we aim to emphasize the diversity of applications in which shape restrictions have played a fruitful role. Second, we intend to provide practitioners with an intuitive understanding of how shape restrictions impact the distribution of estimators and test statistics. Third, we aim to provide an overview of new advances in the theory of estimation and inference under shape restrictions. Throughout the review, we outline open questions and interesting directions for future research.},
  keywords = {irregular models,shape restrictions,uniformity},
  file = {/Users/fpalomba/Zotero/storage/G69X7GR5/Chetverikov et al. - 2018 - The Econometrics of Shape Restrictions.pdf}
}

@article{dawid1979ConditionalIndependenceStatistical,
  title = {Conditional {{Independence}} in {{Statistical Theory}}},
  author = {Dawid, A. P.},
  year = {1979},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {41},
  number = {1},
  eprint = {2984718},
  eprinttype = {jstor},
  pages = {1--31},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-05-17},
  abstract = {Some simple heuristic properties of conditional independence are shown to form a conceptual framework for much of the theory of statistical inference. This framework is illustrated by an examination of the role of conditional independence in several diverse areas of the field of statistics. Topics covered include sufficiency and ancillarity, parameter identification, causal inference, prediction sufficiency, data selection mechanisms, invariant statistical models and a subjectivist approach to model-building.},
  file = {/Users/fpalomba/Zotero/storage/YEJ3DF37/Dawid_1979_Conditional Independence in Statistical Theory.pdf}
}

@article{debreu1960ReviewRDLuce,
  title = {Review of {{RD Luce}}, {{Individual}} Choice Behavior: {{A}} Theoretical Analysis},
  shorttitle = {Review of {{RD Luce}}, {{Individual}} Choice Behavior},
  author = {Debreu, Gerard},
  year = {1960},
  journal = {American Economic Review},
  volume = {50},
  number = {1},
  pages = {186--188},
  file = {/Users/fpalomba/Zotero/storage/3IUYXA32/openurl.html}
}

@article{dechaisemartin2018FuzzyDifferencesinDifferences,
  title = {Fuzzy {{Differences-in-Differences}}},
  author = {De Chaisemartin, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2018},
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {85},
  number = {2},
  pages = {999--1028},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdx049},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/ACFGJV2T/de Chaisemartin_D’HaultfŒuille_2018_Fuzzy Differences-in-Differences.pdf}
}

@article{dechaisemartin2020TwoWayFixedEffects,
  title = {Two-{{Way Fixed Effects Estimators}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2020},
  month = sep,
  journal = {American Economic Review},
  volume = {110},
  number = {9},
  pages = {2964--2996},
  issn = {0002-8282},
  doi = {10.1257/aer.20181169},
  urldate = {2023-02-06},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/RZFV8L2K/de Chaisemartin_D’Haultfœuille_2020_Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.pdf}
}

@techreport{dechaisemartin2022DifferenceinDifferencesEstimatorsIntertemporal,
  title = {Difference-in-{{Differences Estimators}} of {{Intertemporal Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = mar,
  number = {w29873},
  pages = {w29873},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29873},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/JHQAVFWS/de Chaisemartin_D'Haultfoeuille_2022_Difference-in-Differences Estimators of Intertemporal Treatment Effects.pdf}
}

@techreport{dechaisemartin2022TwoWayFixedEffects,
  title = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}: {{A Survey}}},
  shorttitle = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = jan,
  number = {w29691},
  pages = {w29691},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29691},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/JIVFFN55/de Chaisemartin_D'Haultfoeuille_2022_Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous.pdf}
}

@techreport{dechaisemartin2022TwowayFixedEffectsa,
  title = {Two-Way {{Fixed Effects}} and {{Differences-in-Differences Estimators}} with {{Several Treatments}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = oct,
  number = {w30564},
  pages = {w30564},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w30564},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/8PND2X32/de Chaisemartin_D'Haultfoeuille_2022_Two-way Fixed Effects and Differences-in-Differences Estimators with Several.pdf}
}

@article{delgado2015NonnestedTestingSpatial,
  title = {Non-Nested Testing of Spatial Correlation},
  author = {Delgado, Miguel A. and Robinson, Peter M.},
  year = {2015},
  month = jul,
  journal = {Journal of Econometrics},
  volume = {187},
  number = {1},
  pages = {385--401},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2015.02.044},
  urldate = {2023-05-15},
  abstract = {We develop non-nested tests in a general spatial, spatio-temporal or panel data context. The spatial aspect can be interpreted quite generally, in either a geographical sense, or employing notions of economic distance, or when parametric modelling arises in part from a common factor or other structure. In the former case, observations may be regularly-spaced across one or more dimensions, as is typical with much spatio-temporal data, or irregularly-spaced across all dimensions; both isotropic models and non-isotropic models can be considered, and a wide variety of correlation structures. In the second case, models involving spatial weight matrices are covered, such as ``spatial autoregressive models''. The setting is sufficiently general to potentially cover other parametric structures such as certain factor models, and vector-valued observations, and here our preliminary asymptotic theory for parameter estimates is of some independent value. The test statistic is based on a Gaussian pseudo-likelihood ratio, and is shown to have an asymptotic standard normal distribution under the null hypothesis that one of the two models is correct; this limit theory rests strongly on a central limit theorem for the Gaussian pseudo-maximum likelihood parameter estimates. A small Monte Carlo study of finite-sample performance is included.},
  langid = {english},
  keywords = {Non-nested test,Pseudo maximum likelihood estimation,Spatial correlation},
  file = {/Users/fpalomba/Zotero/storage/TILPBUIR/Delgado_Robinson_2015_Non-nested testing of spatial correlation.pdf;/Users/fpalomba/Zotero/storage/GJQ3C77C/S0304407615000950.html}
}

@misc{dorn2021SharpSensitivityAnalysis,
  title = {Sharp {{Sensitivity Analysis}} for {{Inverse Propensity Weighting}} via {{Quantile Balancing}}},
  author = {Dorn, Jacob and Guo, Kevin},
  year = {2021},
  month = apr,
  number = {arXiv:2102.04543},
  eprint = {2102.04543},
  primaryclass = {econ, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.04543},
  urldate = {2022-12-16},
  abstract = {Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This paper introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya (2019), which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan (2006)'s marginal sensitivity model.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/AJCTSKDU/Dorn and Guo - 2021 - Sharp Sensitivity Analysis for Inverse Propensity .pdf;/Users/fpalomba/Zotero/storage/ZYNR63DT/2102.html}
}

@article{doukhan1999NewWeakDependence,
  title = {A New Weak Dependence Condition and Applications to Moment Inequalities},
  author = {Doukhan, Paul and Louhichi, Sana},
  year = {1999},
  month = dec,
  journal = {Stochastic Processes and their Applications},
  volume = {84},
  number = {2},
  pages = {313--342},
  issn = {0304-4149},
  doi = {10.1016/S0304-4149(99)00055-1},
  urldate = {2023-03-21},
  abstract = {The purpose of this paper is to propose a unifying weak dependence condition. Mixing sequences, functions of associated or Gaussian sequences, Bernoulli shifts as well as models with a Markovian representation are examples of the models considered. We establish Marcinkiewicz\textendash Zygmund, Rosenthal and exponential inequalities for general sequences of centered random variables. Inequalities are stated in terms of the decay rate for the covariance of products of the initial random variables subject to the condition that the gap of time between both products tends to infinity. As applications of those notions, we obtain a version of the functional CLT and an invariance principle for the empirical process},
  langid = {english},
  keywords = {Central Limit Theorem,Inequalities,Mixing,Positive dependence,Rosenthal inequality,Stationary sequences},
  file = {/Users/fpalomba/Zotero/storage/P6P7LLYI/Doukhan_Louhichi_1999_A new weak dependence condition and applications to moment inequalities.pdf;/Users/fpalomba/Zotero/storage/BZ2U3IGS/S0304414999000551.html}
}

@article{fan1992VariableBandwidthLocal,
  title = {Variable {{Bandwidth}} and {{Local Linear Regression Smoothers}}},
  author = {Fan, Jianqing and Gijbels, Irene},
  year = {1992},
  month = dec,
  journal = {The Annals of Statistics},
  volume = {20},
  number = {4},
  pages = {2008--2036},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176348900},
  urldate = {2023-04-12},
  abstract = {In this paper we introduce an appealing nonparametric method for estimating the mean regression function. The proposed method combines the ideas of local linear smoothers and variable bandwidth. Hence, it also inherits the advantages of both approaches. We give expressions for the conditional MSE and MISE of the estimator. Minimization of the MISE leads to an explicit formula for an optimal choice of the variable bandwidth. Moreover, the merits of considering a variable bandwidth are discussed. In addition, we show that the estimator does not have boundary effects, and hence does not require modifications at the boundary. The performance of a corresponding plug-in estimator is investigated. Simulations illustrate the proposed estimation method.},
  keywords = {62G07,62G20,62J99,boundary effects,local linear smoother,mean squared error,Nonparametric regression,optimalities,variable bandwidth},
  file = {/Users/fpalomba/Zotero/storage/XETZMEJT/Fan_Gijbels_1992_Variable Bandwidth and Local Linear Regression Smoothers.pdf}
}

@book{fan1996LocalPolynomialModelling,
  title = {Local {{Polynomial Modelling}} and {{Its Applications}}: {{Monographs}} on {{Statistics}} and {{Applied Probability}} 66},
  shorttitle = {Local {{Polynomial Modelling}} and {{Its Applications}}},
  author = {Fan, Jianqing and Gijbels, Ir{\`e}ne},
  year = {1996},
  month = jan,
  publisher = {{Routledge}},
  address = {{New York}},
  doi = {10.1201/9780203748725},
  abstract = {Data-analytic approaches to regression problems, arising from many scientific disciplines are described in this book. The aim of these nonparametric methods is to relax assumptions on the form of a regression function and to let data search for a suitable function that describes the data well. The use of these nonparametric functions with parametric techniques can yield very powerful data analysis tools. Local polynomial modeling and its applications provides an up-to-date picture on state-of-the-art nonparametric regression techniques. The emphasis of the book is on methodologies rather than on theory, with a particular focus on applications of nonparametric techniques to various statistical problems. High-dimensional data-analytic tools are presented, and the book includes a variety of examples. This will be a valuable reference for research and applied statisticians, and will serve as a textbook for graduate students and others interested in nonparametric regression.},
  isbn = {978-0-203-74872-5},
  file = {/Users/fpalomba/Zotero/storage/XIGHBEGQ/Fan_1996_Local Polynomial Modelling and Its Applications.pdf}
}

@article{fan1998AutomaticLocalSmoothing,
  title = {Automatic {{Local Smoothing}} for {{Spectral Density Estimation}}},
  author = {Fan, Jianqing and Kreutzberger, Eva},
  year = {1998},
  journal = {Scandinavian Journal of Statistics},
  volume = {25},
  number = {2},
  pages = {359--369},
  issn = {1467-9469},
  doi = {10.1111/1467-9469.00109},
  urldate = {2023-02-23},
  abstract = {This article uses local polynomial techniques to fit Whittle's likelihood for spectral density estimation. Asymptotic sampling properties of the proposed estimators are derived, and adaptation of the proposed estimator to the boundary effect is demonstrated. We show that the Whittle likelihood-based estimator has advantages over the least-squares based log-periodogram. The bandwidth for the Whittle likelihood-based method is chosen by a simple adjustment of a bandwidth selector proposed in Fan \& Gijbels (1995). The effectiveness of the proposed procedure is demonstrated by a few simulated and real numerical examples. Our simulation results support the asymptotic theory that the likelihood based spectral density and log-spectral density estimators are the most appealing among their peers},
  langid = {english},
  keywords = {bandwidth selection,local polynomial fit,periodogram,spectral density estimation,Whittle likelihood},
  file = {/Users/fpalomba/Zotero/storage/ZVQE5FBW/Fan_Kreutzberger_1998_Automatic Local Smoothing for Spectral Density Estimation.pdf;/Users/fpalomba/Zotero/storage/HDLSUMVT/1467-9469.html}
}

@misc{fang2021InferenceLargeScaleLinear,
  title = {Inference for {{Large-Scale Linear Systems}} with {{Known Coefficients}}},
  author = {Fang, Zheng and Santos, Andres and Shaikh, Azeem M. and Torgovitsky, Alexander},
  year = {2021},
  month = sep,
  number = {arXiv:2009.08568},
  eprint = {2009.08568},
  primaryclass = {econ},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.08568},
  urldate = {2022-12-16},
  abstract = {This paper considers the problem of testing whether there exists a non-negative solution to a possibly under-determined system of linear equations with known coefficients. This hypothesis testing problem arises naturally in a number of settings, including random coefficient, treatment effect, and discrete choice models, as well as a class of linear programming problems. As a first contribution, we obtain a novel geometric characterization of the null hypothesis in terms of identified parameters satisfying an infinite set of inequality restrictions. Using this characterization, we devise a test that requires solving only linear programs for its implementation, and thus remains computationally feasible in the high-dimensional applications that motivate our analysis. The asymptotic size of the proposed test is shown to equal at most the nominal level uniformly over a large class of distributions that permits the number of linear equations to grow with the sample size.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics},
  file = {/Users/fpalomba/Zotero/storage/JZF9S2CN/Fang et al. - 2021 - Inference for Large-Scale Linear Systems with Know.pdf;/Users/fpalomba/Zotero/storage/N2NNBIB3/2009.html}
}

@article{fang2021ProjectionFrameworkTesting,
  title = {A {{Projection Framework}} for {{Testing Shape Restrictions That Form Convex Cones}}},
  author = {Fang, Zheng and Seo, Juwon},
  year = {2021},
  journal = {Econometrica},
  volume = {89},
  number = {5},
  pages = {2439--2458},
  issn = {1468-0262},
  doi = {10.3982/ECTA17764},
  urldate = {2022-12-16},
  abstract = {This paper develops a uniformly valid and asymptotically nonconservative test based on projection for a class of shape restrictions. The key insight we exploit is that these restrictions form convex cones, a simple and yet elegant structure that has been barely harnessed in the literature. Based on a monotonicity property afforded by such a geometric structure, we construct a bootstrap procedure that, unlike many studies in nonstandard settings, dispenses with estimation of local parameter spaces, and the critical values are obtained in a way as simple as computing the test statistic. Moreover, by appealing to strong approximations, our framework accommodates nonparametric regression models as well as distributional/density-related and structural settings. Since the test entails a tuning parameter (due to the nonstandard nature of the problem), we propose a data-driven choice and prove its validity. Monte Carlo simulations confirm that our test works well.},
  langid = {english},
  keywords = {convex cone,Nonstandard inference,projection,shape restrictions,strong approximations},
  file = {/Users/fpalomba/Zotero/storage/BYXI6UVI/Fang and Seo - 2021 - A Projection Framework for Testing Shape Restricti.pdf;/Users/fpalomba/Zotero/storage/R3IAC2NQ/ECTA17764.html}
}

@techreport{fortin1999OptimalBandwidthSelection,
  type = {Working {{Paper}}},
  title = {Optimal Bandwidth Selection in Non-Parametric Spectral Density Estimation: {{Review}} and Simulation},
  shorttitle = {Optimal Bandwidth Selection in Non-Parametric Spectral Density Estimation},
  author = {Fortin, Ines and Kuzmics, Christoph},
  year = {1999},
  number = {62},
  institution = {{Reihe \"Okonomie / Economics Series}},
  urldate = {2023-02-23},
  abstract = {This paper deals with optimal window width choice in non-parametric lag- or spectral window estimation of the spectral density of a stationary zero-mean process. Several approaches are reviewed: the cross-validation based methods described by Hurvich (1985), Beltrao \& Bloomfield (1987) and Hurvich \& Beltrao (1990), an iterative procedure due to Buehlmann (1996), and a bootstrap approach followed by Franke \& Haerdle (1992). These methods are compared in terms of the mean square error, the mean square percentage error, and a third measure of distance between the true spectral density and its estimate. The comparison is based on a small simulation study. The processes that are simulated are in the class of ARMA (5,5) processes. Based on the simulation evidence, we suggest to use a slightly modified version of Buehlmann's (1996) iterative method.},
  copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/7GL9RYXM/Fortin_Kuzmics_1999_Optimal bandwidth selection in non-parametric spectral density estimation.pdf}
}

@techreport{freyaldenhoven2021VisualizationIdentificationEstimation,
  title = {Visualization, {{Identification}}, and {{Estimation}} in the {{Linear Panel Event-Study Design}}},
  author = {Freyaldenhoven, Simon and Hansen, Christian and P{\'e}rez, Jorge P{\'e}rez and Shapiro, Jesse},
  year = {2021},
  month = aug,
  number = {w29170},
  pages = {w29170},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29170},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/6QQ67UIB/Freyaldenhoven et al_2021_Visualization, Identification, and Estimation in the Linear Panel Event-Study.pdf}
}

@article{gafarov2018DeltamethodInferenceClass,
  title = {Delta-Method Inference for a Class of Set-Identified {{SVARs}}},
  author = {Gafarov, Bulat and Meier, Matthias and Montiel Olea, Jos{\'e} Luis},
  year = {2018},
  month = apr,
  journal = {Journal of Econometrics},
  volume = {203},
  number = {2},
  pages = {316--327},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2017.12.004},
  urldate = {2022-12-16},
  abstract = {We study vector autoregressions that impose equality and/or inequality restrictions to set-identify the dynamic responses to a single structural shock. We make three contributions. First, we present an algorithm to compute the largest and smallest value that an impulse-response coefficient can attain over its identified set. Second, we provide conditions under which these largest and smallest values are directionally differentiable functions of the model's reduced-form parameters. Third, we propose a delta-method approach to conduct inference about the structural impulse-response coefficients. We use our results to assess the effects of the announcement of the Quantitative Easing program in August 2010.},
  langid = {english},
  keywords = {Directional differentiability,Set-identification,Sign restrictions,SVAR,Unconventional monetary policy},
  file = {/Users/fpalomba/Zotero/storage/BRLUPNL7/Gafarov et al. - 2018 - Delta-method inference for a class of set-identifi.pdf;/Users/fpalomba/Zotero/storage/TKRATYQL/S0304407617302440.html}
}

@article{giacomini2021RobustBayesianInference,
  title = {Robust {{Bayesian Inference}} for {{Set-Identified Models}}},
  author = {Giacomini, Raffaella and Kitagawa, Toru},
  year = {2021},
  journal = {Econometrica},
  volume = {89},
  number = {4},
  pages = {1519--1556},
  issn = {1468-0262},
  doi = {10.3982/ECTA16773},
  urldate = {2022-12-16},
  abstract = {This paper reconciles the asymptotic disagreement between Bayesian and frequentist inference in set-identified models by adopting a multiple-prior (robust) Bayesian approach. We propose new tools for Bayesian inference in set-identified models and show that they have a well-defined posterior interpretation in finite samples and are asymptotically valid from the frequentist perspective. The main idea is to construct a prior class that removes the source of the disagreement: the need to specify an unrevisable prior for the structural parameter given the reduced-form parameter. The corresponding class of posteriors can be summarized by reporting the `posterior lower and upper probabilities' of a given event and/or the `set of posterior means' and the associated `robust credible region'. We show that the set of posterior means is a consistent estimator of the true identified set and the robust credible region has the correct frequentist asymptotic coverage for the true identified set if it is convex. Otherwise, the method provides posterior inference about the convex hull of the identified set. For impulse-response analysis in set-identified Structural Vector Autoregressions, the new tools can be used to overcome or quantify the sensitivity of standard Bayesian inference to the choice of an unrevisable prior.},
  langid = {english},
  keywords = {asymptotic coverage,consistency,credible region,identified set,identifying restrictions,impulse-response analysis,Multiple priors},
  file = {/Users/fpalomba/Zotero/storage/A9WEF7I5/Giacomini and Kitagawa - 2021 - Robust Bayesian Inference for Set-Identified Model.pdf;/Users/fpalomba/Zotero/storage/6DFEJWZ9/ECTA16773.html}
}

@techreport{goldsmith-pinkham2022ContaminationBiasLinear,
  title = {Contamination {{Bias}} in {{Linear Regressions}}},
  author = {{Goldsmith-Pinkham}, Paul and Hull, Peter and Koles{\'a}r, Michal},
  year = {2022},
  month = jun,
  number = {w30108},
  pages = {w30108},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w30108},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CRP7UTST/Goldsmith-Pinkham et al_2022_Contamination Bias in Linear Regressions.pdf}
}

@article{goodman-bacon2019YouVeBeena,
  title = {So You've Been Told to Do My Difference-in-Differences Thing: {{A}} Guide},
  shorttitle = {So You've Been Told to Do My Difference-in-Differences Thing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2019},
  journal = {Vanderbilt University},
  file = {/Users/fpalomba/Zotero/storage/DQBNJA42/Goodman-Bacon_2019_So you’ve been told to do my difference-in-differences thing.pdf}
}

@article{goodman-bacon2021DifferenceindifferencesVariationTreatment,
  title = {Difference-in-Differences with Variation in Treatment Timing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  series = {Themed {{Issue}}: {{Treatment Effect}} 1},
  volume = {225},
  number = {2},
  pages = {254--277},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2021.03.014},
  urldate = {2023-02-21},
  abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, ''pre'' and ''post'', and two groups, ''treatment'' and ''control''. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  langid = {english},
  keywords = {Difference-in-differences,Treatment effect heterogeneity,Two-way fixed effects,Variation in treatment timing},
  file = {/Users/fpalomba/Zotero/storage/9YL2PX7F/Goodman-Bacon_2021_Difference-in-differences with variation in treatment timing.pdf;/Users/fpalomba/Zotero/storage/NSXD4QGV/S0304407621001445.html}
}

@article{gourieroux1991SimulationBasedInference,
  title = {Simulation {{Based Inference}} in {{Models}} with {{Heterogeneity}}},
  author = {Gouri{\'e}roux, Christian and Monfort, Alain},
  year = {1991},
  journal = {Annales d'\'Economie et de Statistique},
  number = {20/21},
  eprint = {20075807},
  eprinttype = {jstor},
  pages = {69--107},
  publisher = {{[GENES, ADRES]}},
  issn = {0769-489X},
  doi = {10.2307/20075807},
  urldate = {2023-05-28},
  abstract = {In this paper we discuss the usefulness, for models with heterogeneity, of simulation techniques in inference procedures, like maximum likelihood method, generalized moments method or pseudo maximum likelihood methods. These procedures are studied from the point of view of consistency, asymptotic normality, convergence rates and possible asymptotic bias. We carefully distinguish the case where the simulations are different for all the observations from the case where they are identical. /// Dans cet article nous discutons l'utilit\'e, pour les mod\`eles avec h\'et\'erog\'en\'eit\'e, des m\'ethodes de simulation dans les proc\'edures d'inf\'erence, comme le maximum de vraisemblance, la m\'ethode des moments g\'en\'eralis\'ee et les m\'ethodes du pseudo maximum de vraisemblance. Ces m\'ethodes sont \'etudi\'ees du point de vue de la convergence, de la normalit\'e asymptotique, des vitesses de convergence et d'\'eventuels biais asymptotiques. On distingue soigneusement le cas o\`u les simulations sont diff\'erentes pour chaque observation et le cas o\`u elles sont identiques.},
  file = {/Users/fpalomba/Zotero/storage/HZC69JFT/Gouriéroux_Monfort_1990_Simulation Based Inference in Models with Heterogeneity.pdf}
}

@inproceedings{gumbel1935ValeursExtremesDistributions,
  title = {Les Valeurs Extr\^emes Des Distributions Statistiques},
  booktitle = {Annales de l'institut {{Henri Poincar\'e}}},
  author = {Gumbel, Emil Julius},
  year = {1935},
  volume = {5},
  pages = {115--158},
  file = {/Users/fpalomba/Zotero/storage/TZ8QDAWB/Gumbel_1935_Les valeurs extrêmes des distributions statistiques.pdf}
}

@article{gupta2018AutoregressiveSpatialSpectral,
  title = {Autoregressive Spatial Spectral Estimates},
  author = {Gupta, Abhimanyu},
  year = {2018},
  month = mar,
  journal = {Journal of Econometrics},
  volume = {203},
  number = {1},
  pages = {80--95},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2017.10.006},
  urldate = {2023-05-22},
  abstract = {Nonparametric spectral density estimates find many uses in econometrics. For stationary random fields on a regular spatial lattice, we propose an autoregressive nonparametric spectral density estimate that is guaranteed positive even when suitable edge-effect correction is employed and is simple to compute using least squares. Our estimate is based on truncating a true half-plane infinite autoregressive representation, while also allowing the truncation length to diverge in all dimensions to avoid the potential bias due to truncation at a fixed lag-length. Uniform consistency of the proposed estimate is established, and new criteria for order selection are also suggested and studied in practical settings. The asymptotic distribution of the estimate is shown to be zero-mean normal and independent at fixed distinct frequencies, mirroring the behaviour for time series. A small Monte Carlo experiment examines finite sample performance. Technically the key to the results is the covariance structure of stationary random fields defined on regularly spaced lattices. We show the covariance matrix to satisfy a generalization of the Toeplitz property familiar from time series analysis.},
  langid = {english},
  keywords = {Central limit theorem,Covariance matrix,HAC estimation,Lattice data,Random field,Spatial process,Spectral density estimation},
  file = {/Users/fpalomba/Zotero/storage/6M6ECUUN/Gupta_2018_Autoregressive spatial spectral estimates.pdf;/Users/fpalomba/Zotero/storage/NL7LHW5U/S0304407617302361.html}
}

@misc{hagemann2013RobustSpectralAnalysis,
  title = {Robust {{Spectral Analysis}}},
  author = {Hagemann, Andreas},
  year = {2013},
  month = aug,
  number = {arXiv:1111.1965},
  eprint = {1111.1965},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1111.1965},
  urldate = {2023-03-26},
  abstract = {In this paper I introduce quantile spectral densities that summarize the cyclical behavior of time series across their whole distribution by analyzing periodicities in quantile crossings. This approach can capture systematic changes in the impact of cycles on the distribution of a time series and allows robust spectral estimation and inference in situations where the dependence structure is not accurately captured by the auto-covariance function. I study the statistical properties of quantile spectral estimators in a large class of nonlinear time series models and discuss inference both at fixed and across all frequencies. Monte Carlo experiments illustrate the advantages of quantile spectral analysis over classical methods when standard assumptions are violated.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/fpalomba/Zotero/storage/ABFD5F47/Hagemann_2013_Robust Spectral Analysis.pdf;/Users/fpalomba/Zotero/storage/LDWFI4SZ/1111.html}
}

@article{hahn2013AsymptoticVarianceSemiparametric,
  title = {Asymptotic {{Variance}} of {{Semiparametric Estimators With Generated Regressors}}},
  author = {Hahn, Jinyong and Ridder, Geert},
  year = {2013},
  journal = {Econometrica},
  volume = {81},
  number = {1},
  pages = {315--340},
  issn = {1468-0262},
  doi = {10.3982/ECTA9609},
  urldate = {2023-05-17},
  abstract = {We study the asymptotic distribution of three-step estimators of a finite-dimensional parameter vector where the second step consists of one or more nonparametric regressions on a regressor that is estimated in the first step. The first-step estimator is either parametric or nonparametric. Using Newey's (1994) path-derivative method, we derive the contribution of the first-step estimator to the influence function. In this derivation, it is important to account for the dual role that the first-step estimator plays in the second-step nonparametric regression, that is, that of conditioning variable and that of argument.},
  langid = {english},
  keywords = {asymptotic variance,generated regressors,Semiparametric estimation},
  file = {/Users/fpalomba/Zotero/storage/N5M7YPHI/Hahn_Ridder_2013_Asymptotic Variance of Semiparametric Estimators With Generated Regressors.pdf;/Users/fpalomba/Zotero/storage/49L6RHC4/ECTA9609.html}
}

@book{hajivassiliou2000PracticalIssuesMaximum,
  title = {Some Practical Issues in Maximum Simulated Likelihood},
  author = {Hajivassiliou, Vassilis A.},
  year = {2000},
  publisher = {{Suntory-Toyota International Centre for Economics and Related Disciplines \ldots}},
  file = {/Users/fpalomba/Zotero/storage/G75GMKBT/Hajivassiliou_1997_Some practical issues in maximum simulated likelihood.pdf}
}

@book{hannan1970MultipleTimeSeries,
  title = {Multiple {{Time Series}}},
  author = {Hannan, Edward James},
  year = {1970},
  publisher = {{Wiley}},
  abstract = {The Wiley Series in Probability and Statistics is a collection of topics of current research interests in both pure and applied statistics and probability developments in the field and classical methods. This series provides essential and invaluable reading for all statisticians, whether in academia, industry, government, or research.},
  googlebooks = {QYkQAQAAIAAJ},
  isbn = {978-0-471-34805-4},
  langid = {english},
  keywords = {Mathematics / General,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/Users/fpalomba/Zotero/storage/EUYNL8XQ/Hannan_1970_Multiple Time Series.pdf}
}

@article{hannig2004KernelSmoothingPeriodogramsa,
  title = {Kernel Smoothing of Periodograms under {{Kullback}}\textendash{{Leibler}} Discrepancy},
  author = {Hannig, Jan and Lee, Thomas C. M.},
  year = {2004},
  month = jul,
  journal = {Signal Processing},
  volume = {84},
  number = {7},
  pages = {1255--1266},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2004.04.007},
  urldate = {2023-02-23},
  abstract = {Kernel smoothing on the periodogram is a popular nonparametric method for spectral density estimation. Most important in the implementation of this method is the choice of the bandwidth, or span, for smoothing. One idealized way of choosing the bandwidth is to choose it as the one that minimizes the Kullback\textendash Leibler (KL) discrepancy between the smoothed estimate and the true spectrum. However, this method fails in practice, as the KL discrepancy is an unknown quantity. This paper introduces an estimator for this discrepancy, so that the bandwidth that minimizes the unknown discrepancy can be empirically approximated via the minimization of it. It is shown that this discrepancy estimator is consistent. Numerical results also suggest that this empirical choice of bandwidth often outperforms some other commonly used bandwidth choices. The same idea is also applied to choose the bandwidth for log-periodogram smoothing.},
  langid = {english},
  keywords = {Bandwidth selection,Kullback\textendash Leibler discrepancy,Periodogram and log-periodogram smoothing,Relative entropy,Spectral density estimation},
  file = {/Users/fpalomba/Zotero/storage/RBNMQ7UH/Hannig_Lee_2004_Kernel smoothing of periodograms under Kullback–Leibler discrepancy.pdf;/Users/fpalomba/Zotero/storage/UF8UEMG7/S0165168404000702.html}
}

@article{hardle1992KernelRegressionSmoothing,
  title = {Kernel {{Regression Smoothing}} of {{Time Series}}},
  author = {H{\"a}rdle, Wolfgang and Vieu, Philippe},
  year = {1992},
  journal = {Journal of Time Series Analysis},
  volume = {13},
  number = {3},
  pages = {209--232},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1992.tb00103.x},
  urldate = {2023-03-16},
  abstract = {Abstract. A class of non-parametric regression smoothers for times series is defined by the kernel method. The kernel approach allows flexible modelling of a time series without reference to a specific parametric class. The technique is applicable to detection of non-linear dependences in time series and to prediction in smooth regression models with serially correlated observations. In practice these estimators are to be tuned by a smoothing parameter. A data-driven selector for this smoothing parameter is presented that asymptotically minimizes a squared error measure. We prove asymptotic optimality of this selector. We illustrate the technique with a simulated example and by constructing a smooth prediction curve for the variation of gold prices. In both cases the non-parametric method proves to be useful in uncovering non-linear structure.},
  langid = {english},
  keywords = {autoregressive processes,data-driven bandwidth,kernel,Prediction,time series analysis,{$\alpha$}-mixing},
  file = {/Users/fpalomba/Zotero/storage/44TY9VN5/Härdle and Vieu - 1992 - Kernel Regression Smoothing of Time Series.pdf;/Users/fpalomba/Zotero/storage/A7LDFRFP/j.1467-9892.1992.tb00103.html}
}

@article{hart1990DataDrivenBandwidthChoice,
  title = {Data-{{Driven Bandwidth Choice}} for {{Density Estimation Based}} on {{Dependent Data}}},
  author = {Hart, Jeffrey D. and Vieu, Philippe},
  year = {1990},
  journal = {The Annals of Statistics},
  volume = {18},
  number = {2},
  eprint = {2242138},
  eprinttype = {jstor},
  pages = {873--890},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  urldate = {2023-03-16},
  abstract = {The bandwidth selection problem in kernel density estimation is investigated in situations where the observed data are dependent. The classical leave-out technique is extended, and thereby a class of cross-validated bandwidths is defined. These bandwidths are shown to be asymptotically optimal under a strong mixing condition. The leave-one out, or ordinary, form of cross-validation remains asymptotically optimal under the dependence model considered. However, a simulation study shows that when the data are strongly enough correlated, the ordinary version of cross-validation can be improved upon in finite-sized samples.},
  file = {/Users/fpalomba/Zotero/storage/Q4QASUBW/Hart and Vieu - 1990 - Data-Driven Bandwidth Choice for Density Estimatio.pdf}
}

@article{horvitz1952GeneralizationSamplingReplacement,
  title = {A {{Generalization}} of {{Sampling Without Replacement}} from a {{Finite Universe}}},
  author = {Horvitz, D. G. and Thompson, D. J.},
  year = {1952},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {47},
  number = {260},
  pages = {663--685},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1952.10483446},
  urldate = {2023-05-17},
  abstract = {This paper presents a general technique for the treatment of samples drawn without replacement from finite universes when unequal selection probabilities are used. Two sampling schemes are discussed in connection with the problem of determining optimum selection probabilities according to the information available in a supplementary variable. Admittedly, these two schemes have limited application. They should prove useful, however, for the first stage of sampling with multi-stage designs, since both permit unbiased estimation of the sampling variance without resorting to additional assumptions. * Journal Paper No. J2139 of the Iowa Agricultural Experiment Station, Ames, Iowa, Project 1005. Presented to the Institute of Mathematical Statistics, March 17, 1951.},
  file = {/Users/fpalomba/Zotero/storage/5LDNNNN2/Horvitz_Thompson_1952_A Generalization of Sampling Without Replacement from a Finite Universe.pdf}
}

@misc{https://www.ilsussidiario.net/redazione2014NonMaiTroppo,
  title = {{Non \`e mai troppo tardi/ Riassunto ultima puntata 25 febbraio: Alberto Manzi entra in Rai e insegna a milioni di italiani}},
  shorttitle = {{Non \`e mai troppo tardi/ Riassunto ultima puntata 25 febbraio}},
  author = {{https://www.ilsussidiario.net/redazione}},
  year = {2014},
  month = feb,
  journal = {IlSussidiario.net},
  urldate = {2023-04-17},
  abstract = {E andata in onda ieri sera, marted\`i 25 febbraio, una nuova puntata di Non \`e mai troppo tardi con Claudio Santamaria. Ecco cos'\`e successo ad Alberto Manzi, il maestro pi\`u famoso d'Italia.},
  chapter = {Cinema e Tv},
  howpublished = {https://www.ilsussidiario.net/news/cinema-televisione-e-media/2014/2/24/non-mai-troppo-tardi-chi-e-alberto-manzi-il-maestro-che-ha-insegnato-a-leggere-e-scrivere-con-la-tv/471875/},
  langid = {italian},
  file = {/Users/fpalomba/Zotero/storage/YED9C8G3/471875.html}
}

@misc{huang2022VariancebasedSensitivityAnalysis,
  title = {Variance-Based Sensitivity Analysis for Weighting Estimators Result in More Informative Bounds},
  author = {Huang, Melody and Pimentel, Samuel D.},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01691},
  eprint = {2208.01691},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.01691},
  urldate = {2023-01-23},
  abstract = {Weighting methods are popular tools for estimating causal effects; assessing their robustness under unobserved confounding is important in practice. In the following paper, we introduce a new set of sensitivity models called "variance-based sensitivity models". Variance-based sensitivity models characterize the bias from omitting a confounder by bounding the distributional differences that arise in the weights from omitting a confounder, with several notable innovations over existing approaches. First, the variance-based sensitivity models can be parameterized with respect to a simple \$R\^2\$ parameter that is both standardized and bounded. We introduce a formal benchmarking procedure that allows researchers to use observed covariates to reason about plausible parameter values in an interpretable and transparent way. Second, we show that researchers can estimate valid confidence intervals under a set of variance-based sensitivity models, and provide extensions for researchers to incorporate their substantive knowledge about the confounder to help tighten the intervals. Last, we highlight the connection between our proposed approach and existing sensitivity analyses, and demonstrate both, empirically and theoretically, that variance-based sensitivity models can provide improvements on both the stability and tightness of the estimated confidence intervals over existing methods. We illustrate our proposed approach on a study examining blood mercury levels using the National Health and Nutrition Examination Survey (NHANES).},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/A7Q55I3D/Huang and Pimentel - 2022 - Variance-based sensitivity analysis for weighting .pdf;/Users/fpalomba/Zotero/storage/X7AAEHMQ/2208.html}
}

@article{hurvich1985DataDrivenChoiceSpectrum,
  title = {Data-{{Driven Choice}} of a {{Spectrum Estimate}}: {{Extending}} the {{Applicability}} of {{Cross-Validation Methods}}},
  shorttitle = {Data-{{Driven Choice}} of a {{Spectrum Estimate}}},
  author = {Hurvich, Clifford M.},
  year = {1985},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {80},
  number = {392},
  pages = {933--940},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1985.10478207},
  urldate = {2023-02-23},
  abstract = {I develop methods of objectively choosing a spectrum estimate from a general class C of available estimates. C can, for example, simultaneously include Blackman\textemdash Tukey and autoregressive estimates, so the statistician no longer needs to choose one type or the other arbitrarily. The methods work by extending the applicability of existing cross-validatory techniques through the introduction of generalized leave-out-one spectrum estimates. As special cases, I obtain new objective smoothness parameter selection methods for both autoregressive and Blackman\textemdash Tukey estimates. In a Monte Carlo study, I demonstrate the effectiveness of the particular methods that result from generalizing Wahba's CVMSE.},
  keywords = {Autoregressive order selection,Autoregressive spectrum estimation,Bandwidth determination,Blackman\textemdash Tukey spectrum estimation,Kernel spectrum estimation},
  file = {/Users/fpalomba/Zotero/storage/3L4PITD3/Hurvich_1985_Data-Driven Choice of a Spectrum Estimate.pdf}
}

@article{imai2014CovariateBalancingPropensity,
  title = {Covariate Balancing Propensity Score},
  author = {Imai, Kosuke and Ratkovic, Marc},
  year = {2014},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {76},
  number = {1},
  eprint = {24772753},
  eprinttype = {jstor},
  pages = {243--263},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {1369-7412},
  urldate = {2023-05-13},
  abstract = {The propensity score plays a central role in a variety of causal inference settings. In particular, matching and weighting methods based on the estimated propensity score have become increasingly common in the analysis of observational data. Despite their popularity and theoretical appeal, the main practical difficulty of these methods is that the propensity score must be estimated. Researchers have found that slight misspecification of the propensity score model can result in substantial bias of estimated treatment effects. We introduce covariate balancing propensity score (CBPS) methodology, which models treatment assignment while optimizing the covariate balance. The CBPS exploits the dual characteristics of the propensity score as a covariate balancing score and the conditional probability of treatment assignment. The estimation of the CBPS is done within the generalized method-of-moments or empirical likelihood framework. We find that the CBPS dramatically improves the poor empirical performance of propensity score matching and weighting methods reported in the literature. We also show that the CBPS can be extended to other important settings, including the estimation of the generalized propensity score for non-binary treatments and the generalization of experimental estimates to a target population. Open source software is available for implementing the methods proposed.},
  file = {/Users/fpalomba/Zotero/storage/23KUI53Q/Imai_Ratkovic_2014_Covariate balancing propensity score.pdf}
}

@article{imbens2000RolePropensityScore,
  title = {The Role of the Propensity Score in Estimating Dose-Response Functions},
  author = {Imbens, {\relax GW}},
  year = {2000},
  month = sep,
  journal = {Biometrika},
  volume = {87},
  number = {3},
  pages = {706--710},
  issn = {0006-3444},
  doi = {10.1093/biomet/87.3.706},
  urldate = {2023-05-17},
  abstract = {Estimation of average treatment effects in observational studies often requires adjustment for differences in pre-treatment variables. If the number of pre-treatment variables is large, standard covariance adjustment methods are often inadequate. Rosenbaum \&amp; Rubin (1983) propose an alternative method for adjusting for pre-treatment variables for the binary treatment case based on the so-called propensity score. Here an extension of the propensity score methodology is proposed that allows for estimation of average casual effects with multi-valued treatments.},
  file = {/Users/fpalomba/Zotero/storage/39JZVII4/Imbens_2000_The role of the propensity score in estimating dose-response functions.pdf;/Users/fpalomba/Zotero/storage/5PKHEJET/293734.html}
}

@article{jansson2002ConsistentCovarianceMatrix,
  title = {Consistent {{Covariance Matrix Estimation}} for {{Linear Processes}}},
  author = {Jansson, Michael},
  year = {2002},
  month = dec,
  journal = {Econometric Theory},
  volume = {18},
  number = {6},
  pages = {1449--1459},
  publisher = {{Cambridge University Press}},
  issn = {1469-4360, 0266-4666},
  doi = {10.1017/S0266466602186087},
  urldate = {2023-04-06},
  abstract = {Consistency of kernel estimators of the long-run covariance  matrix of a linear process is established under weak moment  and memory conditions. In addition, it is pointed out that some  existing consistency proofs are in error as they stand.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/8PIHI8VF/Jansson - 2002 - CONSISTENT COVARIANCE MATRIX ESTIMATION FOR LINEAR.pdf}
}

@article{joffe1999InvitedCommentaryPropensitya,
  title = {Invited {{Commentary}}: {{Propensity Scores}}},
  shorttitle = {Invited {{Commentary}}},
  author = {Joffe, Marshall M. and Rosenbaum, Paul R.},
  year = {1999},
  month = aug,
  journal = {American Journal of Epidemiology},
  volume = {150},
  number = {4},
  pages = {327--333},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a010011},
  urldate = {2023-05-17},
  abstract = {The propensity score is the conditional probability of exposure to a treatment given observed covariates. In a cohort study, matching or stratifying treated and control subjects on a single variable, the propensity score, tends to balance all of the observed covariates; however, unlike random assignment of treatments, the propensity score may not also balance unobserved covariates. The authors review the uses and limitations of propensity scores and provide a brief outline of associated statistical theory. They also present a new result of using propensity scores in case-cohort studies.Am J Epidemiol 1999; 150: 327\textendash 33.},
  file = {/Users/fpalomba/Zotero/storage/ILYEK4GZ/Joffe_Rosenbaum_1999_Invited Commentary.pdf;/Users/fpalomba/Zotero/storage/HY48TF26/98791.html}
}

@misc{kuosmanen2021DesignFlawSynthetic,
  type = {{{MPRA Paper}}},
  title = {Design {{Flaw}} of the {{Synthetic Control Method}}},
  author = {Kuosmanen, Timo and Zhou, Xun and Eskelinen, Juha and Malo, Pekka},
  year = {2021},
  month = feb,
  urldate = {2023-01-13},
  abstract = {Synthetic control method (SCM) identifies causal treatment effects by constructing a counterfactual treatment unit as a convex combination of donors in the control group, such that the weights of donors and predictors are jointly optimized during the pre-treatment period. This paper demonstrates that the true optimal solution to the SCM problem is typically a corner solution where all weight is assigned to a single predictor, contradicting the intended purpose of predictors. To address this inherent design flaw, we propose to determine the predictor weights and donor weights separately. We show how the donor weights can be optimized when the predictor weights are given, and consider alternative data-driven approaches to determine the predictor weights. Re-examination of the two original empirical applications to Basque terrorism and California's tobacco control program demonstrates the complete and utter failure of the existing SCM algorithms and illustrates our proposed remedies.},
  howpublished = {https://mpra.ub.uni-muenchen.de/106390/},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/FKSMMJA9/Kuosmanen et al_2021_Design Flaw of the Synthetic Control Method.pdf;/Users/fpalomba/Zotero/storage/5QTCGN6K/106390.html}
}

@inproceedings{lechner2001IdentificationEstimationCausal,
  title = {Identification and Estimation of Causal Effects of Multiple Treatments under the Conditional Independence Assumption},
  booktitle = {Econometric {{Evaluation}} of {{Labour Market Policies}}},
  author = {Lechner, Michael},
  editor = {Lechner, Michael and Pfeiffer, Friedhelm},
  year = {2001},
  series = {{{ZEW Economic Studies}}},
  pages = {43--58},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-642-57615-7_3},
  abstract = {The assumption that the assignment to treatments is ignorable conditional on attributes plays an important role in the applied statistic and econometric evaluation literature. Another term for it is conditional independence assumption (CIA). This paper discusses identification using CIA when there are more than two types of mutually exclusive treatments. It turns out that low dimensional balancing scores, similar to the ones valid in the case of only two treatments, exist and can be used for identification of various causal effects. Therefore, a comparable reduction of the dimension of the estimation problem is achieved and the approach retains its basic simplicity. The paper also outlines a matching estimator potentially suitable in that framework.},
  isbn = {978-3-642-57615-7},
  langid = {english},
  keywords = {balancing score,causal model,matching,programme evaluation,propensity score,Treatment effects},
  file = {/Users/fpalomba/Zotero/storage/KVDW6LCU/Lechner_2001_Identification and estimation of causal effects of multiple treatments under.pdf}
}

@article{lee1997SimpleSpanSelector,
  title = {A {{Simple Span Selector}} for {{Periodogram Smoothing}}},
  author = {Lee, Thomas C. M.},
  year = {1997},
  journal = {Biometrika},
  volume = {84},
  number = {4},
  eprint = {2337667},
  eprinttype = {jstor},
  pages = {965--969},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.1093/biomet/84.4.965},
  urldate = {2023-02-23},
  abstract = {One approach to estimating the spectral density of a stationary time series is to smooth the periodogram and one important component of this approach is the choice of the span for smoothing. This note proposes a new span selector which is based on unbiased risk estimation. The proposed span selector is simple, and does not impose strong conditions on the unknown spectrum. For example, it does not require the unknown spectrum to possess a second derivative, which is a typical requirement of most plug-in type or spline-based methods. The finite sample performance of the proposed span selector is illustrated via a small simulation.},
  file = {/Users/fpalomba/Zotero/storage/ATU5T227/Lee_1997_A Simple Span Selector for Periodogram Smoothing.pdf}
}

@article{lee2001StabilizedBandwidthSelection,
  title = {A Stabilized Bandwidth Selection Method for Kernel Smoothing of the Periodogram},
  author = {Lee, Thomas C. M.},
  year = {2001},
  month = feb,
  journal = {Signal Processing},
  volume = {81},
  number = {2},
  pages = {419--430},
  issn = {0165-1684},
  doi = {10.1016/S0165-1684(00)00218-8},
  urldate = {2023-02-23},
  abstract = {One popular method for nonparametric spectral density estimation is to perform kernel smoothing on the periodogram, and one important component of this method is the choice of the bandwidth (or span) for smoothing. This paper proposes a new bandwidth selection method that is based on a coupling of the so-called plug-in and the unbiased risk estimation ideas. This new method is easy to describe, simple to implement, and does not impose severe conditions on the unknown spectrum. Numerical results suggest that this new method often outperforms some other commonly used bandwidth selection methods. The new methodology is also applied to choose the bandwidth for log-periodogram smoothing.},
  langid = {english},
  keywords = {Bandwidth selection,Log-periodogram and periodogram smoothing,Plug in,Spectral density estimation,Unbiased risk estimation},
  file = {/Users/fpalomba/Zotero/storage/YNQ2F74I/Lee_2001_A stabilized bandwidth selection method for kernel smoothing of the periodogram.pdf;/Users/fpalomba/Zotero/storage/632HI844/S0165168400002188.html}
}

@inproceedings{lee2009NonparametricSpectralDensity,
  title = {Nonparametric Spectral Density Estimation with Missing Observations},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lee, Thomas C. M. and Zhu, Zhengyuan},
  year = {2009},
  month = apr,
  pages = {3041--3044},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2009.4960265},
  abstract = {Self-consistency is a fundamental principle in statistics for retaining maximum amount of information in the data. In this paper this principle is applied to develop a new method for nonparametric spectrum estimation with missing data. One major advantage of the proposed method is that it can be coupled with any complete data nonparametric spectrum estimation procedure, including kernel smoothing, wavelet and spline estimators. The practical performance of the method is illustrated by a simulation study.},
  keywords = {Astrophysics,Discrete Fourier transforms,Frequency,Geology,Kernel,missing data,nonparametric spectrum estimation,periodogram smoothing,self-consistency,Smoothing methods,Spectral analysis,Spline,Statistics,Time domain analysis},
  file = {/Users/fpalomba/Zotero/storage/XULQHQI7/Lee_Zhu_2009_Nonparametric spectral density estimation with missing observations.pdf;/Users/fpalomba/Zotero/storage/SKZ7ERLX/4960265.html}
}

@book{luce1959IndividualChoiceBehavior,
  title = {Individual {{Choice Behavior}}: {{A Theoretical Analysis}}},
  shorttitle = {Individual {{Choice Behavior}}},
  author = {Luce, R. Duncan},
  year = {1959},
  publisher = {{Courier Corporation}},
  abstract = {This influential treatise presents upper-level undergraduates and graduate students with a mathematical analysis of choice behavior. It begins with the statement of a general axiom upon which the rest of the book rests; the following three chapters, which may be read independently of each other, are devoted to applications of the theory to substantive problems: psychophysics, utility, and learning.Applications to psychophysics include considerations of time- and space-order effects, the Fechnerian assumption, the power law and its relation to discrimination data, interaction of continua, discriminal processes, signal detectability theory, and ranking of stimuli. The next major theme, utility theory, features unusual results that suggest an experiment to test the theory. The final chapters explore learning-related topics, analyzing the stochastic theories of learning as the basic approach\textemdash with the exception that distributions of response strengths are assumed to be transformed rather than response probabilities. The author arrives at three classes of learning operators, both linear and nonlinear, and the text concludes with a useful series of appendixes.},
  googlebooks = {ERQsKkPiKkkC},
  isbn = {978-0-486-15339-1},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@article{mackinnon2022ClusterrobustInferenceGuide,
  title = {Cluster-Robust Inference: {{A}} Guide to Empirical Practice},
  shorttitle = {Cluster-Robust Inference},
  author = {MacKinnon, James G. and Nielsen, Morten {\O}rregaard and Webb, Matthew D.},
  year = {2022},
  month = may,
  journal = {Journal of Econometrics},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2022.04.001},
  urldate = {2023-01-10},
  abstract = {Methods for cluster-robust inference are routinely used in economics and many other disciplines. However, it is only recently that theoretical foundations for the use of these methods in many empirically relevant situations have been developed. In this paper, we use these theoretical results to provide a guide to empirical practice. We do not attempt to present a comprehensive survey of the (very large) literature. Instead, we bridge theory and practice by providing a thorough guide on what to do and why, based on recently available econometric theory and simulation evidence. To practice what we preach, we include an empirical analysis of the effects of the minimum wage on labor supply of teenagers using individual data.},
  langid = {english},
  keywords = {Cluster jackknife,Cluster-robust variance estimator (CRVE),Clustered data,Robust inference,Wild cluster bootstrap},
  file = {/Users/fpalomba/Zotero/storage/63Q4FJIC/MacKinnon et al_2022_Cluster-robust inference.pdf;/Users/fpalomba/Zotero/storage/EW7CHKUG/S0304407622000781.html}
}

@book{maddala1983LimitedDependentQualitativeVariables,
  title = {Limited-{{Dependent}} and {{Qualitative Variables}} in {{Econometrics}}},
  author = {Maddala, G. S.},
  year = {1983},
  series = {Econometric {{Society Monographs}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511810176},
  urldate = {2023-03-09},
  abstract = {This book presents the econometric analysis of single-equation and simultaneous-equation models in which the jointly dependent variables can be continuous, categorical, or truncated. Despite the traditional emphasis on continuous variables in econometrics, many of the economic variables encountered in practice are categorical (those for which a suitable category can be found but where no actual measurement exists) or truncated (those that can be observed only in certain ranges). Such variables are involved, for example, in models of occupational choice, choice of tenure in housing, and choice of type of schooling. Models with regulated prices and rationing, and models for program evaluation, also represent areas of application for the techniques presented by the author.},
  isbn = {978-0-521-33825-7},
  file = {/Users/fpalomba/Zotero/storage/VDRFUI9W/Maddala_1983_Limited-Dependent and Qualitative Variables in Econometrics.pdf;/Users/fpalomba/Zotero/storage/XWXAMAXV/69B8DBC75160713AA3AD1AD979D297B8.html}
}

@misc{malo2020ComputingSyntheticControls,
  type = {{{MPRA Paper}}},
  title = {Computing {{Synthetic Controls Using Bilevel Optimization}}},
  author = {Malo, Pekka and Eskelinen, Juha and Zhou, Xun and Kuosmanen, Timo},
  year = {2020},
  month = nov,
  urldate = {2023-01-13},
  abstract = {The synthetic control method (SCM) is a major innovation in the estimation of causal effects of policy interventions and programs in a comparative case study setting. In this paper, we demonstrate that the data-driven approach to SCM requires solving a bilevel optimization problem. We show how the SCM problem can be solved using iterative algorithms based on Tykhonov descent or KKT approximations.},
  howpublished = {https://mpra.ub.uni-muenchen.de/104085/},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/LSZUGYPC/Malo et al_2020_Computing Synthetic Controls Using Bilevel Optimization.pdf;/Users/fpalomba/Zotero/storage/XE763HWE/104085.html}
}

@article{marron1994TransformationsReduceBoundary,
  title = {Transformations to {{Reduce Boundary Bias}} in {{Kernel Density Estimation}}},
  author = {Marron, J. S. and Ruppert, D.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {56},
  number = {4},
  pages = {653--671},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1994.tb02006.x},
  urldate = {2023-04-12},
  abstract = {We consider kernel estimation of a univariate density whose support is a compact interval. If the density is non-zero at either boundary, then the usual kernel estimator can be seriously biased. `Reflection' at a boundary removes some bias, but unless the first derivative of the density is 0 at the boundary the estimator with reflection can still be much more severely biased at the boundary than in the interior. We propose to transform the data to a density that has its first derivative equal to 0 at both boundaries. The density of the transformed data is estimated, and an estimate of the density of the original data is obtained by a change of variables. The transformation is selected from a parametric family, which is allowed to be quite general in our theoretical study. We propose algorithms where the transformation is either a quartic polynomial, a {$\beta$} cumulative density function (CDF) or a linear combination of a polynomial and a {$\beta$} CDF. The last two types of transformation are designed to accommodate possible poles at the boundaries. The first two algorithms are tested on simulated data and compared with an adjusted kernel method of Rice. We find that our proposal performs similarly to Rice's for densities with one-sided derivatives at the boundaries. Unlike Rice's method, our proposal is guaranteed to produce non-negative estimates. This can be a distinct advantage when the density is 0 at either boundary. Our algorithm for densities with poles outperforms the Rice adjustment when the density has a pole at a boundary.},
  langid = {english},
  keywords = {empirical processes,kernel density estimation,local bandwidths,local least squares estimation,parametric data transformations,pole at boundary,polynomial transformations,reflection about boundary},
  file = {/Users/fpalomba/Zotero/storage/JX9XP54N/Marron_Ruppert_1994_Transformations to Reduce Boundary Bias in Kernel Density Estimation.pdf;/Users/fpalomba/Zotero/storage/VWYSU435/j.2517-6161.1994.tb02006.html}
}

@article{masry1996MultivariateRegressionEstimation,
  title = {Multivariate Regression Estimation Local Polynomial Fitting for Time Series},
  author = {Masry, Elias},
  year = {1996},
  month = dec,
  journal = {Stochastic Processes and their Applications},
  volume = {65},
  number = {1},
  pages = {81--101},
  issn = {0304-4149},
  doi = {10.1016/S0304-4149(96)00095-6},
  urldate = {2023-03-16},
  abstract = {We consider the estimation of the multivariate regression function m(x1, \ldots, xd) = E[{$\psi$}(Yd)|X1 = x1, \ldots, Xd = xd], and its partial derivatives, for stationary random processes Yi, Xi using local higher-order polynomial fitting. Particular cases of {$\psi$} yield estimation of the conditional mean, conditional moments and conditional distributions. Joint asymptotic normality is established for estimates of the regression function and its partial derivatives for strongly mixing and {$\varrho$}-mixing processes. Expressions for the bias and variance/covariance matrix (of the asymptotically normal distribution) for these estimators are given.},
  langid = {english},
  keywords = {Joint asymptotic normality,Local polynomial fitting,Mixing processes,Multivariate regression estimation},
  file = {/Users/fpalomba/Zotero/storage/TZMZGKNA/Masry - 1996 - Multivariate regression estimation local polynomia.pdf;/Users/fpalomba/Zotero/storage/QIL3QZFK/S0304414996000956.html}
}

@article{masry1997LocalPolynomialEstimation,
  title = {Local {{Polynomial Estimation}} of {{Regression Functions}} for {{Mixing Processes}}},
  author = {Masry, Elias and Fan, Jianqing},
  year = {1997},
  journal = {Scandinavian Journal of Statistics},
  volume = {24},
  number = {2},
  eprint = {4616446},
  eprinttype = {jstor},
  pages = {165--179},
  publisher = {{[Board of the Foundation of the Scandinavian Journal of Statistics, Wiley]}},
  issn = {0303-6898},
  urldate = {2023-03-16},
  abstract = {Local polynomial fitting has many exciting statistical properties which where established under i.i.d. setting. However, the need for non-linear time series modeling, constructing predictive intervals, understanding divergence of non-linear time series requires the development of the theory of local polynomial fitting for dependent data. In this paper, we study the problem of estimating conditional mean functions and their derivatives via a local polynomial fit. The functions include conditional moments, conditional distribution as well as conditional density functions. Joint asymptotic normality for derivative estimation is established for both strongly mixing and {$\rho$}-mixing processes.},
  file = {/Users/fpalomba/Zotero/storage/9GLXV5IA/Masry and Fan - 1997 - Local Polynomial Estimation of Regression Function.pdf}
}

@book{mcelroy2020TimeSeriesFirst,
  title = {Time {{Series}}: {{A First Course}} with {{Bootstrap Starter}}},
  shorttitle = {Time {{Series}}},
  author = {McElroy, Tucker and Politis, Dimitris N.},
  year = {2020},
  publisher = {{CRC Press}},
  abstract = {Time Series: A First Course with Bootstrap Starter provides an introductory course on time series analysis that satisfies the triptych of (i) mathematical completeness, (ii) computational illustration and implementation, and (iii) conciseness and accessibility to upper-level undergraduate and M.S. students. Basic theoretical results are presented in a mathematically convincing way, and the methods of data analysis are developed through examples and exercises parsed in R. A student with a basic course in mathematical statistics will learn both how to analyze time series and how to interpret the results.  The book provides the foundation of time series methods, including linear filters and a geometric approach to prediction. The important paradigm of ARMA models is studied in-depth, as well as frequency domain methods. Entropy and other information theoretic notions are introduced, with applications to time series modeling. The second half of the book focuses on statistical inference, the fitting of time series models, as well as computational facets of forecasting. Many time series of interest are nonlinear in which case classical inference methods can fail, but bootstrap methods may come to the rescue. Distinctive features of the book are the emphasis on geometric notions and the frequency domain, the discussion of entropy maximization, and a thorough treatment of recent computer-intensive methods for time series such as subsampling and the bootstrap. There are more than 600 exercises, half of which involve R coding and/or data analysis. Supplements include a website with 12 key data sets and all R code for the book's examples, as well as the solutions to exercises.},
  googlebooks = {Sun5tgAACAAJ},
  isbn = {978-1-4398-7651-0},
  langid = {english}
}

@misc{mcelroy2022EstimatingSpectralDensity,
  title = {Estimating the {{Spectral Density}} at {{Frequencies Near Zero}}},
  author = {McElroy, Tucker and Politis, Dimitris},
  year = {2022},
  month = aug,
  number = {arXiv:2208.02300},
  eprint = {2208.02300},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.02300},
  urldate = {2023-04-12},
  abstract = {Estimating the spectral density function \$f(w)\$ for some \$w\textbackslash in [-\textbackslash pi, \textbackslash pi]\$ has been traditionally performed by kernel smoothing the periodogram and related techniques. Kernel smoothing is tantamount to local averaging, i.e., approximating \$f(w)\$ by a constant over a window of small width. Although \$f(w)\$ is uniformly continuous and periodic with period \$2\textbackslash pi\$, in this paper we recognize the fact that \$w=0\$ effectively acts as a boundary point in the underlying kernel smoothing problem, and the same is true for \$w=\textbackslash pm \textbackslash pi\$. It is well-known that local averaging may be suboptimal in kernel regression at (or near) a boundary point. As an alternative, we propose a local polynomial regression of the periodogram or log-periodogram when \$w\$ is at (or near) the points 0 or \$\textbackslash pm \textbackslash pi\$. The case \$w=0\$ is of particular importance since \$f(0)\$ is the large-sample variance of the sample mean; hence, estimating \$f(0)\$ is crucial in order to conduct any sort of inference on the mean.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/KJ3MBFSP/McElroy_Politis_2022_Estimating the Spectral Density at Frequencies Near Zero.pdf;/Users/fpalomba/Zotero/storage/EEEMNYFJ/2208.html}
}

@incollection{mcfadden1974FrontiersEconometrics,
  title = {Frontiers in {{Econometrics}}},
  booktitle = {Conditional Logit Analysis of Qualitative Choice Behavior},
  author = {McFadden, Daniel},
  year = {1974},
  pages = {105--142},
  publisher = {{Academic Press}},
  isbn = {0-12-776150-0},
  file = {/Users/fpalomba/Zotero/storage/23T7KBQ7/McFadden_1974_Conditional logit analysis of qualitative choice behavior.pdf;/Users/fpalomba/Zotero/storage/5T389T9K/10002395479.html}
}

@article{mcfadden2000MixedMNLModels,
  title = {Mixed {{MNL}} Models for Discrete Response},
  author = {McFadden, Daniel and Train, Kenneth},
  year = {2000},
  journal = {Journal of Applied Econometrics},
  volume = {15},
  number = {5},
  pages = {447--470},
  issn = {1099-1255},
  doi = {10.1002/1099-1255(200009/10)15:5<447::AID-JAE570>3.0.CO;2-1},
  urldate = {2023-05-04},
  abstract = {This paper considers mixed, or random coefficients, multinomial logit (MMNL) models for discrete response, and establishes the following results. Under mild regularity conditions, any discrete choice model derived from random utility maximization has choice probabilities that can be approximated as closely as one pleases by a MMNL model. Practical estimation of a parametric mixing family can be carried out by Maximum Simulated Likelihood Estimation or Method of Simulated Moments, and easily computed instruments are provided that make the latter procedure fairly efficient. The adequacy of a mixing specification can be tested simply as an omitted variable test with appropriately defined artificial variables. An application to a problem of demand for alternative vehicles shows that MMNL provides a flexible and computationally practical approach to discrete response analysis. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/IDBCXLKK/McFadden_Train_2000_Mixed MNL models for discrete response.pdf;/Users/fpalomba/Zotero/storage/27U9YTXR/10)155447AID-JAE5703.0.html}
}

@article{mombeni2017LinexDiscrepancyBandwidth,
  title = {Linex Discrepancy for Bandwidth Selection},
  author = {Mombeni, H. and Rezaei, S. and Nadarajah, S.},
  year = {2017},
  month = aug,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {7},
  pages = {5054--5069},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2016.1143102},
  urldate = {2023-02-23},
  abstract = {A bandwidth selection based on Linex discrepancy is proposed for kernel smoothing of periodogram. The selection minimizes Linex discrepancy between the smoothed and true spectrums. Two estimators are introduced for Linex discrepancy. The bandwidth choice outperforms some common bandwidth choices.},
  keywords = {62E99,Bandwidth selection,Linex discrepancy,Periodogram smoothing,Spectral density estimation},
  file = {/Users/fpalomba/Zotero/storage/X4E8UIM4/Mombeni et al_2017_Linex discrepancy for bandwidth selection.pdf}
}

@article{neumann2008GoodnessofFitTestsMarkovian,
  title = {Goodness-of-{{Fit Tests}} for {{Markovian Time Series Models}}: {{Central Limit Theory}} and {{Bootstrap Approximations}}},
  shorttitle = {Goodness-of-{{Fit Tests}} for {{Markovian Time Series Models}}},
  author = {Neumann, Michael H. and Paparoditis, Efstathios},
  year = {2008},
  journal = {Bernoulli},
  volume = {14},
  number = {1},
  eprint = {25464931},
  eprinttype = {jstor},
  pages = {14--46},
  publisher = {{International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability}},
  issn = {1350-7265},
  urldate = {2023-03-17},
  abstract = {New goodness-of-fit tests for Markovian models in time series analysis are developed which are based on the difference between a fully nonparametric estimate of the one-step transition distribution function of the observed process and that of the model class postulated under the null hypothesis. The model specification under the null allows for Markovian models, the transition mechanisms of which depend on an unknown vector of parameters and an unspecified distribution of i.i.d. innovations. Asymptotic properties of the test statistic are derived and the critical values of the test are found using appropriate bootstrap schemes. General properties of the bootstrap for Markovian processes are derived. A new central limit theorem for triangular arrays of weakly dependent random variables is obtained. For the proof of stochastic equicontinuity of multidimensional empirical processes, we use a simple approach based on an anisotropic tiling of the space. The finite-sample behavior of the proposed test is illustrated by some numerical examples and a real-data application is given.},
  file = {/Users/fpalomba/Zotero/storage/I4NK7KBS/Neumann and Paparoditis - 2008 - Goodness-of-Fit Tests for Markovian Time Series Mo.pdf}
}

@article{neumann2013CentralLimitTheorem,
  title = {A Central Limit Theorem for Triangular Arrays of Weakly Dependent Random Variables, with Applications in Statistics},
  author = {Neumann, Michael H.},
  year = {2013},
  journal = {ESAIM: Probability and Statistics},
  volume = {17},
  pages = {120--134},
  publisher = {{EDP Sciences}},
  issn = {1292-8100, 1262-3318},
  doi = {10.1051/ps/2011144},
  urldate = {2023-03-17},
  abstract = {We derive a central limit theorem for triangular arrays of possibly nonstationary random variables satisfying a condition of weak dependence in the sense of Doukhan and Louhichi [\emph{Stoch. Proc. Appl. {$<$}i/{$>$}\textbf{84 {$<$}b/{$>$}(1999) 313\textendash 342]. The proof uses a new variant of the Lindeberg method: the behavior of the partial sums is compared to that of partial sums of \emph{dependent {$<$}i/{$>$}Gaussian random variables. We also discuss a few applications in statistics which show that our central limit theorem is tailor-made for statistics of different type.}}}},
  copyright = {\textcopyright{} EDP Sciences, SMAI, 2013},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/UT42SN3Q/Neumann - 2013 - A central limit theorem for triangular arrays of w.pdf}
}

@article{parzen1957ConsistentEstimatesSpectrum,
  title = {On {{Consistent Estimates}} of the {{Spectrum}} of a {{Stationary Time Series}}},
  author = {Parzen, Emanuel},
  year = {1957},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {2},
  eprint = {2237156},
  eprinttype = {jstor},
  pages = {329--348},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  urldate = {2023-02-13},
  abstract = {This paper is concerned with the spectral analysis of wide sense stationary time series which possess a spectral density function and whose fourth moment functions satisfy an integrability condition (which includes Gaussian processes). Consistent estimates are obtained for the spectral density function as well as for the spectral distribution function and a general class of spectral averages. Optimum consistent estimates are chosen on the basis of criteria involving the notions of order of consistency and asymptotic variance. The problem of interpolating the estimated spectral density, so that only a finite number of quantities need be computed to determine the entire graph, is also discussed. Both continuous and discrete time series are treated.},
  file = {/Users/fpalomba/Zotero/storage/GFVA2CL5/Parzen_1957_On Consistent Estimates of the Spectrum of a Stationary Time Series.pdf}
}

@book{percival2020SpectralAnalysisUnivariate,
  title = {Spectral {{Analysis}} for {{Univariate Time Series}}},
  author = {Percival, Donald B. and Walden, Andrew T.},
  year = {2020},
  publisher = {{Cambridge University Press}},
  urldate = {2023-02-23},
  file = {/Users/fpalomba/Zotero/storage/LVPNS3JF/Percival_Walden_Spectral Analysis for Univariate Time Series.pdf;/Users/fpalomba/Zotero/storage/BHZPFQZ8/books.html}
}

@techreport{pickett2022MythsSyntheticControl,
  title = {The {{Myths}} of {{Synthetic Control}}: {{Recommendations}} for {{Practice}}},
  author = {Pickett, Robert E M and Hill, Jennifer and Cowan, Sarah K},
  year = {2022},
  institution = {{New York University}},
  abstract = {To estimate a causal effect of an intervention, researchers must identify a control group that can stand in for what might have happened to the treatment group in the absence of that intervention. This is complicated in the absence of a randomized experiment and further complicated when few units (possibly only one) are treated. When random assignment is impossible but data are available on units over time, researchers increasingly rely on synthetic control (SC) methods to create a pseudo-counterfactual. This estimate is constructed by differentially weighting control units that did not receive the treatment so that the constructed counterfactual is comparable in its pre-treatment trajectory to the treated unit. Since its origin twenty years ago, methodologists have tweaked the synthetic control technique. In the absence of empirically informed guidance on precisely how to implement SC, a number of accepted pieces of wisdom have arisen: (1) SC is robust to various implementations; (2) covariates are unnecessary, and (3) pre-treatment prediction error can guide model selection. We describe each of these in detail and conduct simulations that suggest, both for standard and alternative implementations of SC, that the purported truths are not supported by empirical evidence. Instead of relying on these `myths', we offer practical advice derived from our simulation results for more cautious implementation and interpretation of results.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/BUKEUYFR/Pickett et al. - The Myths of Synthetic Control Recommendations fo.pdf}
}

@techreport{ponomarev2022EfficientEstimationDirectionally,
  title = {Efficient {{Estimation}} of {{Directionally Differentiable Functionals}}},
  author = {Ponomarev, Kirill},
  year = {2022},
  institution = {{Duke University}},
  file = {/Users/fpalomba/Zotero/storage/CFXPF3ZY/Ponomarev, Kirill - 2022 - Efficient Estimation of Directionally Differentiab.pdf}
}

@book{priestley1981SpectralAnalysisTime,
  title = {Spectral {{Analysis}} and {{Time Series}}},
  author = {Priestley, M. B.},
  year = {1981},
  publisher = {{Academic Press}},
  googlebooks = {RVTYvwEACAAJ},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/DHXM8A32/priestley (1981) - spectral analysis and time series. volumes 1,2.pdf;/Users/fpalomba/Zotero/storage/EH3TWBA8/Priestley_1981_Spectral Analysis and Time Series.pdf}
}

@article{robins1994EstimationRegressionCoefficients,
  title = {Estimation of {{Regression Coefficients When Some Regressors Are Not Always Observed}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {427},
  eprint = {2290910},
  eprinttype = {jstor},
  pages = {846--866},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290910},
  urldate = {2023-01-04},
  abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector {$\alpha$}\textsubscript{0} of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of {$\alpha$}\textsubscript{0} is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations.},
  file = {/Users/fpalomba/Zotero/storage/HSPCW7FL/Robins et al_1994_Estimation of Regression Coefficients When Some Regressors Are Not Always.pdf}
}

@article{robins1995SemiparametricEfficiencyMultivariate,
  title = {Semiparametric {{Efficiency}} in {{Multivariate Regression Models}} with {{Missing Data}}},
  author = {Robins, James M. and Rotnitzky, Andrea},
  year = {1995},
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {429},
  eprint = {2291135},
  eprinttype = {jstor},
  pages = {122--129},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291135},
  urldate = {2023-01-04},
  abstract = {We consider the efficiency bound for the estimation of the parameters of semiparametric models defined solely by restrictions on the means of a vector of correlated outcomes, Y, when the data on Y are missing at random. We show that the semiparametric variance bound is the asymptotic variance of the optimal estimator in a class of inverse probability of censoring weighted estimators and that this bound is unchanged if the data are missing completely at random. For this case we study the asymptotic performance of the generalized estimating equations (GEE) estimators of mean parameters and show that the optimal GEE estimator is inefficient except for special cases. The optimal weighted estimator depends on unknown population quantities. But for monotone missing data, we propose an adaptive estimator whose asymptotic variance can achieve the bound.},
  file = {/Users/fpalomba/Zotero/storage/Z9AZAA7X/Robins and Rotnitzky - 1995 - Semiparametric Efficiency in Multivariate Regressi.pdf}
}

@incollection{robinson1983ReviewVariousApproaches,
  title = {Review of Various Approaches to Power Spectrum Estimation},
  booktitle = {Handbook of {{Statistics}}},
  author = {Robinson, P. M.},
  year = {1983},
  month = jan,
  series = {Time {{Series}} in the {{Frequency Domain}}},
  volume = {3},
  pages = {343--368},
  publisher = {{Elsevier}},
  doi = {10.1016/S0169-7161(83)03018-7},
  urldate = {2023-03-30},
  abstract = {This chapter presents the review of various approaches to power spectrum estimation. Many time series in the natural sciences and economics contain very strong periodic effects, and their detection has been the objective of some of the earliest investigations of time series. An important periodic effect will manifest itself in a readily identifiable spectral peak at the corresponding frequency, its influence on the process being measured by the magnitude of the peak. The presence of spectral peaks leads, however, to serious difficulties in power spectrum estimation. In some applications, such as in seismography, the objective is to distinguish between two stationary time series or to classify a series and the power spectrum is a convenient discriminator. The Wiener\textendash Kolmogorov theory of prediction and smoothing leads to frequency-domain formulas that require power spectrum estimates for their practical implementation. Because of their nonparametric nature, spectrum estimates are unlikely to be accurate or reliable unless based on a substantial amount of data.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/9KALD5D6/Robinson_1983_Review of various approaches to power spectrum estimation.pdf;/Users/fpalomba/Zotero/storage/WI5JPXRF/S0169716183030187.html}
}

@article{robinson2006ModifiedWhittleEstimation,
  title = {Modified {{Whittle}} Estimation of Multilateral Models on a Lattice},
  author = {Robinson, P. M. and Vidal Sanz, J.},
  year = {2006},
  month = may,
  journal = {Journal of Multivariate Analysis},
  volume = {97},
  number = {5},
  pages = {1090--1120},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2005.05.013},
  urldate = {2023-05-18},
  abstract = {In the estimation of parametric models for stationary spatial or spatio-temporal data on a d-dimensional lattice, for d{$\geqslant$}2, the achievement of asymptotic efficiency under Gaussianity, and asymptotic normality more generally, with standard convergence rate, faces two obstacles. One is the ``edge effect'', which worsens with increasing d. The other is the possible difficulty of computing a continuous-frequency form of Whittle estimate or a time domain Gaussian maximum likelihood estimate, due mainly to the Jacobian term. This is especially a problem in ``multilateral'' models, which are naturally expressed in terms of lagged values in both directions for one or more of the d dimensions. An extension of the discrete-frequency Whittle estimate from the time series literature deals conveniently with the computational problem, but when subjected to a standard device for avoiding theedge effect has disastrous asymptotic performance, along with finite sample numerical drawbacks, the objective function lacking a minimum-distance interpretation and losing any global convexity properties. We overcome these problems by first optimizing a standard, guaranteed non-negative, discrete-frequency, Whittle function, without edge-effect correction, providing an estimate with a slow convergence rate, then improving this by a sequence of computationally convenient approximate Newton iterations using a modified, almost-unbiased periodogram, the desired asymptotic properties being achieved after finitelymany steps. The asymptotic regime allows increase in both directions of all d dimensions, with the central limit theorem established after re-ordering as a triangular array. However our work offers something new for ``unilateral'' models also. When the data are non-Gaussian, asymptotic variances of all parameter estimates may be affected, and we propose consistent, non-negative definite estimates of the asymptotic variance matrix.},
  langid = {english},
  keywords = {Consistent variance estimation,Edge effect,Multilateral modelling,Spatial data,Whittle estimation},
  file = {/Users/fpalomba/Zotero/storage/SURCA4BK/Robinson_Vidal Sanz_2006_Modified Whittle estimation of multilateral models on a lattice.pdf;/Users/fpalomba/Zotero/storage/IWHQEZHY/S0047259X05000916.html}
}

@article{romano2016EfficientComputationAdjusted,
  title = {Efficient Computation of Adjusted P-Values for Resampling-Based Stepdown Multiple Testing},
  author = {Romano, Joseph P. and Wolf, Michael},
  year = {2016},
  month = jun,
  journal = {Statistics \& Probability Letters},
  volume = {113},
  pages = {38--40},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2016.02.012},
  urldate = {2023-05-02},
  abstract = {There has been a recent interest in reporting p-values adjusted for the resampling-based stepdown multiple testing procedures proposed in Romano and Wolf (2005a,b). The original papers only describe how to carry out multiple testing at a fixed significance level. Computing adjusted p-values instead in an efficient manner is not entirely trivial. Therefore, this paper fills an apparent gap by detailing such an algorithm.},
  langid = {english},
  keywords = {Adjusted -values,Multiple testing,Resampling,Stepdown procedure},
  file = {/Users/fpalomba/Zotero/storage/DDI4HNAT/Romano_Wolf_2016_Efficient computation of adjusted p-values for resampling-based stepdown.pdf;/Users/fpalomba/Zotero/storage/H4CM7AJ6/S0167715216000389.html}
}

@article{rosenbaum1983CentralRolePropensity,
  title = {The {{Central Role}} of the {{Propensity Score}} in {{Observational Studies}} for {{Causal Effects}}},
  author = {Rosenbaum, Paul R. and Rubin, Donald B.},
  year = {1983},
  journal = {Biometrika},
  volume = {70},
  number = {1},
  eprint = {2335942},
  eprinttype = {jstor},
  pages = {41--55},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.2307/2335942},
  urldate = {2023-05-17},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a two-dimensional plot.},
  file = {/Users/fpalomba/Zotero/storage/R24P4UGB/Rosenbaum_Rubin_1983_The Central Role of the Propensity Score in Observational Studies for Causal.pdf}
}

@article{rosenblatt1956CentralLimitTheorem,
  title = {A Central Limit Theorem and a Strong Mixing Condition},
  author = {Rosenblatt, M.},
  year = {1956},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {42},
  number = {1},
  pages = {43--47},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.42.1.43},
  urldate = {2023-03-28},
  file = {/Users/fpalomba/Zotero/storage/SVWWNSCJ/Rosenblatt_1956_A central limit theorem and a strong mixing condition.pdf}
}

@article{rosenblatt1984AsymptoticNormalityStrong,
  title = {Asymptotic {{Normality}}, {{Strong Mixing}} and {{Spectral Density Estimates}}},
  author = {Rosenblatt, M.},
  year = {1984},
  journal = {The Annals of Probability},
  volume = {12},
  number = {4},
  eprint = {2243355},
  eprinttype = {jstor},
  pages = {1167--1180},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0091-1798},
  urldate = {2023-03-29},
  abstract = {Asymptotic normality is proven for spectral density estimates assuming strong mixing and a limited number of moment conditions for the process analyzed. The result holds for a large class of processes that are not linear and does not require the existence of all moments.},
  file = {/Users/fpalomba/Zotero/storage/MW4SPQTJ/Rosenblatt_1984_Asymptotic Normality, Strong Mixing and Spectral Density Estimates.pdf}
}

@book{rosenblatt1985StationarySequencesRandom,
  title = {Stationary {{Sequences}} and {{Random Fields}}},
  author = {Rosenblatt, Murray},
  year = {1985},
  publisher = {{Birkh\"auser}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4612-5156-9},
  urldate = {2023-03-29},
  isbn = {978-0-8176-3264-9 978-1-4612-5156-9},
  langid = {english},
  keywords = {Banach Space,differential equation,Hilbert space,Likelihood,Maximum,Time series,Variance},
  file = {/Users/fpalomba/Zotero/storage/6G4P35S2/Rosenblatt_1985_Stationary Sequences and Random Fields.pdf}
}

@misc{roth2023WhatTrendingDifferenceinDifferences,
  title = {What's {{Trending}} in {{Difference-in-Differences}}? {{A Synthesis}} of the {{Recent Econometrics Literature}}},
  shorttitle = {What's {{Trending}} in {{Difference-in-Differences}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C. and Bilinski, Alyssa and Poe, John},
  year = {2023},
  month = jan,
  number = {arXiv:2201.01194},
  eprint = {2201.01194},
  primaryclass = {econ, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.01194},
  urldate = {2023-02-06},
  abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of ``canonical'' assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on \$(i)\$ multiple periods and variation in treatment timing, \$(ii)\$ potential violations of parallel trends, or \$(iii)\$ alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/IQ4VH3BZ/Roth et al_2023_What's Trending in Difference-in-Differences.pdf;/Users/fpalomba/Zotero/storage/KD42QJTD/2201.html}
}

@article{roth2023WhenParallelTrends,
  title = {When {{Is Parallel Trends Sensitive}} to {{Functional Form}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
  year = {2023},
  journal = {Econometrica},
  volume = {91},
  number = {2},
  pages = {737--747},
  issn = {1468-0262},
  doi = {10.3982/ECTA19402},
  urldate = {2023-03-24},
  abstract = {This paper assesses when the validity of difference-in-differences depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger ``parallel trends''-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition for parallel trends to be insensitive to functional form is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. These conditions have testable implications, and we introduce falsification tests for the null that parallel trends is insensitive to functional form.},
  langid = {english},
  keywords = {Difference-in-differences,functional form,robustness,testable implications},
  file = {/Users/fpalomba/Zotero/storage/3T7ISNS6/Roth_Sant'Anna_2023_When Is Parallel Trends Sensitive to Functional Form.pdf;/Users/fpalomba/Zotero/storage/8YICCC6G/ECTA19402.html}
}

@article{scott1973CentralLimitTheorems,
  title = {Central {{Limit Theorems}} for {{Martingales}} and for {{Processes}} with {{Stationary Increments Using}} a {{Skorokhod Representation Approach}}},
  author = {Scott, D. J.},
  year = {1973},
  journal = {Advances in Applied Probability},
  volume = {5},
  number = {1},
  eprint = {1425967},
  eprinttype = {jstor},
  pages = {119--137},
  publisher = {{Applied Probability Trust}},
  issn = {0001-8678},
  doi = {10.2307/1425967},
  urldate = {2023-03-31},
  abstract = {The Skorokhod representation for martingales is used to obtain a functional central limit theorem (or invariance principle) for martingales. It is clear from the method of proof that this result may in fact be extended to the case of triangular arrays in which each row is a martingale sequence and the second main result is a functional central limit theorem for such arrays. These results are then used to obtain two functional central limit theorems for processes with stationary ergodic increments following on from the work of Gordin. The first of these theorems extends a result of Billingsley for {$\varphi$} -mixing sequences.},
  file = {/Users/fpalomba/Zotero/storage/6UQE48CP/Scott_1973_Central Limit Theorems for Martingales and for Processes with Stationary.pdf}
}

@article{shao2007AsymptoticSpectralTheory,
  title = {Asymptotic {{Spectral Theory}} for {{Nonlinear Time Series}}},
  author = {Shao, Xiaofeng and Wu, Wei Biao},
  year = {2007},
  journal = {The Annals of Statistics},
  volume = {35},
  number = {4},
  eprint = {25464559},
  eprinttype = {jstor},
  pages = {1773--1801},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364},
  doi = {10.1214/009053606000001479},
  urldate = {2023-05-28},
  abstract = {We consider asymptotic problems in spectral analysis of stationary causal processes. Limiting distributions of periodograms and smoothed periodogram spectral density estimates are obtained and applications to the spectral domain bootstrap are given. Instead of the commonly used strong mixing conditions, in our asymptotic spectral theory we impose conditions only involving (conditional) moments, which are easily verifiable for a variety of nonlinear time series.},
  file = {/Users/fpalomba/Zotero/storage/MXTESVRH/Shao_Wu_2007_Asymptotic spectral theory for nonlinear time series.pdf}
}

@article{shao2007LocalWhittleEstimation,
  title = {Local {{Whittle}} Estimation of Fractional Integration for Nonlinear Processes},
  author = {Shao, Xiaofeng and Wu, Wei Biao},
  year = {2007},
  journal = {Econometric Theory},
  volume = {23},
  number = {5},
  pages = {899--929},
  publisher = {{Cambridge University Press}},
  file = {/Users/fpalomba/Zotero/storage/7TLZM6U5/Shao_Wu_2007_Local Whittle estimation of fractional integration for nonlinear processes.pdf;/Users/fpalomba/Zotero/storage/ZF4F45SX/6282132A69DEED0334103557ED297BA8.html}
}

@article{stoica1999OptimallySmoothedPeriodogram,
  title = {Optimally Smoothed Periodogram},
  author = {Stoica, Petre and Sundin, Tomas},
  year = {1999},
  month = nov,
  journal = {Signal Processing},
  volume = {78},
  number = {3},
  pages = {253--264},
  issn = {0165-1684},
  doi = {10.1016/S0165-1684(99)00066-3},
  urldate = {2023-02-23},
  abstract = {Locally smoothing the periodogram yields one of the most successful methods for non-parametric spectral estimation. Nevertheless, no much guideline was available for choosing the window parameters and span for smoothing. Here we derive the window parameters and span that minimize an asymptotically unbiased estimate of the spectral estimation error variance. The so-obtained optimally smoothed periodogram is shown to outperform the commonly used uniformly smoothed periodogram. Furthermore, a technique is presented where the window parameters are chosen separately for each of a number of different frequency bands. This technique reduces the variance of the spectral estimates even further, at the price of an increased computational complexity. Zusammenfassung Die lokale Gl\"attung des Periodogramms ergibt eine der erfolgreichsten Methoden zur nichtparametrischen Spektralsch\"atzung. Allerdings existieren kaum Richtlinien f\"ur die Wahl der bei der Gl\"attung verwendeten Fensterparameter und -spannweite. In dieser Arbeit leiten wir jene Fensterparameter und -spannweite ab, welche einen asymptotisch erwartungstreuen Sch\"atzwert der Varianz des Spektralsch\"atzfehlers minimieren. Es wird gezeigt, da\ss{} das derart erhaltene optimal gegl\"attete Periodogramm das \"ublicherweise verwendete gleichf\"ormig gegl\"attete Periodogramm an Leistungsf\"ahigkeit \"ubertrifft. Weiters wird eine Methode pr\"asentiert, bei der die Fensterparameter separat f\"ur jedes Frequenzband innerhalb einer bestimmten Gruppe von verschiedenen Frequenzb\"andern gew\"ahlt werden. Diese Methode verringert die Varianz des Spektralsch\"atzers sogar noch weiter, was mit einem h\"oheren Rechenaufwand erkauft wird. R\'esum\'e Le lissage local du p\'eriodogramme constitue l'une des m\'ethodes les plus fructueuses pour l'estimation spectrale non param\'etrique. Toutefois aucune indication n'existait jusqu'\`a pr\'esent pour le choix des param\`etres de la fen\^etre et de l'\'etendue du lissage. Nous d\'erivons ici les param\`etres de la fen\^etre et l'\'etendue qui minimisent une estim\'ee asymptotiquement non biais\'ee de la variance de l'erreur d'estimation spectrale. Le p\'eriodogramme optimalement liss\'e obtenu ainsi est montr\'e surpasser le p\'eriodogramme liss\'e commun\'ement utilis\'e. De plus une technique dans laquelle les param\`etres de la fen\^etre sont choisis s\'epar\'ement pour une certain nombre de bandes de fr\'equences diff\'erentes est pr\'esent\'ee. Cette technique r\'eduit encore davantage la variance des estim\'ees spectrales, au prix d'une complexit\'e de calcul accrue.},
  langid = {english},
  keywords = {Non-parametric methods,Optimal spectral window,Smoothed periodogram,Spectral analysis},
  file = {/Users/fpalomba/Zotero/storage/TWZ5YR65/Stoica_Sundin_1999_Optimally smoothed periodogram.pdf;/Users/fpalomba/Zotero/storage/7CVZ85LN/S0165168499000663.html}
}

@article{sun2021EstimatingDynamicTreatmenta,
  title = {Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects},
  author = {Sun, Liyang and Abraham, Sarah},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {175--199},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.09.006},
  urldate = {2023-02-06},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/7WH3JBIL/Sun_Abraham_2021_Estimating dynamic treatment effects in event studies with heterogeneous.pdf}
}

@article{thurstone1927LawComparativeJudgment,
  title = {A Law of Comparative Judgment.},
  author = {Thurstone, Louis L.},
  year = {1927},
  journal = {Psychological review},
  volume = {34},
  number = {4},
  pages = {273},
  publisher = {{Psychological Review Company}},
  file = {/Users/fpalomba/Zotero/storage/NRBERHAX/Thurstone_1927_A law of comparative judgment.pdf;/Users/fpalomba/Zotero/storage/FSH8INHL/1928-00527-001.html}
}

@article{wahba1980AutomaticSmoothingLoga,
  title = {Automatic {{Smoothing}} of the {{Log Periodogram}}},
  author = {Wahba, Grace},
  year = {1980},
  journal = {Journal of the American Statistical Association},
  volume = {75},
  number = {369},
  eprint = {2287399},
  eprinttype = {jstor},
  pages = {122--132},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2287399},
  urldate = {2023-02-23},
  abstract = {We develop an objective optimum smoothing procedure for an estimate of the log spectral density, based on smoothing the log periodogram with a smoothing spline. We call the result the optimally smoothed spline (OSS) estimate. We show that an unbiased estimate {\^R}({$\lambda$}) of the expected integrated mean square error can be obtained as a function of the smoothing, or "bandwidth" parameter {$\lambda$}. The smoothing parameter then is chosen as that {$\lambda$} that minimizes {\^R}({$\lambda$}). The degree of the smoothing spline (equivalently, the "shape" parameter of the window) also can be chosen this way. Results of some Monte Carlo experiments demonstrating the effectiveness of the method on selected examples are given.},
  file = {/Users/fpalomba/Zotero/storage/SP7358C9/Wahba_1980_Automatic Smoothing of the Log Periodogram.pdf}
}

@book{white2000AsymptoticTheoryEconometricians,
  title = {Asymptotic {{Theory}} for {{Econometricians}}},
  author = {White, Halbert},
  year = {2000},
  month = oct,
  publisher = {{Emerald Group Publishing Limited}},
  abstract = {This book provides the tools and concepts necessary to study the behavior of econometric estimators and test statistics in large samples. An econometric estimator is a solution to an optimization problem; that is, a problem that requires a body of techniques to determine a specific solution in a defined set of possible alternatives that best satisfies a selected object function or set of constraints. Thus, this highly mathematical book investigates situations concerning large numbers, in which the assumptions of the classical linear model fail. Economists, of course, face these situations often. It includes completely revised chapter seven on functional central limit theory and its applications, specifically unit root regression, spurious regression, and regression with cointegrated processes. It includes updated material on: central limit theory; asymptotically efficient instrumental variables estimation; estimation of asymptotic covariance matrices; efficient estimation with estimated error covariance matrices; and efficient IV estimation.},
  googlebooks = {FyG3AAAAIAAJ},
  isbn = {978-0-12-746652-1},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Economics / Theory},
  file = {/Users/fpalomba/Zotero/storage/FFAAHGID/White_2000_Asymptotic Theory for Econometricians.pdf}
}

@article{whittle1957CurvePeriodogramSmoothing,
  title = {Curve and {{Periodogram Smoothing}}},
  author = {Whittle, P.},
  year = {1957},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {19},
  number = {1},
  eprint = {2983994},
  eprinttype = {jstor},
  pages = {38--63},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  urldate = {2023-02-23},
  abstract = {The difficulty in constructing smoothing formulae is to express quantitatively the type of smoothness one expects of the curve one is estimating. An argument is given in Sections 1 and 3 for formulating this "smoothness hypothesis" in terms of the properties of a population of curves of which the curve being estimated is a member. In equation (20) we obtain a solution for the matrix of optimum weighting coefficients in terms of certain "population moments" of the ordinates of the curve. Explicit formulae based on special assumptions are deduced in equations (34), (56)-(58). General information is gained on the way the optimum smoothing function and the variance of the smoothed estimate vary with the sample size and with the assumed degree of smoothness of the parent curve.},
  file = {/Users/fpalomba/Zotero/storage/4R9MTDZB/Whittle_1957_Curve and Periodogram Smoothing.pdf}
}

@article{withers1981CentralLimitTheorems,
  title = {Central {{Limit Theorems}} for Dependent Variables. {{I}}},
  author = {Withers, C. S.},
  year = {1981},
  month = dec,
  journal = {Zeitschrift f\"ur Wahrscheinlichkeitstheorie und Verwandte Gebiete},
  volume = {57},
  number = {4},
  pages = {509--534},
  issn = {1432-2064},
  doi = {10.1007/BF01025872},
  urldate = {2023-03-17},
  abstract = {This paper gives a flexible approach to proving the Central Limit Theorem (C.L.T.) for triangular arrays of dependent random variables (r.v.s) which satisfy a weak `mixing' condition called {$\mathscr{l}$}-mixing. Roughly speaking, an array of real r.v.s is said to be {$\mathscr{l}$}-mixing if linear combinations of its `past' and `future' are asymptotically independent. All the usual mixing conditions (such as strong mixing, absolute regularity, uniform mixing, {$\varrho$}-mixing and {$\psi$}-mixing) are special cases of {$\mathscr{l}$}-mixing. Linear processes are shown to be {$\mathscr{l}$}-mixing under weak conditions. The main result makes no assumption of stationarity. A secondary result generalises a C.L.T. that Rosenblatt gave for strong mixing samples which are `nearly second order stationary'.},
  langid = {english},
  keywords = {Linear Combination,Probability Theory,Result Generalise,Secondary Result,Stochastic Process},
  file = {/Users/fpalomba/Zotero/storage/598VVTVW/Withers - 1981 - Central Limit Theorems for dependent variables. I.pdf}
}

@phdthesis{wooldridge1986AsymptoticPropertiesEconometric,
  title = {Asymptotic {{Properties}} of {{Econometric Estimators}}},
  author = {Wooldridge, Jeffrey M.},
  year = {1986},
  address = {{United States -- California}},
  urldate = {2023-03-21},
  abstract = {It has long been recognized that deriving the finite sample properties of econometric estimators is often intractable. Consequently, asymptotic analysis has played an important role in econometrics. General results proceed under the observation that many problems have a common mathematical structure. Though current asymptotic analysis allows consideration of many models, certain restrictive features rule out situations of interest to economists. Current unified frameworks exclude nonparametric procedures, discontinuous and nonsmooth objective functions, and models with dependent, trending observations. The first chapter of this dissertation examines the strong consistency of estimators that solve a minimization problem. The general theorem allows for most situations of interest. Examples include nonparametric estimation by the method of sieves, quasi maximum likelihood estimation and a rank correlation estimator. Chapter two developes central limit theorems and invariance principles suitable for econometric applications. Because economists deal frequently with time series data, it is vital to have available a CLT which allows for dependence and trending moments. The CLT derived in chapter two is applicable to functions of entire histories of mixing processes. Trending moments are also accommodated, thereby filling a void left by previous CLTs. Chapter two also establishes an invariance principle which is applicable to functions of mixing processes. For doubly indexed arrays it relaxes the dependence and asymptotic covariance stationarity requirements previously imposed. Chapter three investigates limiting distributions of econometric estimators. The general analysis leads easily to consideration of both smooth and nonsmooth problems. For smooth problems, that is those for which the score has a nontrivial derivative, results are obtained that do not require uniform convergence of the Hessian. Moreover, for estimators that solve a smooth minimization problem, conditions are given that allow trending variables in nonlinear models. By applying the invariance principle of chapter two, results for linear models containing unit roots and means nonlinear in time are obtained. These results extend those currently available. Finally, chapter three presents analysis of nonsmooth problems. The example given is least absolute deviations estimation in a dynamic linear model.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798206508765},
  langid = {english},
  school = {University of California, San Diego},
  keywords = {Social sciences},
  file = {/Users/fpalomba/Zotero/storage/E9665DRW/WOOLDRIDGE_Asymptotic Properties of Econometric Estimators.pdf}
}

@article{wooldridge1988InvariancePrinciplesCentral,
  title = {Some {{Invariance Principles}} and {{Central Limit Theorems}} for {{Dependent Heterogeneous Processes}}},
  author = {Wooldridge, Jeffrey M. and White, Halbert},
  year = {1988},
  journal = {Econometric Theory},
  volume = {4},
  number = {2},
  eprint = {3532293},
  eprinttype = {jstor},
  pages = {210--230},
  publisher = {{Cambridge University Press}},
  issn = {0266-4666},
  urldate = {2023-03-21},
  abstract = {Building on work of McLeish [14,15], we present a number of invariance principles for doubly indexed arrays of stochastic processes which may exhibit considerable dependence, heterogeneity, and/or trending moments. In particular, we consider possibly time-varying functions of infinite histories of heterogeneous mixing processes and obtain general invariance results, with central limit theorems following as corollaries. These results are formulated so as to apply to economic time series, which may exhibit some or all of the features allowed in our theorems. Results are given for the case of both scalar and vector stochastic processes. Using an approach recently pioneered by Phillips [19-21], and Phillips and Durlauf [23], we apply our results to least squares estimation of unit root models.},
  file = {/Users/fpalomba/Zotero/storage/229UMLGJ/Wooldridge_White_1988_Some Invariance Principles and Central Limit Theorems for Dependent.pdf}
}

@book{wooldridge2018IntroductoryEconometricsModern,
  title = {Introductory {{Econometrics}}: {{A Modern Approach}}},
  shorttitle = {Introductory {{Econometrics}}},
  author = {Wooldridge, Jeffrey M.},
  year = {2018},
  month = sep,
  edition = {Seventh},
  publisher = {{Cengage Learning}},
  abstract = {Discover how empirical researchers today actually think about and apply econometric methods with the practical, professional approach in Wooldridge's INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 6E. Unlike traditional books, this unique presentation demonstrates how econometrics has moved beyond just a set of abstract tools to become genuinely useful for answering questions in business, policy evaluation, and forecasting environments. INTRODUCTORY ECONOMETRICS is organized around the type of data being analyzed with a systematic approach that only introduces assumptions as they are needed. This makes the material easier to understand and, ultimately, leads to better econometric practices. Packed with timely, relevant applications, the book introduces the latest emerging developments in the field. Gain a full understanding of the impact of econometrics in real practice today with the insights and applications found only in INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 6E.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
  googlebooks = {wUF4BwAAQBAJ},
  isbn = {978-1-305-44638-0},
  langid = {english},
  keywords = {Business \& Economics / Econometrics},
  file = {/Users/fpalomba/Zotero/storage/SJG5YRFA/Wooldridge_2015_Introductory Econometrics.pdf}
}

@techreport{wooldridge2022TwoWayFixedEffects,
  title = {Two-{{Way Fixed Effects}}, the {{Two-Way Mundlak Regression}}, and {{Difference-in-Differences Estimators}} by {{Jeffrey M}}. {{Wooldridge}} :: {{SSRN}}},
  author = {Wooldridge, Jeffrey M.},
  year = {2022},
  institution = {{Michigan University}},
  urldate = {2023-02-06},
  file = {/Users/fpalomba/Zotero/storage/8YEH3EU3/Wooldridge_Two-Way Fixed Effects, the Two-Way Mundlak Regression, and.pdf;/Users/fpalomba/Zotero/storage/JXTFXQD7/papers.html}
}

@article{wu2004LimitTheoremsIterated,
  title = {Limit Theorems for Iterated Random Functions},
  author = {Wu, Wei Biao and Shao, Xiaofeng},
  year = {2004},
  journal = {Journal of Applied Probability},
  volume = {41},
  number = {2},
  pages = {425--436},
  publisher = {{Cambridge University Press}},
  file = {/Users/fpalomba/Zotero/storage/LDINW87C/Wu_Shao_2004_Limit theorems for iterated random functions.pdf;/Users/fpalomba/Zotero/storage/4WB8CZ2E/9CA522B1DE090C02B7374ED8212739AB.html}
}

@article{wu2005FourierTransformsStationary,
  title = {Fourier {{Transforms}} of {{Stationary Processes}}},
  author = {Wu, Wei Biao},
  year = {2005},
  journal = {Proceedings of the American Mathematical Society},
  volume = {133},
  number = {1},
  eprint = {4097852},
  eprinttype = {jstor},
  pages = {285--293},
  publisher = {{American Mathematical Society}},
  issn = {0002-9939},
  urldate = {2023-03-31},
  abstract = {We consider the asymptotic behavior of Fourier transforms of stationary and ergodic sequences. Under sufficiently mild conditions, central limit theorems are established for almost all frequencies as well as for a given frequency. Applications to the widely used linear processes and iterated random functions are discussed. Our results shed new light on the foundation of spectral analysis in that the asymptotic distribution of the periodogram, the fundamental quantity in the frequency-domain analysis, is obtained.},
  file = {/Users/fpalomba/Zotero/storage/TPKE4YKU/Wu_2005_Fourier Transforms of Stationary Processes.pdf}
}

@article{wu2005NonlinearSystemTheory,
  title = {Nonlinear System Theory: {{Another}} Look at Dependence},
  shorttitle = {Nonlinear System Theory},
  author = {Wu, Wei Biao},
  year = {2005},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {102},
  number = {40},
  pages = {14150--14154},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.0506715102},
  urldate = {2023-03-28},
  abstract = {Based on the nonlinear system theory, we introduce previously undescribed dependence measures for stationary causal processes. Our physical and predictive dependence measures quantify the degree of dependence of outputs on inputs in physical systems. The proposed dependence measures provide a natural framework for a limit theory for stationary processes. In particular, under conditions with quite simple forms, we present limit theorems for partial sums, empirical processes, and kernel density estimates. The conditions are mild and easily verifiable because they are directly related to the data-generating mechanisms.},
  file = {/Users/fpalomba/Zotero/storage/XW7KTEV8/Wu_2005_Nonlinear system theory.pdf}
}

@article{wu2007LimitTheoremQuadratic,
  title = {A Limit Theorem for Quadratic Forms and Its Applications},
  author = {Wu, Wei Biao and Shao, Xiaofeng},
  year = {2007},
  journal = {Econometric Theory},
  volume = {23},
  number = {5},
  pages = {930--951},
  publisher = {{Cambridge University Press}},
  file = {/Users/fpalomba/Zotero/storage/LPT3RZ23/Wu_Shao_2007_A limit theorem for quadratic forms and its applications.pdf;/Users/fpalomba/Zotero/storage/QL3FZPQP/1B96D429D3393A8144807E9DFA07E5B4.html}
}

@article{yao2007SpectralDensityEstimation,
  title = {Spectral {{Density Estimation Using Sharpened Periodograms}}},
  author = {Yao, Fang and Lee, Thomas C. M.},
  year = {2007},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {55},
  number = {9},
  pages = {4711--4716},
  issn = {1941-0476},
  doi = {10.1109/TSP.2007.896297},
  abstract = {This correspondence introduces the use of sharpened periodograms for spectral density estimation. It is shown analytically that spectrum estimates obtained from smoothing the sharpened periodograms enjoy higher order bias reduction when compared with the ordinary smoothed periodogram estimates. The promising numerical performances of using sharpened periodograms for spectral density estimation are illustrated via numerical experiments.},
  keywords = {Bias reduction,Data preprocessing,data sharpening,Kernel,periodogram smoothing,Probability,sharpened periodograms,Smoothing methods,spectral density estimation,Statistics,unbiased risk estimation},
  file = {/Users/fpalomba/Zotero/storage/RMF8HGEW/Yao_Lee_2007_Spectral Density Estimation Using Sharpened Periodograms.pdf;/Users/fpalomba/Zotero/storage/893N5J79/4291877.html}
}

@misc{zhang2022BoundsSemiparametricInference,
  title = {Bounds and Semiparametric Inference in \${{L}}\^\textbackslash infty\$- and \${{L}}\^2\$-Sensitivity Analysis for Observational Studies},
  author = {Zhang, Yao and Zhao, Qingyuan},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04697},
  eprint = {2211.04697},
  primaryclass = {math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.04697},
  urldate = {2022-12-16},
  abstract = {Sensitivity analysis for the unconfoundedness assumption is a crucial component of observational studies. The marginal sensitivity model has become increasingly popular for this purpose due to its interpretability and mathematical properties. After reviewing the original marginal sensitivity model that imposes a \$L\^\textbackslash infty\$-constraint on the maximum logit difference between the observed and full data propensity scores, we introduce a more flexible \$L\^2\$-analysis framework; sensitivity value is interpreted as the "average" amount of unmeasured confounding in the analysis. We derive analytic solutions to the stochastic optimization problems under the \$L\^2\$-model, which can be used to bound the average treatment effect (ATE). We obtain the efficient influence functions for the optimal values and use them to develop efficient one-step estimators. We show that multiplier bootstrap can be applied to construct a simultaneous confidence band of the ATE. Our proposed methods are illustrated by simulation and real-data studies.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/GS56Y4GI/Zhang and Zhao - 2022 - Bounds and semiparametric inference in $L^infty$-.pdf;/Users/fpalomba/Zotero/storage/7F78EZM9/2211.html}
}
