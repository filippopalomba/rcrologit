@article{abadie2010SyntheticControlMethods,
  title = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}: {{Estimating}} the {{Effect}} of {{California}}'s {{Tobacco Control Program}}},
  shorttitle = {Synthetic {{Control Methods}} for {{Comparative Case Studies}}},
  author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
  year = {2010},
  journal = {Journal of the American Statistical Association},
  volume = {105},
  number = {490},
  eprint = {29747059},
  eprinttype = {jstor},
  pages = {493--505},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  abstract = {Building on an idea in Abadie and Gardeazabal (2003), this article investigates the application of synthetic control methods to comparative case studies. We discuss the advantages of these methods and apply them to study the effects of Proposition 99, a large-scale tobacco control program that California implemented in 1988. We demonstrate that, following Proposition 99, tobacco consumption fell markedly in California relative to a comparable synthetic control region. We estimate that by the year 2000 annual per-capita cigarette sales in California were about 26 packs lower than what they would have been in the absence of Proposition 99. Using new inferential methods proposed in this article, we demonstrate the significance of our estimates. Given that many policy interventions and events of interest in social sciences take place at an aggregate level (countries, regions, cities, etc.) and affect a small number of aggregate units, the potential applicability of synthetic control methods to comparative case studies is very large, especially in situations where traditional regression methods are not appropriate.},
  file = {/Users/fpalomba/Zotero/storage/S2RFYVKY/Abadie et al_2010_Synthetic Control Methods for Comparative Case Studies.pdf}
}

@misc{agarwal2022CausalInferenceCorrupted,
  title = {Causal {{Inference}} with {{Corrupted Data}}: {{Measurement Error}}, {{Missing Values}}, {{Discretization}}, and {{Differential Privacy}}},
  shorttitle = {Causal {{Inference}} with {{Corrupted Data}}},
  author = {Agarwal, Anish and Singh, Rahul},
  year = {2022},
  month = nov,
  number = {arXiv:2107.02780},
  eprint = {arXiv:2107.02780},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2107.02780},
  abstract = {The US Census Bureau will deliberately corrupt data sets derived from the 2020 US Census in an effort to maintain privacy, suggesting a painful trade-off between the privacy of respondents and the precision of economic analysis. To investigate whether this trade-off is inevitable, we formulate a semiparametric model of causal inference with high dimensional corrupted data. We propose a procedure for data cleaning, estimation, and inference with data cleaning-adjusted confidence intervals. We prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments, with a rate of \$n\^\{-1/2\}\$ for semiparametric estimands that degrades gracefully for nonparametric estimands. Our key assumption is that the true covariates are approximately low rank, which we interpret as approximate repeated measurements and validate in the Census. In our analysis, we provide nonasymptotic theoretical contributions to matrix completion, statistical learning, and semiparametric statistics. Calibrated simulations verify the coverage of our data cleaning-adjusted confidence intervals and demonstrate the relevance of our results for 2020 Census data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Economics - Econometrics,G.3,J.4,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/fpalomba/Zotero/storage/WFZU6T9K/Agarwal_Singh_2022_Causal Inference with Corrupted Data.pdf;/Users/fpalomba/Zotero/storage/LECI3SGH/2107.html}
}

@article{angrist1998EstimatingLaborMarket,
  title = {Estimating the {{Labor Market Impact}} of {{Voluntary Military Service Using Social Security Data}} on {{Military Applicants}}},
  author = {Angrist, Joshua D.},
  year = {1998},
  journal = {Econometrica},
  volume = {66},
  number = {2},
  eprint = {2998558},
  eprinttype = {jstor},
  pages = {249--288},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/2998558},
  abstract = {The volunteer armed forces play a major role in the American youth labor market, but little is known about the effects of voluntary military service on earnings. The effects of military service are difficult to measure because veterans are both self-selected and screened by the military. This study uses two strategies to reduce selection bias in estimates of the effects of military service on the earnings of veterans. Both approaches involve the analysis of a special match of Social Security earning records to administrative data on applicants to the armed forces. The first strategy compares applicants who enlisted with applicants who did not enlist, while controlling for most of the characteristics used by the military to select soldiers from the applicant pool. This is implemented using matching methods and regression. The second strategy uses instrumental variables that were generated by an error in the scoring of the exams that screen military applicants. Estimates from both strategies are interpreted using models with heterogeneous potential outcomes. The empirical results suggest that soldiers who served in the early 1980s were paid considerably more than comparable civilians while in the military, and that military service is associated with higher employment rates for veterans after service. In spite of this employment gain, however, military service led to only a modest long-run increase in the civilian earnings of nonwhite veterans while actually reducing the civilian earnings of white veterans.},
  file = {/Users/fpalomba/Zotero/storage/JYUCSPRP/Angrist_1998_Estimating the Labor Market Impact of Voluntary Military Service Using Social.pdf}
}

@misc{arkhangelsky2022RolePropensityScore,
  title = {The {{Role}} of the {{Propensity Score}} in {{Fixed Effect Models}}},
  author = {Arkhangelsky, Dmitry and Imbens, Guido},
  year = {2022},
  month = apr,
  number = {arXiv:1807.02099},
  eprint = {arXiv:1807.02099},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.02099},
  abstract = {We develop a new approach for estimating average treatment effects in observational studies with unobserved group-level heterogeneity. We consider a general model with group-level unconfoundedness and provide conditions under which aggregate statistics -- group-level averages of functions of treatments and covariates -- are sufficient to eliminate differences between groups. Building on these results, we generalize commonly used linear fixed-effect regression estimators in three ways. First, we allow researchers to explicitly select sufficient statistics themselves, whereas the standard specifications make this choice implicit. Second, we suggest using flexible adjustments for sufficient statistics. Finally, we propose robustifying the regression estimators using inverse propensity score weighting. In practice, we recommend researchers use all three modifications.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/UA3KHAS3/Arkhangelsky_Imbens_2022_The Role of the Propensity Score in Fixed Effect Models.pdf;/Users/fpalomba/Zotero/storage/VF5IA92Y/1807.html}
}

@article{baltagi2012LagrangeMultiplierTest,
  title = {A {{Lagrange Multiplier}} Test for Cross-Sectional Dependence in a Fixed Effects Panel Data Model},
  author = {Baltagi, Badi H. and Feng, Qu and Kao, Chihwa},
  year = {2012},
  month = sep,
  journal = {Journal of Econometrics},
  volume = {170},
  number = {1},
  pages = {164--177},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2012.04.004},
  abstract = {It is well known that the standard Breusch and Pagan (1980) LM test for cross-equation correlation in a SUR model is not appropriate for testing cross-sectional dependence in panel data models when the number of cross-sectional units (n) is large and the number of time periods (T) is small. In fact, a scaled version of this LM test was proposed by Pesaran (2004) and its finite sample bias was corrected by Pesaran et~al. (2008). This was done in the context of a heterogeneous panel data model. This paper derives the asymptotic bias of this scaled version of the LM test in the context of a fixed effects homogeneous panel data model. This asymptotic bias is found to be a constant related to n and T, which suggests a simple bias corrected LM test for the null hypothesis. Additionally, the paper carries out some Monte Carlo experiments to compare the finite sample properties of this proposed test with existing tests for cross-sectional dependence.},
  langid = {english},
  keywords = {Cross-sectional dependence,Fixed effects,High dimensional inference,John test,Panel data,test},
  file = {/Users/fpalomba/Zotero/storage/A8Q4VA3C/Baltagi et al_2012_A Lagrange Multiplier test for cross-sectional dependence in a fixed effects.pdf;/Users/fpalomba/Zotero/storage/K9SAMJ6D/S030440761200098X.html}
}

@article{beggs1981AssessingPotentialDemand,
  title = {Assessing the Potential Demand for Electric Cars},
  author = {Beggs, S and Cardell, S and Hausman, J},
  year = {1981},
  month = sep,
  journal = {Journal of Econometrics},
  volume = {17},
  number = {1},
  pages = {1--19},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(81)90056-7},
  abstract = {An ordered logit specification for use on ranked individual data is used to analyze survey data on potential consumer demand for electric cars. In many situations in economics and marketing we would like to be able to forecast consumer demands for goods which have not yet appeared in actual markets. By defining goods as a bundle of underlying attributes, we can use discrete choice models to estimate consumer evaluations. Then new good demand is forecast by use of the estimated coefficients to compare consumer evaluation of the new good to existing choices. When ranked individual data are available, we can estimate separate coefficients for each individual rather than assuming identical coefficients as is usual with logit models. Our results indicate considerable dispersion in individual coefficients. This finding can have important implications for new product analysis.},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CFG8C86J/Beggs et al_1981_Assessing the potential demand for electric cars.pdf;/Users/fpalomba/Zotero/storage/9PZWBN6Y/0304407681900567.html}
}

@article{beltrato1987DeterminingBandwidthKernela,
  title = {Determining the {{Bandwidth}} of a {{Kernel Spectrum Estimate}}},
  author = {BeltraTo, Kaiz{\^o} I. and Bloomfield, Peter},
  year = {1987},
  journal = {Journal of Time Series Analysis},
  volume = {8},
  number = {1},
  pages = {21--38},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1987.tb00418.x},
  abstract = {Abstract. A cross-validated form of Whittle's frequency domain approximation to the likelihood function of a stationary Gaussian process is described, and proposed as a criterion for choosing the bandwidth in a kernel spectrum estimate. The criterion is shown to be equivalent, in large samples, to the mean integrated squared error. The statistical properties of the spectrum estimate whose bandwidth maximizes the criterion have been explored in a limited simulation.},
  langid = {english},
  keywords = {cross-validated likelihood,Density estimation,smoothing parameter},
  file = {/Users/fpalomba/Zotero/storage/U5LLHT43/BeltraTo_Bloomfield_1987_Determining the Bandwidth of a Kernel Spectrum Estimate.pdf;/Users/fpalomba/Zotero/storage/7A3CRS5V/j.1467-9892.1987.tb00418.html}
}

@misc{blandhol2022WhenTSLSActually,
  type = {Working {{Paper}}},
  title = {When Is {{TSLS Actually LATE}}?},
  author = {Blandhol, Christine and Bonney, John and Mogstad, Magne and Torgovitsky, Alexander},
  year = {2022},
  month = jan,
  series = {Working {{Paper Series}}},
  number = {29709},
  eprint = {29709},
  publisher = {{National Bureau of Economic Research}},
  doi = {10.3386/w29709},
  abstract = {Linear instrumental variable estimators, such as two-stage least squares (TSLS), are commonly interpreted as estimating positively weighted averages of causal effects, referred to as local average treatment effects (LATEs). We examine whether the LATE interpretation actually applies to the types of TSLS specifications that are used in practice. We show that if the specification includes covariates\textemdash which most empirical work does\textemdash then the LATE interpretation does not apply in general. Instead, the TSLS estimator will, in general, reflect treatment effects for both compliers and always/nevertakers, and some of the treatment effects for the always/never-takers will necessarily be negatively weighted. We show that the only specifications that have a LATE interpretation are "saturated" specifications that control for covariates nonparametrically, implying that such specifications are both sufficient and necessary for TSLS to have a LATE interpretation, at least without additional parametric assumptions. This result is concerning because, as we document, empirical researchers almost never control for covariates nonparametrically, and rarely discuss or justify parametric specifications of covariates. We develop a decomposition that quantifies the extent to which the usual LATE interpretation fails. We apply the decomposition to four empirical analyses and find strong evidence that the LATE interpretation of TSLS is far from accurate for the types of specifications actually used in practice.},
  archiveprefix = {National Bureau of Economic Research},
  file = {/Users/fpalomba/Zotero/storage/4INPD5BU/Blandhol et al_2022_When is TSLS Actually LATE.pdf}
}

@book{brillinger1981TimeSeriesData,
  title = {Time {{Series}}: {{Data Analysis}} and {{Theory}}},
  shorttitle = {Time {{Series}}},
  author = {Brillinger, David R.},
  year = {1981},
  publisher = {{Holden-Day}},
  abstract = {The nature of time series and their frequency analysis. Foundations. Analytic properties of fourier transforms and complex matrices. Stochastic properties of finite fourier transforms. The estimation of power spectra. Analysis of a linear time invariant relation between a stochastic series and several deterministic series. Estimating the second-order spectra of vector-valued series. Analysis of a linear time invariant between two vector-valued stochastic series. Principal components in the frequency domain. The canonical analysis of time series. Proofs of theorems.},
  googlebooks = {iOdQAAAAMAAJ},
  isbn = {978-0-8162-1150-0},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/C6DJ43RG/Brillinger_1981_Time Series.pdf}
}

@book{brockwell2009TimeSeriesTheory,
  title = {Time {{Series}}: {{Theory}} and {{Methods}}},
  shorttitle = {Time {{Series}}},
  author = {Brockwell, Peter J. and Davis, Richard A.},
  year = {2009},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {This edition contains a large number of additions and corrections scattered throughout the text, including the incorporation of a new chapter on state-space models. The companion diskette for the IBM PC has expanded into the software package ITSM: An Interactive Time Series Modelling Package for the PC, which includes a manual and can be ordered from Springer-Verlag. * We are indebted to many readers who have used the book and programs and made suggestions for improvements. Unfortunately there is not enough space to acknowledge all who have contributed in this way; however, special mention must be made of our prize-winning fault-finders, Sid Resnick and F. Pukelsheim. Special mention should also be made of Anthony Brockwell, whose advice and support on computing matters was invaluable in the preparation of the new diskettes. We have been fortunate to work on the new edition in the excellent environments provided by the University of Melbourne and Colorado State University. We thank Duane Boes particularly for his support and encouragement throughout, and the Australian Research Council and National Science Foundation for their support of research related to the new material. We are also indebted to Springer-Verlag for their constant support and assistance in preparing the second edition. Fort Collins, Colorado P. J. BROCKWELL November, 1990 R. A. DAVIS * /TSM: An Interactive Time Series Modelling Package for the PC by P. J. Brockwell and R. A. Davis. ISBN: 0-387-97482-2; 1991.},
  googlebooks = {TVIpBgAAQBAJ},
  isbn = {978-1-4419-0320-4},
  langid = {english},
  keywords = {Business \& Economics / Econometrics,Business \& Economics / Statistics,Mathematics / Probability \& Statistics / General,Mathematics / Probability \& Statistics / Stochastic Processes},
  file = {/Users/fpalomba/Zotero/storage/24LNBKRP/Brockwell_Davis_2009_Time Series.pdf}
}

@article{buhlmann1996LocallyAdaptiveLagWindow,
  title = {Locally {{Adaptive Lag-Window Spectral Estimation}}},
  author = {B{\"u}hlmann, Peter},
  year = {1996},
  journal = {Journal of Time Series Analysis},
  volume = {17},
  number = {3},
  pages = {247--270},
  issn = {1467-9892},
  doi = {10.1111/j.1467-9892.1996.tb00275.x},
  abstract = {Abstract. We propose a procedure for the locally optimal window width in nonparametric spectral estimation, minimizing the asymptotic mean square error at a fixed frequency {$\Lambda$} of a lag-window estimator. Our approach is based on an iterative plug-in scheme. Besides the estimation of a spectral density at a fixed frequency, e.g. at frequency {$\Lambda$}= 0, our procedure allows to perform nonparametric spectral estimation with variable window width which adapts to the smoothness of the true underlying density.},
  langid = {english},
  keywords = {Bandwidth,iterative plug-in,nonparametric spectral estimation,strong-mixing sequence,time series,window width},
  file = {/Users/fpalomba/Zotero/storage/WLJC6K3Q/Bühlmann_1996_Locally Adaptive Lag-Window Spectral Estimation.pdf;/Users/fpalomba/Zotero/storage/WK8QQGLR/j.1467-9892.1996.tb00275.html}
}

@misc{callaway2021DifferenceinDifferencesContinuousTreatmenta,
  title = {Difference-in-{{Differences}} with a {{Continuous Treatment}}},
  author = {Callaway, Brantly and {Goodman-Bacon}, Andrew and Sant'Anna, Pedro H. C.},
  year = {2021},
  month = jul,
  journal = {arXiv.org},
  doi = {10.48550/arXiv.2107.02637},
  abstract = {This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work.},
  howpublished = {https://arxiv.org/abs/2107.02637v2},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CYD5PTSA/Callaway et al_2021_Difference-in-Differences with a Continuous Treatment.pdf}
}

@article{callaway2021DifferenceinDifferencesMultipleTimea,
  title = {Difference-in-{{Differences}} with Multiple Time Periods},
  author = {Callaway, Brantly and Sant'Anna, Pedro H.C.},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {200--230},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.12.001},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/WT64EGPK/Callaway_Sant’Anna_2021_Difference-in-Differences with multiple time periods.pdf}
}

@book{cameron2005MicroeconometricsMethodsApplications,
  title = {Microeconometrics: Methods and Applications},
  shorttitle = {Microeconometrics},
  author = {Cameron, A. Colin and Trivedi, Pravin K.},
  year = {2005},
  publisher = {{Cambridge university press}},
  file = {/Users/fpalomba/Zotero/storage/4GQDWTVQ/Cameron_Trivedi_2005_Microeconometrics.pdf;/Users/fpalomba/Zotero/storage/BMSBCDR6/Cameron and Trivedi (2005) - Microeconometrics Methods and applications.pdf;/Users/fpalomba/Zotero/storage/2JGRGM42/books.html}
}

@article{chetverikov2018EconometricsShapeRestrictions,
  title = {The {{Econometrics}} of {{Shape Restrictions}}},
  author = {Chetverikov, Denis and Santos, Andres and Shaikh, Azeem M.},
  year = {2018},
  journal = {Annual Review of Economics},
  volume = {10},
  number = {1},
  pages = {31--63},
  doi = {10.1146/annurev-economics-080217-053417},
  abstract = {We review recent developments in the econometrics of shape restrictions and their role in applied work. Our objectives are threefold. First, we aim to emphasize the diversity of applications in which shape restrictions have played a fruitful role. Second, we intend to provide practitioners with an intuitive understanding of how shape restrictions impact the distribution of estimators and test statistics. Third, we aim to provide an overview of new advances in the theory of estimation and inference under shape restrictions. Throughout the review, we outline open questions and interesting directions for future research.},
  keywords = {irregular models,shape restrictions,uniformity},
  file = {/Users/fpalomba/Zotero/storage/G69X7GR5/Chetverikov et al. - 2018 - The Econometrics of Shape Restrictions.pdf}
}

@article{debreu1960ReviewRDLuce,
  title = {Review of {{RD Luce}}, {{Individual}} Choice Behavior: {{A}} Theoretical Analysis},
  shorttitle = {Review of {{RD Luce}}, {{Individual}} Choice Behavior},
  author = {Debreu, Gerard},
  year = {1960},
  journal = {American Economic Review},
  volume = {50},
  number = {1},
  pages = {186--188},
  file = {/Users/fpalomba/Zotero/storage/3IUYXA32/openurl.html}
}

@article{dechaisemartin2018FuzzyDifferencesinDifferences,
  title = {Fuzzy {{Differences-in-Differences}}},
  author = {De Chaisemartin, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2018},
  month = apr,
  journal = {The Review of Economic Studies},
  volume = {85},
  number = {2},
  pages = {999--1028},
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdx049},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/ACFGJV2T/de Chaisemartin_D’HaultfŒuille_2018_Fuzzy Differences-in-Differences.pdf}
}

@article{dechaisemartin2020TwoWayFixedEffects,
  title = {Two-{{Way Fixed Effects Estimators}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2020},
  month = sep,
  journal = {American Economic Review},
  volume = {110},
  number = {9},
  pages = {2964--2996},
  issn = {0002-8282},
  doi = {10.1257/aer.20181169},
  abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE ) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator. (JEL C21, C23, D72, J31, J51, L82)},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/RZFV8L2K/de Chaisemartin_D’Haultfœuille_2020_Two-Way Fixed Effects Estimators with Heterogeneous Treatment Effects.pdf}
}

@techreport{dechaisemartin2022DifferenceinDifferencesEstimatorsIntertemporal,
  title = {Difference-in-{{Differences Estimators}} of {{Intertemporal Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = mar,
  number = {w29873},
  pages = {w29873},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29873},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/JHQAVFWS/de Chaisemartin_D'Haultfoeuille_2022_Difference-in-Differences Estimators of Intertemporal Treatment Effects.pdf}
}

@techreport{dechaisemartin2022TwoWayFixedEffects,
  title = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}: {{A Survey}}},
  shorttitle = {Two-{{Way Fixed Effects}} and {{Differences-in-Differences}} with {{Heterogeneous Treatment Effects}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = jan,
  number = {w29691},
  pages = {w29691},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29691},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/JIVFFN55/de Chaisemartin_D'Haultfoeuille_2022_Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous.pdf}
}

@techreport{dechaisemartin2022TwowayFixedEffectsa,
  title = {Two-Way {{Fixed Effects}} and {{Differences-in-Differences Estimators}} with {{Several Treatments}}},
  author = {{de Chaisemartin}, Cl{\'e}ment and D'Haultfoeuille, Xavier},
  year = {2022},
  month = oct,
  number = {w30564},
  pages = {w30564},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w30564},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/8PND2X32/de Chaisemartin_D'Haultfoeuille_2022_Two-way Fixed Effects and Differences-in-Differences Estimators with Several.pdf}
}

@misc{dorn2021SharpSensitivityAnalysis,
  title = {Sharp {{Sensitivity Analysis}} for {{Inverse Propensity Weighting}} via {{Quantile Balancing}}},
  author = {Dorn, Jacob and Guo, Kevin},
  year = {2021},
  month = apr,
  number = {arXiv:2102.04543},
  eprint = {arXiv:2102.04543},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.04543},
  abstract = {Inverse propensity weighting (IPW) is a popular method for estimating treatment effects from observational data. However, its correctness relies on the untestable (and frequently implausible) assumption that all confounders have been measured. This paper introduces a robust sensitivity analysis for IPW that estimates the range of treatment effects compatible with a given amount of unobserved confounding. The estimated range converges to the narrowest possible interval (under the given assumptions) that must contain the true treatment effect. Our proposal is a refinement of the influential sensitivity analysis by Zhao, Small, and Bhattacharya (2019), which we show gives bounds that are too wide even asymptotically. This analysis is based on new partial identification results for Tan (2006)'s marginal sensitivity model.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/AJCTSKDU/Dorn and Guo - 2021 - Sharp Sensitivity Analysis for Inverse Propensity .pdf;/Users/fpalomba/Zotero/storage/ZYNR63DT/2102.html}
}

@book{fan1996LocalPolynomialModelling,
  title = {Local {{Polynomial Modelling}} and {{Its Applications}}: {{Monographs}} on {{Statistics}} and {{Applied Probability}} 66},
  shorttitle = {Local {{Polynomial Modelling}} and {{Its Applications}}},
  author = {Fan, Jianqing},
  year = {1996},
  month = jan,
  publisher = {{Routledge}},
  address = {{New York}},
  doi = {10.1201/9780203748725},
  abstract = {Data-analytic approaches to regression problems, arising from many scientific disciplines are described in this book. The aim of these nonparametric methods is to relax assumptions on the form of a regression function and to let data search for a suitable function that describes the data well. The use of these nonparametric functions with parametric techniques can yield very powerful data analysis tools. Local polynomial modeling and its applications provides an up-to-date picture on state-of-the-art nonparametric regression techniques. The emphasis of the book is on methodologies rather than on theory, with a particular focus on applications of nonparametric techniques to various statistical problems. High-dimensional data-analytic tools are presented, and the book includes a variety of examples. This will be a valuable reference for research and applied statisticians, and will serve as a textbook for graduate students and others interested in nonparametric regression.},
  isbn = {978-0-203-74872-5},
  file = {/Users/fpalomba/Zotero/storage/XIGHBEGQ/Fan_1996_Local Polynomial Modelling and Its Applications.pdf}
}

@article{fan1998AutomaticLocalSmoothing,
  title = {Automatic {{Local Smoothing}} for {{Spectral Density Estimation}}},
  author = {Fan, Jianqing and Kreutzberger, Eva},
  year = {1998},
  journal = {Scandinavian Journal of Statistics},
  volume = {25},
  number = {2},
  pages = {359--369},
  issn = {1467-9469},
  doi = {10.1111/1467-9469.00109},
  abstract = {This article uses local polynomial techniques to fit Whittle's likelihood for spectral density estimation. Asymptotic sampling properties of the proposed estimators are derived, and adaptation of the proposed estimator to the boundary effect is demonstrated. We show that the Whittle likelihood-based estimator has advantages over the least-squares based log-periodogram. The bandwidth for the Whittle likelihood-based method is chosen by a simple adjustment of a bandwidth selector proposed in Fan \& Gijbels (1995). The effectiveness of the proposed procedure is demonstrated by a few simulated and real numerical examples. Our simulation results support the asymptotic theory that the likelihood based spectral density and log-spectral density estimators are the most appealing among their peers},
  langid = {english},
  keywords = {bandwidth selection,local polynomial fit,periodogram,spectral density estimation,Whittle likelihood},
  file = {/Users/fpalomba/Zotero/storage/ZVQE5FBW/Fan_Kreutzberger_1998_Automatic Local Smoothing for Spectral Density Estimation.pdf;/Users/fpalomba/Zotero/storage/HDLSUMVT/1467-9469.html}
}

@misc{fang2021InferenceLargeScaleLinear,
  title = {Inference for {{Large-Scale Linear Systems}} with {{Known Coefficients}}},
  author = {Fang, Zheng and Santos, Andres and Shaikh, Azeem M. and Torgovitsky, Alexander},
  year = {2021},
  month = sep,
  number = {arXiv:2009.08568},
  eprint = {arXiv:2009.08568},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.08568},
  abstract = {This paper considers the problem of testing whether there exists a non-negative solution to a possibly under-determined system of linear equations with known coefficients. This hypothesis testing problem arises naturally in a number of settings, including random coefficient, treatment effect, and discrete choice models, as well as a class of linear programming problems. As a first contribution, we obtain a novel geometric characterization of the null hypothesis in terms of identified parameters satisfying an infinite set of inequality restrictions. Using this characterization, we devise a test that requires solving only linear programs for its implementation, and thus remains computationally feasible in the high-dimensional applications that motivate our analysis. The asymptotic size of the proposed test is shown to equal at most the nominal level uniformly over a large class of distributions that permits the number of linear equations to grow with the sample size.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics},
  file = {/Users/fpalomba/Zotero/storage/JZF9S2CN/Fang et al. - 2021 - Inference for Large-Scale Linear Systems with Know.pdf;/Users/fpalomba/Zotero/storage/N2NNBIB3/2009.html}
}

@article{fang2021ProjectionFrameworkTesting,
  title = {A {{Projection Framework}} for {{Testing Shape Restrictions That Form Convex Cones}}},
  author = {Fang, Zheng and Seo, Juwon},
  year = {2021},
  journal = {Econometrica},
  volume = {89},
  number = {5},
  pages = {2439--2458},
  issn = {1468-0262},
  doi = {10.3982/ECTA17764},
  abstract = {This paper develops a uniformly valid and asymptotically nonconservative test based on projection for a class of shape restrictions. The key insight we exploit is that these restrictions form convex cones, a simple and yet elegant structure that has been barely harnessed in the literature. Based on a monotonicity property afforded by such a geometric structure, we construct a bootstrap procedure that, unlike many studies in nonstandard settings, dispenses with estimation of local parameter spaces, and the critical values are obtained in a way as simple as computing the test statistic. Moreover, by appealing to strong approximations, our framework accommodates nonparametric regression models as well as distributional/density-related and structural settings. Since the test entails a tuning parameter (due to the nonstandard nature of the problem), we propose a data-driven choice and prove its validity. Monte Carlo simulations confirm that our test works well.},
  langid = {english},
  keywords = {convex cone,Nonstandard inference,projection,shape restrictions,strong approximations},
  file = {/Users/fpalomba/Zotero/storage/BYXI6UVI/Fang and Seo - 2021 - A Projection Framework for Testing Shape Restricti.pdf;/Users/fpalomba/Zotero/storage/R3IAC2NQ/ECTA17764.html}
}

@techreport{fortin1999OptimalBandwidthSelection,
  type = {Working {{Paper}}},
  title = {Optimal Bandwidth Selection in Non-Parametric Spectral Density Estimation: {{Review}} and Simulation},
  shorttitle = {Optimal Bandwidth Selection in Non-Parametric Spectral Density Estimation},
  author = {Fortin, Ines and Kuzmics, Christoph},
  year = {1999},
  number = {62},
  institution = {{Reihe \"Okonomie / Economics Series}},
  abstract = {This paper deals with optimal window width choice in non-parametric lag- or spectral window estimation of the spectral density of a stationary zero-mean process. Several approaches are reviewed: the cross-validation based methods described by Hurvich (1985), Beltrao \& Bloomfield (1987) and Hurvich \& Beltrao (1990), an iterative procedure due to Buehlmann (1996), and a bootstrap approach followed by Franke \& Haerdle (1992). These methods are compared in terms of the mean square error, the mean square percentage error, and a third measure of distance between the true spectral density and its estimate. The comparison is based on a small simulation study. The processes that are simulated are in the class of ARMA (5,5) processes. Based on the simulation evidence, we suggest to use a slightly modified version of Buehlmann's (1996) iterative method.},
  copyright = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/7GL9RYXM/Fortin_Kuzmics_1999_Optimal bandwidth selection in non-parametric spectral density estimation.pdf}
}

@techreport{freyaldenhoven2021VisualizationIdentificationEstimation,
  title = {Visualization, {{Identification}}, and {{Estimation}} in the {{Linear Panel Event-Study Design}}},
  author = {Freyaldenhoven, Simon and Hansen, Christian and P{\'e}rez, Jorge P{\'e}rez and Shapiro, Jesse},
  year = {2021},
  month = aug,
  number = {w29170},
  pages = {w29170},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w29170},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/6QQ67UIB/Freyaldenhoven et al_2021_Visualization, Identification, and Estimation in the Linear Panel Event-Study.pdf}
}

@article{gafarov2018DeltamethodInferenceClass,
  title = {Delta-Method Inference for a Class of Set-Identified {{SVARs}}},
  author = {Gafarov, Bulat and Meier, Matthias and Montiel Olea, Jos{\'e} Luis},
  year = {2018},
  month = apr,
  journal = {Journal of Econometrics},
  volume = {203},
  number = {2},
  pages = {316--327},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2017.12.004},
  abstract = {We study vector autoregressions that impose equality and/or inequality restrictions to set-identify the dynamic responses to a single structural shock. We make three contributions. First, we present an algorithm to compute the largest and smallest value that an impulse-response coefficient can attain over its identified set. Second, we provide conditions under which these largest and smallest values are directionally differentiable functions of the model's reduced-form parameters. Third, we propose a delta-method approach to conduct inference about the structural impulse-response coefficients. We use our results to assess the effects of the announcement of the Quantitative Easing program in August 2010.},
  langid = {english},
  keywords = {Directional differentiability,Set-identification,Sign restrictions,SVAR,Unconventional monetary policy},
  file = {/Users/fpalomba/Zotero/storage/BRLUPNL7/Gafarov et al. - 2018 - Delta-method inference for a class of set-identifi.pdf;/Users/fpalomba/Zotero/storage/TKRATYQL/S0304407617302440.html}
}

@article{giacomini2021RobustBayesianInference,
  title = {Robust {{Bayesian Inference}} for {{Set-Identified Models}}},
  author = {Giacomini, Raffaella and Kitagawa, Toru},
  year = {2021},
  journal = {Econometrica},
  volume = {89},
  number = {4},
  pages = {1519--1556},
  issn = {1468-0262},
  doi = {10.3982/ECTA16773},
  abstract = {This paper reconciles the asymptotic disagreement between Bayesian and frequentist inference in set-identified models by adopting a multiple-prior (robust) Bayesian approach. We propose new tools for Bayesian inference in set-identified models and show that they have a well-defined posterior interpretation in finite samples and are asymptotically valid from the frequentist perspective. The main idea is to construct a prior class that removes the source of the disagreement: the need to specify an unrevisable prior for the structural parameter given the reduced-form parameter. The corresponding class of posteriors can be summarized by reporting the `posterior lower and upper probabilities' of a given event and/or the `set of posterior means' and the associated `robust credible region'. We show that the set of posterior means is a consistent estimator of the true identified set and the robust credible region has the correct frequentist asymptotic coverage for the true identified set if it is convex. Otherwise, the method provides posterior inference about the convex hull of the identified set. For impulse-response analysis in set-identified Structural Vector Autoregressions, the new tools can be used to overcome or quantify the sensitivity of standard Bayesian inference to the choice of an unrevisable prior.},
  langid = {english},
  keywords = {asymptotic coverage,consistency,credible region,identified set,identifying restrictions,impulse-response analysis,Multiple priors},
  file = {/Users/fpalomba/Zotero/storage/A9WEF7I5/Giacomini and Kitagawa - 2021 - Robust Bayesian Inference for Set-Identified Model.pdf;/Users/fpalomba/Zotero/storage/6DFEJWZ9/ECTA16773.html}
}

@techreport{goldsmith-pinkham2022ContaminationBiasLinear,
  title = {Contamination {{Bias}} in {{Linear Regressions}}},
  author = {{Goldsmith-Pinkham}, Paul and Hull, Peter and Koles{\'a}r, Michal},
  year = {2022},
  month = jun,
  number = {w30108},
  pages = {w30108},
  address = {{Cambridge, MA}},
  institution = {{National Bureau of Economic Research}},
  doi = {10.3386/w30108},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/CRP7UTST/Goldsmith-Pinkham et al_2022_Contamination Bias in Linear Regressions.pdf}
}

@article{goodman-bacon2019YouVeBeena,
  title = {So You've Been Told to Do My Difference-in-Differences Thing: {{A}} Guide},
  shorttitle = {So You've Been Told to Do My Difference-in-Differences Thing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2019},
  journal = {Vanderbilt University},
  file = {/Users/fpalomba/Zotero/storage/DQBNJA42/Goodman-Bacon_2019_So you’ve been told to do my difference-in-differences thing.pdf}
}

@article{goodman-bacon2021DifferenceindifferencesVariationTreatment,
  title = {Difference-in-Differences with Variation in Treatment Timing},
  author = {{Goodman-Bacon}, Andrew},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  series = {Themed {{Issue}}: {{Treatment Effect}} 1},
  volume = {225},
  number = {2},
  pages = {254--277},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2021.03.014},
  abstract = {The canonical difference-in-differences (DD) estimator contains two time periods, ''pre'' and ''post'', and two groups, ''treatment'' and ''control''. Most DD applications, however, exploit variation across groups of units that receive treatment at different times. This paper shows that the two-way fixed effects estimator equals a weighted average of all possible two-group/two-period DD estimators in the data. A causal interpretation of two-way fixed effects DD estimates requires both a parallel trends assumption and treatment effects that are constant over time. I show how to decompose the difference between two specifications, and provide a new analysis of models that include time-varying controls.},
  langid = {english},
  keywords = {Difference-in-differences,Treatment effect heterogeneity,Two-way fixed effects,Variation in treatment timing},
  file = {/Users/fpalomba/Zotero/storage/9YL2PX7F/Goodman-Bacon_2021_Difference-in-differences with variation in treatment timing.pdf;/Users/fpalomba/Zotero/storage/NSXD4QGV/S0304407621001445.html}
}

@inproceedings{gumbel1935ValeursExtremesDistributions,
  title = {Les Valeurs Extr\^emes Des Distributions Statistiques},
  booktitle = {Annales de l'institut {{Henri Poincar\'e}}},
  author = {Gumbel, Emil Julius},
  year = {1935},
  volume = {5},
  pages = {115--158},
  file = {/Users/fpalomba/Zotero/storage/TZ8QDAWB/Gumbel_1935_Les valeurs extrêmes des distributions statistiques.pdf}
}

@article{hannig2004KernelSmoothingPeriodogramsa,
  title = {Kernel Smoothing of Periodograms under {{Kullback}}\textendash{{Leibler}} Discrepancy},
  author = {Hannig, Jan and Lee, Thomas C. M.},
  year = {2004},
  month = jul,
  journal = {Signal Processing},
  volume = {84},
  number = {7},
  pages = {1255--1266},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2004.04.007},
  abstract = {Kernel smoothing on the periodogram is a popular nonparametric method for spectral density estimation. Most important in the implementation of this method is the choice of the bandwidth, or span, for smoothing. One idealized way of choosing the bandwidth is to choose it as the one that minimizes the Kullback\textendash Leibler (KL) discrepancy between the smoothed estimate and the true spectrum. However, this method fails in practice, as the KL discrepancy is an unknown quantity. This paper introduces an estimator for this discrepancy, so that the bandwidth that minimizes the unknown discrepancy can be empirically approximated via the minimization of it. It is shown that this discrepancy estimator is consistent. Numerical results also suggest that this empirical choice of bandwidth often outperforms some other commonly used bandwidth choices. The same idea is also applied to choose the bandwidth for log-periodogram smoothing.},
  langid = {english},
  keywords = {Bandwidth selection,Kullback–Leibler discrepancy,Periodogram and log-periodogram smoothing,Relative entropy,Spectral density estimation},
  file = {/Users/fpalomba/Zotero/storage/RBNMQ7UH/Hannig_Lee_2004_Kernel smoothing of periodograms under Kullback–Leibler discrepancy.pdf;/Users/fpalomba/Zotero/storage/UF8UEMG7/S0165168404000702.html}
}

@misc{huang2022VariancebasedSensitivityAnalysis,
  title = {Variance-Based Sensitivity Analysis for Weighting Estimators Result in More Informative Bounds},
  author = {Huang, Melody and Pimentel, Samuel D.},
  year = {2022},
  month = aug,
  number = {arXiv:2208.01691},
  eprint = {arXiv:2208.01691},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2208.01691},
  abstract = {Weighting methods are popular tools for estimating causal effects; assessing their robustness under unobserved confounding is important in practice. In the following paper, we introduce a new set of sensitivity models called "variance-based sensitivity models". Variance-based sensitivity models characterize the bias from omitting a confounder by bounding the distributional differences that arise in the weights from omitting a confounder, with several notable innovations over existing approaches. First, the variance-based sensitivity models can be parameterized with respect to a simple \$R\^2\$ parameter that is both standardized and bounded. We introduce a formal benchmarking procedure that allows researchers to use observed covariates to reason about plausible parameter values in an interpretable and transparent way. Second, we show that researchers can estimate valid confidence intervals under a set of variance-based sensitivity models, and provide extensions for researchers to incorporate their substantive knowledge about the confounder to help tighten the intervals. Last, we highlight the connection between our proposed approach and existing sensitivity analyses, and demonstrate both, empirically and theoretically, that variance-based sensitivity models can provide improvements on both the stability and tightness of the estimated confidence intervals over existing methods. We illustrate our proposed approach on a study examining blood mercury levels using the National Health and Nutrition Examination Survey (NHANES).},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/A7Q55I3D/Huang and Pimentel - 2022 - Variance-based sensitivity analysis for weighting .pdf;/Users/fpalomba/Zotero/storage/X7AAEHMQ/2208.html}
}

@article{hurvich1985DataDrivenChoiceSpectrum,
  title = {Data-{{Driven Choice}} of a {{Spectrum Estimate}}: {{Extending}} the {{Applicability}} of {{Cross-Validation Methods}}},
  shorttitle = {Data-{{Driven Choice}} of a {{Spectrum Estimate}}},
  author = {Hurvich, Clifford M.},
  year = {1985},
  month = dec,
  journal = {Journal of the American Statistical Association},
  volume = {80},
  number = {392},
  pages = {933--940},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.1985.10478207},
  abstract = {I develop methods of objectively choosing a spectrum estimate from a general class C of available estimates. C can, for example, simultaneously include Blackman\textemdash Tukey and autoregressive estimates, so the statistician no longer needs to choose one type or the other arbitrarily. The methods work by extending the applicability of existing cross-validatory techniques through the introduction of generalized leave-out-one spectrum estimates. As special cases, I obtain new objective smoothness parameter selection methods for both autoregressive and Blackman\textemdash Tukey estimates. In a Monte Carlo study, I demonstrate the effectiveness of the particular methods that result from generalizing Wahba's CVMSE.},
  keywords = {Autoregressive order selection,Autoregressive spectrum estimation,Bandwidth determination,Blackman—Tukey spectrum estimation,Kernel spectrum estimation},
  file = {/Users/fpalomba/Zotero/storage/3L4PITD3/Hurvich_1985_Data-Driven Choice of a Spectrum Estimate.pdf}
}

@misc{kuosmanen2021DesignFlawSynthetic,
  type = {{{MPRA Paper}}},
  title = {Design {{Flaw}} of the {{Synthetic Control Method}}},
  author = {Kuosmanen, Timo and Zhou, Xun and Eskelinen, Juha and Malo, Pekka},
  year = {2021},
  month = feb,
  abstract = {Synthetic control method (SCM) identifies causal treatment effects by constructing a counterfactual treatment unit as a convex combination of donors in the control group, such that the weights of donors and predictors are jointly optimized during the pre-treatment period. This paper demonstrates that the true optimal solution to the SCM problem is typically a corner solution where all weight is assigned to a single predictor, contradicting the intended purpose of predictors. To address this inherent design flaw, we propose to determine the predictor weights and donor weights separately. We show how the donor weights can be optimized when the predictor weights are given, and consider alternative data-driven approaches to determine the predictor weights. Re-examination of the two original empirical applications to Basque terrorism and California's tobacco control program demonstrates the complete and utter failure of the existing SCM algorithms and illustrates our proposed remedies.},
  howpublished = {https://mpra.ub.uni-muenchen.de/106390/},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/FKSMMJA9/Kuosmanen et al_2021_Design Flaw of the Synthetic Control Method.pdf;/Users/fpalomba/Zotero/storage/5QTCGN6K/106390.html}
}

@article{lee1997SimpleSpanSelector,
  title = {A {{Simple Span Selector}} for {{Periodogram Smoothing}}},
  author = {Lee, Thomas C. M.},
  year = {1997},
  journal = {Biometrika},
  volume = {84},
  number = {4},
  eprint = {2337667},
  eprinttype = {jstor},
  pages = {965--969},
  publisher = {{[Oxford University Press, Biometrika Trust]}},
  issn = {0006-3444},
  doi = {10.1093/biomet/84.4.965},
  abstract = {One approach to estimating the spectral density of a stationary time series is to smooth the periodogram and one important component of this approach is the choice of the span for smoothing. This note proposes a new span selector which is based on unbiased risk estimation. The proposed span selector is simple, and does not impose strong conditions on the unknown spectrum. For example, it does not require the unknown spectrum to possess a second derivative, which is a typical requirement of most plug-in type or spline-based methods. The finite sample performance of the proposed span selector is illustrated via a small simulation.},
  file = {/Users/fpalomba/Zotero/storage/ATU5T227/Lee_1997_A Simple Span Selector for Periodogram Smoothing.pdf}
}

@article{lee2001StabilizedBandwidthSelection,
  title = {A Stabilized Bandwidth Selection Method for Kernel Smoothing of the Periodogram},
  author = {Lee, Thomas C. M.},
  year = {2001},
  month = feb,
  journal = {Signal Processing},
  volume = {81},
  number = {2},
  pages = {419--430},
  issn = {0165-1684},
  doi = {10.1016/S0165-1684(00)00218-8},
  abstract = {One popular method for nonparametric spectral density estimation is to perform kernel smoothing on the periodogram, and one important component of this method is the choice of the bandwidth (or span) for smoothing. This paper proposes a new bandwidth selection method that is based on a coupling of the so-called plug-in and the unbiased risk estimation ideas. This new method is easy to describe, simple to implement, and does not impose severe conditions on the unknown spectrum. Numerical results suggest that this new method often outperforms some other commonly used bandwidth selection methods. The new methodology is also applied to choose the bandwidth for log-periodogram smoothing.},
  langid = {english},
  keywords = {Bandwidth selection,Log-periodogram and periodogram smoothing,Plug in,Spectral density estimation,Unbiased risk estimation},
  file = {/Users/fpalomba/Zotero/storage/YNQ2F74I/Lee_2001_A stabilized bandwidth selection method for kernel smoothing of the periodogram.pdf;/Users/fpalomba/Zotero/storage/632HI844/S0165168400002188.html}
}

@inproceedings{lee2009NonparametricSpectralDensity,
  title = {Nonparametric Spectral Density Estimation with Missing Observations},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Lee, Thomas C. M. and Zhu, Zhengyuan},
  year = {2009},
  month = apr,
  pages = {3041--3044},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2009.4960265},
  abstract = {Self-consistency is a fundamental principle in statistics for retaining maximum amount of information in the data. In this paper this principle is applied to develop a new method for nonparametric spectrum estimation with missing data. One major advantage of the proposed method is that it can be coupled with any complete data nonparametric spectrum estimation procedure, including kernel smoothing, wavelet and spline estimators. The practical performance of the method is illustrated by a simulation study.},
  keywords = {Astrophysics,Discrete Fourier transforms,Frequency,Geology,Kernel,missing data,nonparametric spectrum estimation,periodogram smoothing,self-consistency,Smoothing methods,Spectral analysis,Spline,Statistics,Time domain analysis},
  file = {/Users/fpalomba/Zotero/storage/XULQHQI7/Lee_Zhu_2009_Nonparametric spectral density estimation with missing observations.pdf;/Users/fpalomba/Zotero/storage/SKZ7ERLX/4960265.html}
}

@book{luce1959IndividualChoiceBehavior,
  title = {Individual {{Choice Behavior}}: {{A Theoretical Analysis}}},
  shorttitle = {Individual {{Choice Behavior}}},
  author = {Luce, R. Duncan},
  year = {1959},
  publisher = {{Courier Corporation}},
  abstract = {This influential treatise presents upper-level undergraduates and graduate students with a mathematical analysis of choice behavior. It begins with the statement of a general axiom upon which the rest of the book rests; the following three chapters, which may be read independently of each other, are devoted to applications of the theory to substantive problems: psychophysics, utility, and learning.Applications to psychophysics include considerations of time- and space-order effects, the Fechnerian assumption, the power law and its relation to discrimination data, interaction of continua, discriminal processes, signal detectability theory, and ranking of stimuli. The next major theme, utility theory, features unusual results that suggest an experiment to test the theory. The final chapters explore learning-related topics, analyzing the stochastic theories of learning as the basic approach\textemdash with the exception that distributions of response strengths are assumed to be transformed rather than response probabilities. The author arrives at three classes of learning operators, both linear and nonlinear, and the text concludes with a useful series of appendixes.},
  googlebooks = {ERQsKkPiKkkC},
  isbn = {978-0-486-15339-1},
  langid = {english},
  keywords = {Mathematics / Probability \& Statistics / General}
}

@article{mackinnon2022ClusterrobustInferenceGuide,
  title = {Cluster-Robust Inference: {{A}} Guide to Empirical Practice},
  shorttitle = {Cluster-Robust Inference},
  author = {MacKinnon, James G. and Nielsen, Morten {\O}rregaard and Webb, Matthew D.},
  year = {2022},
  month = may,
  journal = {Journal of Econometrics},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2022.04.001},
  abstract = {Methods for cluster-robust inference are routinely used in economics and many other disciplines. However, it is only recently that theoretical foundations for the use of these methods in many empirically relevant situations have been developed. In this paper, we use these theoretical results to provide a guide to empirical practice. We do not attempt to present a comprehensive survey of the (very large) literature. Instead, we bridge theory and practice by providing a thorough guide on what to do and why, based on recently available econometric theory and simulation evidence. To practice what we preach, we include an empirical analysis of the effects of the minimum wage on labor supply of teenagers using individual data.},
  langid = {english},
  keywords = {Cluster jackknife,Cluster-robust variance estimator (CRVE),Clustered data,Robust inference,Wild cluster bootstrap},
  file = {/Users/fpalomba/Zotero/storage/63Q4FJIC/MacKinnon et al_2022_Cluster-robust inference.pdf;/Users/fpalomba/Zotero/storage/EW7CHKUG/S0304407622000781.html}
}

@book{maddala1983LimitedDependentQualitativeVariables,
  title = {Limited-{{Dependent}} and {{Qualitative Variables}} in {{Econometrics}}},
  author = {Maddala, G. S.},
  year = {1983},
  series = {Econometric {{Society Monographs}}},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511810176},
  abstract = {This book presents the econometric analysis of single-equation and simultaneous-equation models in which the jointly dependent variables can be continuous, categorical, or truncated. Despite the traditional emphasis on continuous variables in econometrics, many of the economic variables encountered in practice are categorical (those for which a suitable category can be found but where no actual measurement exists) or truncated (those that can be observed only in certain ranges). Such variables are involved, for example, in models of occupational choice, choice of tenure in housing, and choice of type of schooling. Models with regulated prices and rationing, and models for program evaluation, also represent areas of application for the techniques presented by the author.},
  isbn = {978-0-521-33825-7},
  file = {/Users/fpalomba/Zotero/storage/VDRFUI9W/Maddala_1983_Limited-Dependent and Qualitative Variables in Econometrics.pdf;/Users/fpalomba/Zotero/storage/XWXAMAXV/69B8DBC75160713AA3AD1AD979D297B8.html}
}

@misc{malo2020ComputingSyntheticControls,
  type = {{{MPRA Paper}}},
  title = {Computing {{Synthetic Controls Using Bilevel Optimization}}},
  author = {Malo, Pekka and Eskelinen, Juha and Zhou, Xun and Kuosmanen, Timo},
  year = {2020},
  month = nov,
  abstract = {The synthetic control method (SCM) is a major innovation in the estimation of causal effects of policy interventions and programs in a comparative case study setting. In this paper, we demonstrate that the data-driven approach to SCM requires solving a bilevel optimization problem. We show how the SCM problem can be solved using iterative algorithms based on Tykhonov descent or KKT approximations.},
  howpublished = {https://mpra.ub.uni-muenchen.de/104085/},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/LSZUGYPC/Malo et al_2020_Computing Synthetic Controls Using Bilevel Optimization.pdf;/Users/fpalomba/Zotero/storage/XE763HWE/104085.html}
}

@incollection{mcfadden1974FrontiersEconometrics,
  title = {Frontiers in Econometrics},
  booktitle = {Conditional Logit Analysis of Qualitative Choice Behavior},
  author = {McFadden, Daniel},
  publisher = {Academic Press},
  year = {1974},
  pages = {105--142},
  isbn = {0-12-776150-0},
  file = {/Users/fpalomba/Zotero/storage/23T7KBQ7/McFadden_1974_Conditional logit analysis of qualitative choice behavior.pdf;/Users/fpalomba/Zotero/storage/5T389T9K/10002395479.html}
}

@article{mcfadden2000MixedMNLModels,
  title = {Mixed {{MNL}} Models for Discrete Response},
  author = {McFadden, Daniel and Train, Kenneth},
  year = {2000},
  journal = {Journal of applied Econometrics},
  volume = {15},
  number = {5},
  pages = {447--470},
  publisher = {{Wiley Online Library}},
  file = {/Users/fpalomba/Zotero/storage/R2PWYJD4/10)155447AID-JAE5703.0.html}
}

@article{mombeni2017LinexDiscrepancyBandwidth,
  title = {Linex Discrepancy for Bandwidth Selection},
  author = {Mombeni, H. and Rezaei, S. and Nadarajah, S.},
  year = {2017},
  month = aug,
  journal = {Communications in Statistics - Simulation and Computation},
  volume = {46},
  number = {7},
  pages = {5054--5069},
  publisher = {{Taylor \& Francis}},
  issn = {0361-0918},
  doi = {10.1080/03610918.2016.1143102},
  abstract = {A bandwidth selection based on Linex discrepancy is proposed for kernel smoothing of periodogram. The selection minimizes Linex discrepancy between the smoothed and true spectrums. Two estimators are introduced for Linex discrepancy. The bandwidth choice outperforms some common bandwidth choices.},
  keywords = {62E99,Bandwidth selection,Linex discrepancy,Periodogram smoothing,Spectral density estimation},
  file = {/Users/fpalomba/Zotero/storage/X4E8UIM4/Mombeni et al_2017_Linex discrepancy for bandwidth selection.pdf}
}

@article{parzen1957ConsistentEstimatesSpectrum,
  title = {On {{Consistent Estimates}} of the {{Spectrum}} of a {{Stationary Time Series}}},
  author = {Parzen, Emanuel},
  year = {1957},
  journal = {The Annals of Mathematical Statistics},
  volume = {28},
  number = {2},
  eprint = {2237156},
  eprinttype = {jstor},
  pages = {329--348},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851},
  abstract = {This paper is concerned with the spectral analysis of wide sense stationary time series which possess a spectral density function and whose fourth moment functions satisfy an integrability condition (which includes Gaussian processes). Consistent estimates are obtained for the spectral density function as well as for the spectral distribution function and a general class of spectral averages. Optimum consistent estimates are chosen on the basis of criteria involving the notions of order of consistency and asymptotic variance. The problem of interpolating the estimated spectral density, so that only a finite number of quantities need be computed to determine the entire graph, is also discussed. Both continuous and discrete time series are treated.},
  file = {/Users/fpalomba/Zotero/storage/GFVA2CL5/Parzen_1957_On Consistent Estimates of the Spectrum of a Stationary Time Series.pdf}
}


@book{priestley1981SpectralAnalysisTime,
  title = {Spectral {{Analysis}} and {{Time Series}}},
  author = {Priestley, M. B.},
  year = {1981},
  publisher = {{Academic Press}},
  googlebooks = {RVTYvwEACAAJ},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/DHXM8A32/priestley (1981) - spectral analysis and time series. volumes 1,2.pdf;/Users/fpalomba/Zotero/storage/EH3TWBA8/Priestley_1981_Spectral Analysis and Time Series.pdf}
}

@article{robins1994EstimationRegressionCoefficients,
  title = {Estimation of {{Regression Coefficients When Some Regressors Are Not Always Observed}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {427},
  eprint = {2290910},
  eprinttype = {jstor},
  pages = {846--866},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290910},
  abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector {$\alpha$}\textsubscript{0} of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of {$\alpha$}\textsubscript{0} is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations.},
  file = {/Users/fpalomba/Zotero/storage/HSPCW7FL/Robins et al_1994_Estimation of Regression Coefficients When Some Regressors Are Not Always.pdf}
}

@article{robins1995SemiparametricEfficiencyMultivariate,
  title = {Semiparametric {{Efficiency}} in {{Multivariate Regression Models}} with {{Missing Data}}},
  author = {Robins, James M. and Rotnitzky, Andrea},
  year = {1995},
  journal = {Journal of the American Statistical Association},
  volume = {90},
  number = {429},
  eprint = {2291135},
  eprinttype = {jstor},
  pages = {122--129},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2291135},
  abstract = {We consider the efficiency bound for the estimation of the parameters of semiparametric models defined solely by restrictions on the means of a vector of correlated outcomes, Y, when the data on Y are missing at random. We show that the semiparametric variance bound is the asymptotic variance of the optimal estimator in a class of inverse probability of censoring weighted estimators and that this bound is unchanged if the data are missing completely at random. For this case we study the asymptotic performance of the generalized estimating equations (GEE) estimators of mean parameters and show that the optimal GEE estimator is inefficient except for special cases. The optimal weighted estimator depends on unknown population quantities. But for monotone missing data, we propose an adaptive estimator whose asymptotic variance can achieve the bound.},
  file = {/Users/fpalomba/Zotero/storage/Z9AZAA7X/Robins and Rotnitzky - 1995 - Semiparametric Efficiency in Multivariate Regressi.pdf}
}

@misc{roth2023WhatTrendingDifferenceinDifferences,
  title = {What's {{Trending}} in {{Difference-in-Differences}}? {{A Synthesis}} of the {{Recent Econometrics Literature}}},
  shorttitle = {What's {{Trending}} in {{Difference-in-Differences}}?},
  author = {Roth, Jonathan and Sant'Anna, Pedro H. C. and Bilinski, Alyssa and Poe, John},
  year = {2023},
  month = jan,
  number = {arXiv:2201.01194},
  eprint = {arXiv:2201.01194},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.01194},
  abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of ``canonical'' assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on \$(i)\$ multiple periods and variation in treatment timing, \$(ii)\$ potential violations of parallel trends, or \$(iii)\$ alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
  archiveprefix = {arxiv},
  keywords = {Economics - Econometrics,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/IQ4VH3BZ/Roth et al_2023_What's Trending in Difference-in-Differences.pdf;/Users/fpalomba/Zotero/storage/KD42QJTD/2201.html}
}

@article{stoica1999OptimallySmoothedPeriodogram,
  title = {Optimally Smoothed Periodogram},
  author = {Stoica, Petre and Sundin, Tomas},
  year = {1999},
  month = nov,
  journal = {Signal Processing},
  volume = {78},
  number = {3},
  pages = {253--264},
  issn = {0165-1684},
  doi = {10.1016/S0165-1684(99)00066-3},
  abstract = {Locally smoothing the periodogram yields one of the most successful methods for non-parametric spectral estimation. Nevertheless, no much guideline was available for choosing the window parameters and span for smoothing. Here we derive the window parameters and span that minimize an asymptotically unbiased estimate of the spectral estimation error variance. The so-obtained optimally smoothed periodogram is shown to outperform the commonly used uniformly smoothed periodogram. Furthermore, a technique is presented where the window parameters are chosen separately for each of a number of different frequency bands. This technique reduces the variance of the spectral estimates even further, at the price of an increased computational complexity. Zusammenfassung Die lokale Gl\"attung des Periodogramms ergibt eine der erfolgreichsten Methoden zur nichtparametrischen Spektralsch\"atzung. Allerdings existieren kaum Richtlinien f\"ur die Wahl der bei der Gl\"attung verwendeten Fensterparameter und -spannweite. In dieser Arbeit leiten wir jene Fensterparameter und -spannweite ab, welche einen asymptotisch erwartungstreuen Sch\"atzwert der Varianz des Spektralsch\"atzfehlers minimieren. Es wird gezeigt, da\ss{} das derart erhaltene optimal gegl\"attete Periodogramm das \"ublicherweise verwendete gleichf\"ormig gegl\"attete Periodogramm an Leistungsf\"ahigkeit \"ubertrifft. Weiters wird eine Methode pr\"asentiert, bei der die Fensterparameter separat f\"ur jedes Frequenzband innerhalb einer bestimmten Gruppe von verschiedenen Frequenzb\"andern gew\"ahlt werden. Diese Methode verringert die Varianz des Spektralsch\"atzers sogar noch weiter, was mit einem h\"oheren Rechenaufwand erkauft wird. R\'esum\'e Le lissage local du p\'eriodogramme constitue l'une des m\'ethodes les plus fructueuses pour l'estimation spectrale non param\'etrique. Toutefois aucune indication n'existait jusqu'\`a pr\'esent pour le choix des param\`etres de la fen\^etre et de l'\'etendue du lissage. Nous d\'erivons ici les param\`etres de la fen\^etre et l'\'etendue qui minimisent une estim\'ee asymptotiquement non biais\'ee de la variance de l'erreur d'estimation spectrale. Le p\'eriodogramme optimalement liss\'e obtenu ainsi est montr\'e surpasser le p\'eriodogramme liss\'e commun\'ement utilis\'e. De plus une technique dans laquelle les param\`etres de la fen\^etre sont choisis s\'epar\'ement pour une certain nombre de bandes de fr\'equences diff\'erentes est pr\'esent\'ee. Cette technique r\'eduit encore davantage la variance des estim\'ees spectrales, au prix d'une complexit\'e de calcul accrue.},
  langid = {english},
  keywords = {Non-parametric methods,Optimal spectral window,Smoothed periodogram,Spectral analysis},
  file = {/Users/fpalomba/Zotero/storage/TWZ5YR65/Stoica_Sundin_1999_Optimally smoothed periodogram.pdf;/Users/fpalomba/Zotero/storage/7CVZ85LN/S0165168499000663.html}
}

@article{sun2021EstimatingDynamicTreatmenta,
  title = {Estimating Dynamic Treatment Effects in Event Studies with Heterogeneous Treatment Effects},
  author = {Sun, Liyang and Abraham, Sarah},
  year = {2021},
  month = dec,
  journal = {Journal of Econometrics},
  volume = {225},
  number = {2},
  pages = {175--199},
  issn = {03044076},
  doi = {10.1016/j.jeconom.2020.09.006},
  langid = {english},
  file = {/Users/fpalomba/Zotero/storage/7WH3JBIL/Sun_Abraham_2021_Estimating dynamic treatment effects in event studies with heterogeneous.pdf}
}

@article{thurstone1927LawComparativeJudgment,
  title = {A Law of Comparative Judgment.},
  author = {Thurstone, Louis L.},
  year = {1927},
  journal = {Psychological review},
  volume = {34},
  number = {4},
  pages = {273},
  publisher = {{Psychological Review Company}},
  file = {/Users/fpalomba/Zotero/storage/NRBERHAX/Thurstone_1927_A law of comparative judgment.pdf;/Users/fpalomba/Zotero/storage/FSH8INHL/1928-00527-001.html}
}

@article{wahba1980AutomaticSmoothingLoga,
  title = {Automatic {{Smoothing}} of the {{Log Periodogram}}},
  author = {Wahba, Grace},
  year = {1980},
  journal = {Journal of the American Statistical Association},
  volume = {75},
  number = {369},
  eprint = {2287399},
  eprinttype = {jstor},
  pages = {122--132},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2287399},
  abstract = {We develop an objective optimum smoothing procedure for an estimate of the log spectral density, based on smoothing the log periodogram with a smoothing spline. We call the result the optimally smoothed spline (OSS) estimate. We show that an unbiased estimate {\^R}({$\lambda$}) of the expected integrated mean square error can be obtained as a function of the smoothing, or "bandwidth" parameter {$\lambda$}. The smoothing parameter then is chosen as that {$\lambda$} that minimizes {\^R}({$\lambda$}). The degree of the smoothing spline (equivalently, the "shape" parameter of the window) also can be chosen this way. Results of some Monte Carlo experiments demonstrating the effectiveness of the method on selected examples are given.},
  file = {/Users/fpalomba/Zotero/storage/SP7358C9/Wahba_1980_Automatic Smoothing of the Log Periodogram.pdf}
}

@article{whittle1957CurvePeriodogramSmoothing,
  title = {Curve and {{Periodogram Smoothing}}},
  author = {Whittle, P.},
  year = {1957},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {19},
  number = {1},
  eprint = {2983994},
  eprinttype = {jstor},
  pages = {38--63},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {0035-9246},
  abstract = {The difficulty in constructing smoothing formulae is to express quantitatively the type of smoothness one expects of the curve one is estimating. An argument is given in Sections 1 and 3 for formulating this "smoothness hypothesis" in terms of the properties of a population of curves of which the curve being estimated is a member. In equation (20) we obtain a solution for the matrix of optimum weighting coefficients in terms of certain "population moments" of the ordinates of the curve. Explicit formulae based on special assumptions are deduced in equations (34), (56)-(58). General information is gained on the way the optimum smoothing function and the variance of the smoothed estimate vary with the sample size and with the assumed degree of smoothness of the parent curve.},
  file = {/Users/fpalomba/Zotero/storage/4R9MTDZB/Whittle_1957_Curve and Periodogram Smoothing.pdf}
}

@book{wooldridge2018IntroductoryEconometricsModern,
  title = {Introductory {{Econometrics}}: {{A Modern Approach}}},
  shorttitle = {Introductory {{Econometrics}}},
  author = {Wooldridge, Jeffrey M.},
  year = {2018},
  month = sep,
  edition = {Seventh},
  publisher = {{Cengage Learning}},
  abstract = {Discover how empirical researchers today actually think about and apply econometric methods with the practical, professional approach in Wooldridge's INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 6E. Unlike traditional books, this unique presentation demonstrates how econometrics has moved beyond just a set of abstract tools to become genuinely useful for answering questions in business, policy evaluation, and forecasting environments. INTRODUCTORY ECONOMETRICS is organized around the type of data being analyzed with a systematic approach that only introduces assumptions as they are needed. This makes the material easier to understand and, ultimately, leads to better econometric practices. Packed with timely, relevant applications, the book introduces the latest emerging developments in the field. Gain a full understanding of the impact of econometrics in real practice today with the insights and applications found only in INTRODUCTORY ECONOMETRICS: A MODERN APPROACH, 6E.Important Notice: Media content referenced within the product description or the product text may not be available in the ebook version.},
  googlebooks = {wUF4BwAAQBAJ},
  isbn = {978-1-305-44638-0},
  langid = {english},
  keywords = {Business \& Economics / Econometrics},
  file = {/Users/fpalomba/Zotero/storage/SJG5YRFA/Wooldridge_2015_Introductory Econometrics.pdf}
}


@article{yao2007SpectralDensityEstimation,
  title = {Spectral {{Density Estimation Using Sharpened Periodograms}}},
  author = {Yao, Fang and Lee, Thomas C. M.},
  year = {2007},
  month = sep,
  journal = {IEEE Transactions on Signal Processing},
  volume = {55},
  number = {9},
  pages = {4711--4716},
  issn = {1941-0476},
  doi = {10.1109/TSP.2007.896297},
  abstract = {This correspondence introduces the use of sharpened periodograms for spectral density estimation. It is shown analytically that spectrum estimates obtained from smoothing the sharpened periodograms enjoy higher order bias reduction when compared with the ordinary smoothed periodogram estimates. The promising numerical performances of using sharpened periodograms for spectral density estimation are illustrated via numerical experiments.},
  keywords = {Bias reduction,Data preprocessing,data sharpening,Kernel,periodogram smoothing,Probability,sharpened periodograms,Smoothing methods,spectral density estimation,Statistics,unbiased risk estimation},
  file = {/Users/fpalomba/Zotero/storage/RMF8HGEW/Yao_Lee_2007_Spectral Density Estimation Using Sharpened Periodograms.pdf;/Users/fpalomba/Zotero/storage/893N5J79/4291877.html}
}

@misc{zhang2022BoundsSemiparametricInference,
  title = {Bounds and Semiparametric Inference in \${{L}}\^\textbackslash infty\$- and \${{L}}\^2\$-Sensitivity Analysis for Observational Studies},
  author = {Zhang, Yao and Zhao, Qingyuan},
  year = {2022},
  month = nov,
  number = {arXiv:2211.04697},
  eprint = {arXiv:2211.04697},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2211.04697},
  abstract = {Sensitivity analysis for the unconfoundedness assumption is a crucial component of observational studies. The marginal sensitivity model has become increasingly popular for this purpose due to its interpretability and mathematical properties. After reviewing the original marginal sensitivity model that imposes a \$L\^\textbackslash infty\$-constraint on the maximum logit difference between the observed and full data propensity scores, we introduce a more flexible \$L\^2\$-analysis framework; sensitivity value is interpreted as the "average" amount of unmeasured confounding in the analysis. We derive analytic solutions to the stochastic optimization problems under the \$L\^2\$-model, which can be used to bound the average treatment effect (ATE). We obtain the efficient influence functions for the optimal values and use them to develop efficient one-step estimators. We show that multiplier bootstrap can be applied to construct a simultaneous confidence band of the ATE. Our proposed methods are illustrated by simulation and real-data studies.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/fpalomba/Zotero/storage/GS56Y4GI/Zhang and Zhao - 2022 - Bounds and semiparametric inference in $L^infty$-.pdf;/Users/fpalomba/Zotero/storage/7F78EZM9/2211.html}
}
